{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLM with PyTorch FSDP and QLora on Amazon SageMaker AI using ModelTrainer\n",
    "\n",
    "In this notebook, we will fine-tune LLM on Amazon SageMaker AI, using Python scripts and SageMaker ModelTrainer for executing a training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fac8b6",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb9f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e26248",
   "metadata": {},
   "source": [
    "## This cell will restart the kernel. Click \"OK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a109cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f89d7a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d434c1",
   "metadata": {},
   "source": [
    "## Setup Configuration file path\n",
    "\n",
    "If you have created a Managed MLflow server, copy the `ARN` code here and assign a name to the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196500e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "from sagemaker.core.config import load_sagemaker_config\n",
    "\n",
    "sagemaker_session = Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "configs = load_sagemaker_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b937b",
   "metadata": {},
   "source": [
    "If you have your own MLflow tracking server, update the `TrackingServerName` value below to enable experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8055c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    response = boto3.client('sagemaker').describe_mlflow_tracking_server(\n",
    "        TrackingServerName='genai-mlflow-tracker'\n",
    "    )\n",
    "    mlflow_tracking_server_uri = response['TrackingServerArn']\n",
    "except ClientError:\n",
    "    mlflow_tracking_server_uri = \"\"\n",
    "\n",
    "if mlflow_tracking_server_uri == \"\":\n",
    "    print(\"No MLflow Tracking Server Found, experiments will not be tracked.\")\n",
    "else:\n",
    "    print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73066341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"mlflow_uri\"] = mlflow_tracking_server_uri\n",
    "os.environ[\"mlflow_experiment_name\"] = \"Qwen3-4B-Instruct-2507-sft\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f32ac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8754cea",
   "metadata": {},
   "source": [
    "## Visualize and upload the dataset\n",
    "\n",
    "We are going to load [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71190f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "full_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=f\"train[:{num_samples}]\")\n",
    "\n",
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d50468",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_datasets = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split_datasets[\"train\"]\n",
    "test_dataset = train_test_split_datasets[\"test\"]\n",
    "\n",
    "print(f\"Number of train elements: {len(train_dataset)}\")\n",
    "print(f\"Number of test elements: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d81311",
   "metadata": {},
   "source": [
    "Create a prompt template and load the dataset with a random sample to try summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\"\"\"\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def convert_to_messages(sample, system_prompt=\"\"):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": sample[\"Question\"]},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{sample['Complex_CoT']}\\n\\n{sample['Response']}\"}\n",
    "    ]\n",
    "\n",
    "    sample[\"messages\"] = messages\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22534fd9",
   "metadata": {},
   "source": [
    "Use the Hugging Face Trainer class to fine-tune the model. Define the hyperparameters we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0261ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "train_dataset = train_dataset.map(convert_to_messages, remove_columns=list(full_dataset.features), fn_kwargs={\"system_prompt\": SYSTEM_PROMPT})\n",
    "test_dataset = test_dataset.map(convert_to_messages, remove_columns=list(full_dataset.features), fn_kwargs={\"system_prompt\": SYSTEM_PROMPT})\n",
    "\n",
    "#grab a sample from the training and test sets\n",
    "print(f\"Train Sample:\\n{train_dataset[randint(0, len(train_dataset)-1)]}\\n\\n\")\n",
    "print(f\"Test Sample:\\n{test_dataset[randint(0, len(test_dataset)-1)]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2e077",
   "metadata": {},
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffb8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "# Save datasets to s3\n",
    "train_dataset.to_json(\"./data/train/dataset.json\", orient=\"records\")\n",
    "test_dataset.to_json(\"./data/test/dataset.json\", orient=\"records\")\n",
    "\n",
    "s3_client.upload_file(\"./data/train/dataset.json\", bucket_name, f\"{input_path}/train/dataset.json\")\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "s3_client.upload_file(\"./data/test/dataset.json\", bucket_name, f\"{input_path}/test/dataset.json\")\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/dataset.json\"\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_length_distribution\n",
    "\n",
    "plot_length_distribution(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=test_dataset,\n",
    "    bins=20,\n",
    "    figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f4861",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2951204",
   "metadata": {},
   "source": [
    "## Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9f8ee",
   "metadata": {},
   "source": [
    "We are now ready to fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\")\n",
    "\n",
    "use_local_model = True #set to false for the training job to download from HF, otherwise True will download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "if use_local_model:\n",
    "\n",
    "    model_local_location = f\"../models/{model_id_filesafe}\"\n",
    "    print(\"Downloading model \", model_id)\n",
    "    os.makedirs(model_local_location, exist_ok=True)\n",
    "    snapshot_download(repo_id=model_id, local_dir=model_local_location)\n",
    "    print(f\"Model {model_id} downloaded under {model_local_location}\")\n",
    "\n",
    "    if default_prefix:\n",
    "        model_s3_destination = f\"s3://{bucket_name}/{default_prefix}/models/{model_id_filesafe}\"\n",
    "    else:\n",
    "        model_s3_destination = f\"s3://{bucket_name}/models/{model_id_filesafe}\"\n",
    "    \n",
    "    print(f\"Beginning Model Upload...\")\n",
    "\n",
    "    subprocess.run(['aws', 's3', 'cp', model_local_location, model_s3_destination, '--recursive', '--exclude', '.cache/*', '--exclude', '.gitattributes'])\n",
    "    \n",
    "    print(f\"Model Uploaded to: \\n {model_s3_destination}\")\n",
    "\n",
    "    os.environ[\"model_location\"] = model_s3_destination\n",
    "else:\n",
    "    os.environ[\"model_location\"] = model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "\n",
    "# MLflow Config\n",
    "mlflow_uri: \"${mlflow_uri}\"\n",
    "mlflow_experiment_name: \"${mlflow_experiment_name}\"\n",
    "\n",
    "\n",
    "model_id: \"${model_location}\"       # Hugging Face model id, or S3 location\n",
    "\n",
    "# sagemaker specific parameters\n",
    "output_dir: \"/opt/ml/model\"                       # path to where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # path to where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"     # path to where FSx saves test dataset\n",
    "# training parameters\n",
    "max_seq_length: 1500  #512 # 2048\n",
    "lora_r: 8\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1                 \n",
    "learning_rate: 2e-4                    # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 2         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "gradient_checkpointing: true           # use gradient checkpointing\n",
    "fp16: true\n",
    "bf16: false                            # use bfloat16 precision\n",
    "tf32: false                            # use tf32 precision\n",
    "\n",
    "merge_weights: true                    # merge weights in the base model\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9639867",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3073b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/training_config/{model_id_filesafe}\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/training_config/{model_id_filesafe}\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6224c",
   "metadata": {},
   "source": [
    "## Fine-tune model\n",
    "\n",
    "Below will train the model with QLoRA, merge the adapter in the base model and save in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33365b7c",
   "metadata": {},
   "source": [
    "#### Get PyTorch image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea360617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core import image_uris\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sagemaker_session.boto_session.region_name,\n",
    "    version=\"2.6.0\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b014f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.model_trainer import ModelTrainer, InputData, Torchrun, StoppingCondition\n",
    "from sagemaker.core.training.configs import Compute, SourceCode\n",
    "from sagemaker.core.shapes import OutputDataConfig\n",
    "\n",
    "# Define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./scripts\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    entry_script=\"train.py\",\n",
    ")\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    volume_size_in_gb=50\n",
    ")\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f\"train-{model_id.split('/')[-1].replace('.', '-')}-sft-script\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "# Define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    distributed=Torchrun(),\n",
    "    stopping_condition=StoppingCondition(\n",
    "        max_runtime_in_seconds=7200\n",
    "    ),\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/args.yaml\"\n",
    "    },\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=output_path\n",
    "    ),\n",
    "    environment={\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"train\",\n",
    "    data_source=train_dataset_s3_path,\n",
    ")\n",
    "\n",
    "test_input = InputData(\n",
    "    channel_name=\"test\",\n",
    "    data_source=test_dataset_s3_path,\n",
    ")\n",
    "\n",
    "config_input = InputData(\n",
    "    channel_name=\"config\",\n",
    "    data_source=train_config_s3_path,\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "data = [train_input, test_input, config_input]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f07caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "model_trainer.train(input_data_config=data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a159fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae221e0b",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "In the following sections, we are going to deploy the fine-tuned model on an Amazon SageMaker Real-time endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d594e",
   "metadata": {},
   "source": [
    "## Load Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils import get_last_job_name\n",
    "\n",
    "job_prefix = f\"train-{model_id.split('/')[-1].replace('.', '-')}-sft-script\"\n",
    "\n",
    "job_name = get_last_job_name(job_prefix)\n",
    "\n",
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f76ee",
   "metadata": {},
   "source": [
    "#### Inference configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "health_check_timeout = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ec1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\"\n",
    "print(f\"using image to host: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f742cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.core.resources import Model, Endpoint, EndpointConfig\n",
    "from sagemaker.core.shapes import ContainerDefinition, ProductionVariant\n",
    "\n",
    "role = get_execution_role(sagemaker_session, use_default=True)\n",
    "\n",
    "if default_prefix:\n",
    "    model_data=f\"s3://{bucket_name}/{default_prefix}/{job_prefix}/{job_name}/output/model.tar.gz\"\n",
    "else:\n",
    "    model_data=f\"s3://{bucket_name}/{job_prefix}/{job_name}/output/model.tar.gz\"\n",
    "\n",
    "deploy_model_name = f\"Qwen3-4B-sft-{job_name[-8:]}\"\n",
    "\n",
    "core_model = Model.create(\n",
    "    model_name=deploy_model_name,\n",
    "    execution_role_arn=role,\n",
    "    primary_container=ContainerDefinition(\n",
    "        image=inference_image_uri,\n",
    "        model_data_url=model_data,\n",
    "        environment={\n",
    "            'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "            'OPTION_TRUST_REMOTE_CODE': 'true',\n",
    "            'OPTION_ROLLING_BATCH': \"vllm\",\n",
    "            'OPTION_DTYPE': 'bf16',\n",
    "            'OPTION_QUANTIZE': 'fp8',\n",
    "            'OPTION_TENSOR_PARALLEL_DEGREE': 'max',\n",
    "            'OPTION_MAX_ROLLING_BATCH_SIZE': '32',\n",
    "            'OPTION_MODEL_LOADING_TIMEOUT': '3600',\n",
    "            'OPTION_MAX_MODEL_LEN': '4096'\n",
    "        }\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1923c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.common_utils import name_from_base\n",
    "from sagemaker.core.helper.session_helper import _wait_until, _deploy_done\n",
    "\n",
    "endpoint_name = f\"{model_id.split('/')[-1].replace('.', '-')}-sft\"\n",
    "TUNED_ENDPOINT_NAME = name_from_base(endpoint_name)\n",
    "\n",
    "EndpointConfig.create(\n",
    "    endpoint_config_name=TUNED_ENDPOINT_NAME,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"AllTraffic\",\n",
    "            model_name=deploy_model_name,\n",
    "            initial_instance_count=instance_count,\n",
    "            instance_type=instance_type,\n",
    "            container_startup_health_check_timeout_in_seconds=health_check_timeout,\n",
    "            model_data_download_timeout_in_seconds=3600,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "core_endpoint = Endpoint.create(\n",
    "    endpoint_name=TUNED_ENDPOINT_NAME,\n",
    "    endpoint_config_name=TUNED_ENDPOINT_NAME,\n",
    ")\n",
    "\n",
    "_wait_until(lambda: _deploy_done(sagemaker_session.sagemaker_client, TUNED_ENDPOINT_NAME), poll=30)\n",
    "core_endpoint = Endpoint.get(endpoint_name=TUNED_ENDPOINT_NAME)\n",
    "print(f\"Endpoint status: {core_endpoint.endpoint_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edff016",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"A 3-week-old child has been diagnosed with late onset perinatal meningitis, and the CSF culture shows gram-positive bacilli. What characteristic of this bacterium can specifically differentiate it from other bacterial agents?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b09ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = json.dumps({\n",
    "\t\"messages\": messages,\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"return_full_text\": False,\n",
    "        \"max_new_tokens\": 1024\n",
    "    }\n",
    "})\n",
    "\n",
    "response = core_endpoint.invoke(\n",
    "    body=payload,\n",
    "    content_type=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    ")\n",
    "\n",
    "result = json.loads(response.body.read().decode(\"utf-8\"))\n",
    "result[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20682cd7",
   "metadata": {},
   "source": [
    "### Store variables\n",
    "\n",
    "Save the endpoint name for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72815070",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store TUNED_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3b221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
