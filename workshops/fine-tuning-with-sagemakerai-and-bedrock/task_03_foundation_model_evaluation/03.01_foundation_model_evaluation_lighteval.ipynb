{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Model Performance after Fine-Tuning\n",
    "In this example, we will take the pre-existing SageMaker endpoints that you deployed in previous exercises and use them to generate data that can be leveraged for quality comparison. This data can be used to take a quantitative approach to judge the efficacy of fine-tuning your models.\n",
    "\n",
    "This example will run through samples of the medical-o1-reasoning dataset (FreedomIntelligence/medical-o1-reasoning-SFT) on the Hugging Face data hub for medical Q&A and use the [lighteval](https://huggingface.co/docs/lighteval/index) from Hugging Face for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6aecf",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ae28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820edce3",
   "metadata": {},
   "source": [
    "## This cell will restart the kernel. Click \"OK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80104fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cace48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import ROUGE scorer\n",
    "from rouge_score import rouge_scorer\n",
    "# Import v3 SDK\n",
    "from sagemaker.core.resources import Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb37caa",
   "metadata": {},
   "source": [
    "#### Fetch the saved endpoint names from previous sections, or set them manually by uncommenting the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b42ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BASE_ENDPOINT_NAME\n",
    "%store -r TUNED_ENDPOINT_NAME\n",
    "\n",
    "#BASE_ENDPOINT_NAME = \"\"\n",
    "#TUNED_ENDPOINT_NAME = \"\"\n",
    "\n",
    "print(f\"Base Endpoint: {BASE_ENDPOINT_NAME}\")\n",
    "print(f\"Tuned Endpoint: {TUNED_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to evaluate\n",
    "model_to_evaluate = {\n",
    "    \"name\": \"Fine-tuned Model\", \n",
    "    \"endpoint\": TUNED_ENDPOINT_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac906227",
   "metadata": {},
   "source": [
    "Here you will use the the medical-o1-reasoning dataset. The dataset is pre-split into training and test data. We will limit the number of samples to evaluate for the fine-tuned and base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84199e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the number of samples to evaluate (for faster execution)\n",
    "num_samples = 10\n",
    "\n",
    "# Load the test split of the medical-o1-reasoning dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")\n",
    "\n",
    "max_samples = len(dataset)\n",
    "\n",
    "dataset = dataset.shuffle().select(range(min(num_samples, max_samples)))\n",
    "print(f\"Loaded medical-o1-reasoning dataset with {len(dataset)} samples out of {max_samples}\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"\\nQuestion:\\n\", sample[\"Question\"], \"\\n\\n====\\n\")\n",
    "print(\"Complex_CoT:\\n\", sample[\"Complex_CoT\"], \"\\n\\n====\\n\")\n",
    "print(\"Response:\\n\", sample[\"Response\"], \"\\n\\n====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc578ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\"\"\"\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def convert_to_messages(sample, system_prompt=\"\", include_answer=True):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": sample[\"Question\"]},\n",
    "    ]\n",
    "\n",
    "    if include_answer:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{sample['Complex_CoT']}\\n\\n{sample['Response']}\"})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40c701",
   "metadata": {},
   "source": [
    "#### Next, we will create functions to interact with the SageMaker endpoints, define metrics we want to calculate (ROUGE), and define how to evaluate the models with the medical-o1-reasoning dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "rouge_metrics = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    metrics = {}\n",
    "    rouge_scores = {'rouge1_f': [], 'rouge2_f': [], 'rougeL_f': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        rouge_result = rouge_metrics.score(ref, pred)\n",
    "        rouge_scores['rouge1_f'].append(rouge_result['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2_f'].append(rouge_result['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL_f'].append(rouge_result['rougeL'].fmeasure)\n",
    "    \n",
    "    for key in rouge_scores:\n",
    "        metrics[key] = sum(rouge_scores[key]) / len(rouge_scores[key])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0903e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_with_model(endpoint_name, dataset):\n",
    "    \"\"\"\n",
    "    Generate summaries using a model deployed on SageMaker.\n",
    "    Uses v3 SDK Endpoint.invoke() instead of v2 Predictor.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    core_endpoint = Endpoint.get(endpoint_name=endpoint_name)\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Generating Responses\"):\n",
    "        messages = convert_to_messages(example, system_prompt=SYSTEM_PROMPT, include_answer=False)\n",
    "        \n",
    "        payload = json.dumps({\n",
    "            \"messages\": messages,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            response = core_endpoint.invoke(\n",
    "                body=payload,\n",
    "                content_type=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "            )\n",
    "            result = json.loads(response.body.read().decode(\"utf-8\"))\n",
    "            prediction = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "            prediction = \"Error generating summary.\"\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(model_config, dataset):\n",
    "    model_name = model_config[\"name\"]\n",
    "    endpoint_name = model_config[\"endpoint\"]\n",
    "\n",
    "    print(f\"\\nEvaluating model: {model_name} on endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Get references\n",
    "    references = [\"\\n\".join([example[\"Complex_CoT\"], example[\"Response\"]]) for example in dataset]\n",
    "    \n",
    "    # Generate summaries\n",
    "    print(\"\\nGenerating Responses...\")\n",
    "    predictions = generate_summaries_with_model(endpoint_name, dataset)\n",
    "    \n",
    "    # Calculate automated metrics using LightEval\n",
    "    print(\"\\nCalculating evaluation metrics with LightEval...\")\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"num_samples\": len(dataset),\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": predictions[:5],\n",
    "        \"references\": references[:5]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"ROUGE-1 F1: {metrics['rouge1_f']:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {metrics['rouge2_f']:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {metrics['rougeL_f']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14e5ae",
   "metadata": {},
   "source": [
    "#### Evaluate both models\n",
    "\n",
    "**Note: Since the model you trained in this example was only exposed to a small amount of training data and the testing sample is small, you may see varied results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base and fine-tuned models using LightEval metrics\n",
    "start_time = time.time()\n",
    "\n",
    "base_model_config = {\n",
    "    \"name\": \"Base Model\",\n",
    "    \"endpoint\": BASE_ENDPOINT_NAME\n",
    "}\n",
    "\n",
    "# Evaluate base model\n",
    "base_model_results = evaluate_model_on_dataset(base_model_config, dataset)\n",
    "base_model_results[\"evaluation_time\"] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aee57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing fine-tuned model\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "finetuned_model_results = evaluate_model_on_dataset(model_to_evaluate, dataset)\n",
    "finetuned_model_results[\"evaluation_time\"] = time.time() - start_time\n",
    "\n",
    "# Save results\n",
    "base_file_name = base_model_config[\"name\"].replace(' ', '_').lower()\n",
    "finetuned_file_name = model_to_evaluate[\"name\"].replace(' ', '_').lower()\n",
    "\n",
    "with open(f\"{base_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(base_model_results, f)\n",
    "    \n",
    "with open(f\"{finetuned_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(finetuned_model_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6edd82",
   "metadata": {},
   "source": [
    "Create a tabular view to compare the base model and fine-tuned model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "comparison_data.append({\n",
    "    \"Model\": base_model_config[\"name\"],\n",
    "    \"ROUGE-1 F1\": base_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": base_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": base_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": base_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "comparison_data.append({\n",
    "    \"Model\": model_to_evaluate[\"name\"],\n",
    "    \"ROUGE-1 F1\": finetuned_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": finetuned_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": finetuned_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": finetuned_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba37051",
   "metadata": {},
   "source": [
    "Show a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROUGE metrics for both models\n",
    "metrics_to_plot = [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]\n",
    "models = comparison_df[\"Model\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(metrics_to_plot))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = [comparison_df.loc[i, metric] for metric in metrics_to_plot]\n",
    "    plt.bar(index + i*bar_width, values, bar_width, label=model)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Summarization Performance Comparison')\n",
    "plt.xticks(index + bar_width/2, metrics_to_plot)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement from base to fine-tuned model\n",
    "improvement_data = {}\n",
    "\n",
    "for metric in [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]:\n",
    "    base_value = comparison_df.loc[0, metric]\n",
    "    finetuned_value = comparison_df.loc[1, metric]\n",
    "    \n",
    "    if not pd.isna(base_value) and not pd.isna(finetuned_value):\n",
    "        abs_improvement = finetuned_value - base_value\n",
    "        pct_improvement = (abs_improvement / base_value) * 100 if base_value > 0 else float('inf')\n",
    "        \n",
    "        improvement_data[metric] = {\n",
    "            \"Base Model\": base_value,\n",
    "            \"Fine-tuned Model\": finetuned_value,\n",
    "            \"Absolute Improvement\": abs_improvement,\n",
    "            \"% Improvement\": pct_improvement\n",
    "        }\n",
    "\n",
    "improvement_df = pd.DataFrame({\n",
    "    \"Metric\": list(improvement_data.keys()),\n",
    "    \"Base Score\": [improvement_data[m][\"Base Model\"] for m in improvement_data],\n",
    "    \"Fine-tuned Score\": [improvement_data[m][\"Fine-tuned Model\"] for m in improvement_data],\n",
    "    \"Absolute Improvement\": [improvement_data[m][\"Absolute Improvement\"] for m in improvement_data],\n",
    "    \"% Improvement\": [f\"{improvement_data[m]['% Improvement']:.2f}%\" for m in improvement_data]\n",
    "})\n",
    "\n",
    "print(\"Improvement Analysis:\")\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd77c4",
   "metadata": {},
   "source": [
    "## Larger Training/Evaluation Results\n",
    "\n",
    "If you were to train **Qwen3-4B-Instruct-2507** on **5000** samples and evaluate on **100** test items (total training time 32 mins on an ml.g5.12xlarge instance), you would see the following results:\n",
    "\n",
    "![](./images/sft_5000_train_100_test_scores.png)\n",
    "\n",
    "![](images/sft_5000_train_100_test_bars.png)\n",
    "\n",
    "![](images/sft_5000_train_100_test_compare.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73d998",
   "metadata": {},
   "source": [
    "## Detailed Comparison Between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example predictions from both models\n",
    "num_examples = min(2, len(dataset))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {dataset[i]['Question']}\")\n",
    "\n",
    "    ref_cot_answer = '\\n'.join([dataset[i]['Complex_CoT'],dataset[i]['Response']])\n",
    "    print(f\"\\nReference CoT+Answer: {ref_cot_answer}\")\n",
    "    \n",
    "    print(f\"\\nBase Model Summary: {base_model_results['predictions'][i]}\")\n",
    "    print(f\"\\nFine-tuned Model Summary: {finetuned_model_results['predictions'][i]}\")\n",
    "    \n",
    "    # Calculate ROUGE scores for this example\n",
    "    base_rouge = rouge_metrics.score(ref_cot_answer, base_model_results['predictions'][i])\n",
    "    finetuned_rouge = rouge_metrics.score(ref_cot_answer, finetuned_model_results['predictions'][i])\n",
    "    \n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(f\"Base Model - ROUGE-1: {base_rouge['rouge1'].fmeasure:.4f}, ROUGE-2: {base_rouge['rouge2'].fmeasure:.4f}, ROUGE-L: {base_rouge['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Fine-tuned - ROUGE-1: {finetuned_rouge['rouge1'].fmeasure:.4f}, ROUGE-2: {finetuned_rouge['rouge2'].fmeasure:.4f}, ROUGE-L: {finetuned_rouge['rougeL'].fmeasure:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db65b08",
   "metadata": {},
   "source": [
    "# Clean Up Endpoints\n",
    "\n",
    "Run the following code to clean up your base endpoint. It is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ea7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "delete_base_response = sagemaker_client.delete_endpoint(\n",
    "    EndpointName=BASE_ENDPOINT_NAME\n",
    ")\n",
    "\n",
    "print(delete_base_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_basecfg_response = sagemaker_client.delete_endpoint_config(\n",
    "    EndpointConfigName=BASE_ENDPOINT_NAME\n",
    ")\n",
    "print(delete_basecfg_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981c80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
