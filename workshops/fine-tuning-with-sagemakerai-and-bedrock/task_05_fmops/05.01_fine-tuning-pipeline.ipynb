{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running hundreds of experiments, comparing the results, and keeping a track of the ML lifecycle can become very complex. This is where MLflow can help streamline the ML lifecycle, from data preparation to model deployment. By integrating MLflow into your LLM workflow, you can efficiently manage experiment tracking, model versioning, and deployment, providing reproducibility. With MLflow, you can track and compare the performance of multiple LLM experiments, identify the best-performing models, and deploy them to production environments with confidence. \n",
    "\n",
    "You can create workflows with SageMaker Pipelines that enable you to prepare data, fine-tune models, and evaluate model performance with simple Python code for each step. \n",
    "\n",
    "Now you can use SageMaker managed MLflow to run LLM fine-tuning and evaluation experiments at scale. Specifically:\n",
    "\n",
    "- MLflow can manage tracking of fine-tuning experiments, comparing evaluation results of different runs, model versioning, deployment, and configuration (such as data and hyperparameters)\n",
    "- SageMaker Pipelines can orchestrate multiple experiments based on the experiment configuration \n",
    "  \n",
    "\n",
    "The following figure shows the overview of the solution.\n",
    "![](./ml-16670-arch-with-mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- [HuggingFace access token](https://huggingface.co/docs/hub/en/security-tokens) – You need a HuggingFace login token to access the gated Llama 3.2 model and datasets used in this post.\n",
    "\n",
    "- Once you have your HuggingFace access token, navigate to the **steps/finetune_llama3b_hf.py** and update the **'hf_token'** parameter with your access token to download the Llama model for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies\n",
    "Restart the kernel after executing below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, as well as user-defined functions for the pipeline steps like finetune_llama3b_hf and preprocess_llama3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.steps import CacheConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "train_data_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "test_dataset_path = f\"s3://{bucket_name}/{input_path}/test/dataset.json\"\n",
    "\n",
    "pipeline_name = \"deepseek-finetune-pipeline\"\n",
    "    \n",
    "tracking_server_arn = \"arn:aws:sagemaker:us-east-1:905418257479:mlflow-tracking-server/genai-mlflow-tracker\"\n",
    "experiment_name = \"deepseek-finetune-pipeline\"\n",
    "os.environ[\"mlflow_uri\"] = \"\"\n",
    "os.environ[\"mlflow_experiment_name\"] = \"deepseek-finetune-pipeline\"\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\")\n",
    "model_s3_destination=\"s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\"\n",
    "use_local_model = True #set to false for the training job to download from HF, otherwise True will download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        InstanceType: ml.m5.xlarge\n",
    "        Dependencies: ./scripts/requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns: # files or directories to ignore\n",
    "          - \"*.ipynb\" # all notebook files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile requirements.txt\n",
    "# scikit-learn\n",
    "# xgboost==1.7.6\n",
    "# s3fs==0.4.2\n",
    "# sagemaker>=2.199.0,<3\n",
    "# pandas>=2.0.0\n",
    "# gevent\n",
    "# geventhttpclient\n",
    "# shap\n",
    "# matplotlib\n",
    "# fsspec\n",
    "# mlflow==2.13.2\n",
    "# sagemaker-mlflow==0.1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "model_local_location = f\"../models/{model_id_filesafe}\"\n",
    "# print(\"Downloading model \", model_id)\n",
    "# os.makedirs(model_local_location, exist_ok=True)\n",
    "# snapshot_download(repo_id=model_id, local_dir=model_local_location)\n",
    "# print(f\"Model {model_id} downloaded under {model_local_location}\")\n",
    "\n",
    "# if default_prefix:\n",
    "#     model_s3_destination = f\"s3://{bucket_name}/{default_prefix}/models/{model_id_filesafe}\"\n",
    "# else:\n",
    "#     model_s3_destination = f\"s3://{bucket_name}/models/{model_id_filesafe}\"\n",
    "\n",
    "# print(f\"Beginning Model Upload...\")\n",
    "\n",
    "# subprocess.run(['aws', 's3', 'cp', model_local_location, model_s3_destination, '--recursive', '--exclude', '.cache/*', '--exclude', '.gitattributes'])\n",
    "\n",
    "# print(f\"Model Uploaded to: \\n {model_s3_destination}\")\n",
    "\n",
    "# os.environ[\"model_location\"] = model_s3_destination\n",
    "\n",
    "print(model_s3_destination)\n",
    "os.environ[\"model_location\"] = model_s3_destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique for large language models. The parameters here (lora_r, lora_alpha, lora_dropout) control the behavior of LoRA during fine-tuning, affecting the trade-off between model performance and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLflow Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management. **Update the ARN for the MLflow tracking server.**\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location.\n",
    "\n",
    "experiment_name: give appropriate name for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset Configuration\n",
    "\n",
    "For the purpose of fine tuning and evaluation we are going too use `HuggingFaceH4/no_robots` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Steps\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Step**\n",
    "\n",
    "This step handles data preparation. We are going to prepare data for training and evaluation. We will log this data in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"DataPreprocessing\",\n",
    "    instance_type=processing_instance_type,\n",
    "    display_name=\"Data Preprocessing\",\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")\n",
    "def preprocess(\n",
    "    input_path: str,\n",
    "    experiment_name: str,\n",
    "    run_id: str,\n",
    ") -> tuple:\n",
    "    import boto3\n",
    "    import shutil\n",
    "    import sagemaker\n",
    "    from sagemaker.config import load_sagemaker_config\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    bucket_name = sagemaker_session.default_bucket()\n",
    "    default_prefix = sagemaker_session.default_bucket_prefix\n",
    "    configs = load_sagemaker_config()\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "    \n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    df = df[:100]\n",
    "    \n",
    "    # df.head()\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "    \n",
    "    print(\"Number of train elements: \", len(train))\n",
    "    print(\"Number of test elements: \", len(test))\n",
    "    \n",
    "    # custom instruct prompt start\n",
    "    prompt_template = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "    Write a response that appropriately completes the request.\n",
    "    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {{question}}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {{complex_cot}}\n",
    "    \n",
    "    {{answer}}\n",
    "    <|eot_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    # template dataset to add prompt to each sample\n",
    "    def template_dataset(sample):\n",
    "        sample[\"text\"] = prompt_template.format(question=sample[\"Question\"],\n",
    "                                                complex_cot=sample[\"Complex_CoT\"],\n",
    "                                                answer=sample[\"Response\"])\n",
    "        return sample\n",
    "    \n",
    "    from datasets import Dataset, DatasetDict\n",
    "    from random import randint\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train)\n",
    "    test_dataset = Dataset.from_pandas(test)\n",
    "    \n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "    \n",
    "    train_dataset = dataset[\"train\"].map(template_dataset, remove_columns=list(dataset[\"train\"].features))\n",
    "    \n",
    "    print(train_dataset[randint(0, len(dataset))][\"text\"])\n",
    "    \n",
    "    test_dataset = dataset[\"test\"].map(template_dataset, remove_columns=list(dataset[\"test\"].features))\n",
    "    \n",
    "    # save train_dataset to s3 using our SageMaker session\n",
    "    # if default_prefix:\n",
    "    #     input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    # else:\n",
    "    #     input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    if default_prefix:\n",
    "        input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    else:\n",
    "        input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "    # Save datasets to s3\n",
    "    # We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "    train_dataset.to_json(\"./data/train/dataset.json\", orient=\"records\")\n",
    "    test_dataset.to_json(\"./data/test/dataset.json\", orient=\"records\")\n",
    "    train_data_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "    test_dataset_path = f\"s3://{bucket_name}/{input_path}/test/dataset.json\"\n",
    "    s3_client.upload_file(\"./data/train/dataset.json\", bucket_name, f\"{input_path}/train/dataset.json\")\n",
    "    s3_client.upload_file(\"./data/test/dataset.json\", bucket_name, f\"{input_path}/test/dataset.json\")\n",
    "\n",
    "    print(train_data_path)\n",
    "    print(test_dataset_path)\n",
    "\n",
    "    shutil.rmtree(\"./data\")\n",
    "\n",
    "    return experiment_name, run_id, train_data_path, test_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "\n",
    "# MLflow Config\n",
    "mlflow_uri: \"${mlflow_uri}\"\n",
    "mlflow_experiment_name: \"${mlflow_experiment_name}\"\n",
    "\n",
    "\n",
    "model_id: \"${model_location}\"       # Hugging Face model id, or S3 location\n",
    "\n",
    "# sagemaker specific parameters\n",
    "output_dir: \"/opt/ml/model\"                       # path to where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # path to where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"     # path to where FSx saves test dataset\n",
    "# training parameters\n",
    "max_seq_length: 1500  #512 # 2048\n",
    "lora_r: 8\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1                 \n",
    "learning_rate: 2e-4                    # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "gradient_checkpointing: true           # use gradient checkpointing\n",
    "fp16: true\n",
    "bf16: false                            # use bfloat16 precision, also enables FlashAttention2 (requires Ampere/Hopper GPU+ ex:A10, A100, H100)\n",
    "tf32: false                            # use tf32 precision\n",
    "\n",
    "#uncomment here for fsdp - start\n",
    "# fsdp: \"full_shard auto_wrap offload\"\n",
    "# fsdp_config: \n",
    "#     backward_prefetch: \"backward_pre\"\n",
    "#     cpu_ram_efficient_loading: true\n",
    "#     offload_params: true\n",
    "#     forward_prefetch: false\n",
    "#     use_orig_params: true\n",
    "#uncomment here for fsdp - end\n",
    "\n",
    "merge_weights: true                    # merge weights in the base model\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config uploaded to:\n",
      "s3://sagemaker-us-east-1-891377369387/training_config/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/config/args.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/training_config/{model_id_filesafe}\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/training_config/{model_id_filesafe}\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning Step**\n",
    "\n",
    "This is where the actual model adaptation occurs. The step takes the preprocessed data and applies it to fine-tune the base LLM (in this case, a Llama model). It incorporates the LoRA technique for efficient adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelFineTuning\",\n",
    "    instance_type=training_instance_type,\n",
    "    display_name=\"Model Fine Tuning\",\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")\n",
    "def train(\n",
    "    train_dataset_s3_path: str,\n",
    "    test_dataset_s3_path: str,\n",
    "    train_config_s3_path: str,\n",
    "    experiment_name: str,\n",
    "    model_id: str,\n",
    "    run_id: str,\n",
    "):\n",
    "    import sagemaker\n",
    "    import boto3\n",
    "    job_name = \"deepseek-finetune-pipeline\"\n",
    "    from sagemaker.pytorch import PyTorch\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    pytorch_estimator = PyTorch(\n",
    "        entry_point='train.py',\n",
    "        source_dir=\"./scripts\",\n",
    "        job_name=job_name,\n",
    "        base_job_name=job_name,\n",
    "        max_run=50000,\n",
    "        role=role,\n",
    "        framework_version=\"2.2.0\",\n",
    "        py_version=\"py310\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.p3.2xlarge\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        volume_size=50,\n",
    "        disable_output_compression=True,\n",
    "        keep_alive_period_in_seconds=1800,\n",
    "        distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "        hyperparameters={\n",
    "            \"config\": \"/opt/ml/input/data/config/args.yaml\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # define a data input dictonary with our uploaded s3 uris\n",
    "    data = {\n",
    "      'train': train_dataset_s3_path,\n",
    "      'test': test_dataset_s3_path,\n",
    "      'config': train_config_s3_path\n",
    "      }\n",
    "\n",
    "    print(f\"Data for Training Run: {data}\")\n",
    "\n",
    "    pytorch_estimator.fit(data, wait=True)\n",
    "\n",
    "    latest_run_job_name = pytorch_estimator.latest_training_job.job_name\n",
    "    print(f\"Latest Job Name: {latest_run_job_name}\")\n",
    "\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "    # Describe the training job\n",
    "    response = sagemaker_client.describe_training_job(TrainingJobName=latest_run_job_name)\n",
    "\n",
    "    # Extract the model artifacts S3 path\n",
    "    model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "    # Extract the output path (this is the general output location)\n",
    "    output_path = response['OutputDataConfig']['S3OutputPath']\n",
    "\n",
    "    print(f\"Model artifacts S3 path: {model_artifacts_s3_path}\")\n",
    "\n",
    "    return experiment_name, run_id, model_artifacts_s3_path, output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Step**\n",
    "\n",
    "After fine-tuning, this step assesses the model's performance. It uses built-in evaluation function in MLflow to evaluate metrices like toxicity, exact_match etc:\n",
    "\n",
    "It will then log the results in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">▲</span>                                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SyntaxError: </span><span style=\"color: #008700; text-decoration-color: #008700\">'return'</span> outside function\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;91m▲\u001b[0m                                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mSyntaxError: \u001b[0m\u001b[38;2;0;135;0m'return'\u001b[0m outside function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@step(\n",
    "    name=\"ModelEvaluation\",\n",
    "    instance_type=training_instance_type,\n",
    "    display_name=\"Model Evaluation\",\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")\n",
    "def evaluate(\n",
    "    experiment_name: str,\n",
    "    run_id: str,\n",
    "    model_artifacts_s3_path: str,\n",
    "):\n",
    "    # Import libraries\n",
    "    import os\n",
    "    import json\n",
    "    import time\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm.notebook import tqdm\n",
    "    from datasets import load_dataset\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import transformers\n",
    "\n",
    "    # Import LightEval metrics\n",
    "    from lighteval.metrics.metrics_sample import ROUGE, Doc\n",
    "\n",
    "    # Initialize the SageMaker client\n",
    "    sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "    FINETUNED_MODEL_ENDPOINT = \"DeepSeek-R1-Distill-Llama-8B-sft-djl\"  # Update with Fine-tuned model endpoint name\n",
    "\n",
    "    # Define the model to evaluate\n",
    "    model_to_evaluate = {\n",
    "        \"name\": \"Fine-tuned DeepSeek-R1-Distill-Llama-8B\", \n",
    "        \"endpoint\": FINETUNED_MODEL_ENDPOINT\n",
    "    }\n",
    "    # Limit the number of samples to evaluate (for faster execution)\n",
    "    num_samples = 10\n",
    "\n",
    "    # Load the test split of the SAMSum dataset\n",
    "    dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")\n",
    "\n",
    "    max_samples = len(dataset)\n",
    "\n",
    "    dataset = dataset.shuffle().select(range(min(num_samples, max_samples)))\n",
    "    print(f\"Loaded medical-o1-reasoning dataset with {len(dataset)} samples out of {max_samples}\")\n",
    "\n",
    "    # Display a sample from the dataset\n",
    "    sample = dataset[0]\n",
    "\n",
    "    print(\"\\nQuestion:\\n\", sample[\"Question\"], \"\\n\\n====\\n\")\n",
    "    print(\"Complex_CoT:\\n\", sample[\"Complex_CoT\"], \"\\n\\n====\\n\")\n",
    "    print(\"Response:\\n\", sample[\"Response\"], \"\\n\\n====\\n\")\n",
    "\n",
    "\n",
    "    # This function allows you to interact with a deployed SageMaker endpoint to get predictions from the DeepSeek model\n",
    "    def invoke_sagemaker_endpoint(payload, endpoint_name):\n",
    "        \"\"\"\n",
    "        Invoke a SageMaker endpoint with the given payload.\n",
    "\n",
    "        Args:\n",
    "            payload (dict): The input data to send to the endpoint\n",
    "            endpoint_name (str): The name of the SageMaker endpoint\n",
    "\n",
    "        Returns:\n",
    "            dict: The response from the endpoint\n",
    "        \"\"\"\n",
    "        response = sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        response_body = response['Body'].read().decode('utf-8')\n",
    "        return json.loads(response_body)\n",
    "\n",
    "\n",
    "    # Initialize LightEval metrics calculators\n",
    "    rouge_metrics = ROUGE(\n",
    "        methods=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        multiple_golds=False,\n",
    "        bootstrap=False,\n",
    "        normalize_gold=None,\n",
    "        normalize_pred=None\n",
    "    )\n",
    "\n",
    "\n",
    "    def calculate_metrics(predictions, references):\n",
    "        \"\"\"\n",
    "        Calculate all evaluation metrics for summarization using LightEval.\n",
    "\n",
    "        Args:\n",
    "            predictions (list): List of generated summaries\n",
    "            references (list): List of reference summaries\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing all metric scores\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Create Doc objects for the Rouge and BertScore metrics\n",
    "        docs = []\n",
    "        for reference in references:\n",
    "            docs.append(Doc(\n",
    "                {\"target\": reference},\n",
    "                choices=[reference],  # Dummy choices\n",
    "                gold_index=0  # Dummy gold_index\n",
    "            ))\n",
    "\n",
    "        # Calculate ROUGE scores for each prediction-reference pair\n",
    "        rouge_scores = {'rouge1_f': [], 'rouge2_f': [], 'rougeL_f': []}\n",
    "\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # For ROUGE calculation\n",
    "            rouge_result = rouge_metrics.compute(golds=[ref], predictions=[pred])\n",
    "            rouge_scores['rouge1_f'].append(rouge_result['rouge1'])\n",
    "            rouge_scores['rouge2_f'].append(rouge_result['rouge2'])\n",
    "            rouge_scores['rougeL_f'].append(rouge_result['rougeL'])\n",
    "\n",
    "        # Average ROUGE scores\n",
    "        for key in rouge_scores:\n",
    "            metrics[key] = sum(rouge_scores[key]) / len(rouge_scores[key])\n",
    "\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def generate_summaries_with_model(endpoint_name, dataset):\n",
    "        \"\"\"\n",
    "        Generate summaries using a model deployed on SageMaker.\n",
    "\n",
    "        Args:\n",
    "            endpoint_name (str): SageMaker endpoint name\n",
    "            dataset: Dataset containing dialogues\n",
    "\n",
    "        Returns:\n",
    "            list: Generated summaries\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for example in tqdm(dataset, desc=\"Generating Responses\"):\n",
    "            question = example[\"Question\"]\n",
    "\n",
    "            # Prepare the prompt for the model\n",
    "            prompt = f\"\"\"\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>system<|end_header_id|>\n",
    "            You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "            Write a response that appropriately completes the request.\n",
    "            Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            {question}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "            # Payload for SageMaker endpoint\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 512,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Call the model endpoint\n",
    "            try:\n",
    "                response = invoke_sagemaker_endpoint(payload, endpoint_name)\n",
    "\n",
    "                # Extract the generated text\n",
    "                if isinstance(response, list):\n",
    "                    prediction = response[0].get('generated_text', '').strip()\n",
    "                elif isinstance(response, dict):\n",
    "                    prediction = response.get('generated_text', '').strip()\n",
    "                else:\n",
    "                    prediction = str(response).strip\n",
    "\n",
    "                prediction = prediction.split(\"<|eot_id|>\")[0]\n",
    "                # Clean up the generated text\n",
    "                #if \"Summary:\" in prediction:\n",
    "                #    prediction = prediction.split(\"Summary:\", 1)[1].strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "                prediction = \"Error generating summary.\"\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_model_on_dataset(model_config, dataset):\n",
    "        \"\"\"\n",
    "        Evaluate a fine-tuned model on the SamSum dataset using both automated and human metrics.\n",
    "\n",
    "        Args:\n",
    "            model_config (dict): Model configuration with name and endpoint\n",
    "            dataset: SamSum dataset for evaluation\n",
    "\n",
    "        Returns:\n",
    "            dict: Evaluation results\n",
    "        \"\"\"\n",
    "        model_name = model_config[\"name\"]\n",
    "        endpoint_name = model_config[\"endpoint\"]\n",
    "\n",
    "        print(f\"\\nEvaluating model: {model_name} on endpoint: {endpoint_name}\")\n",
    "\n",
    "        # Get references\n",
    "        references = [\"\\n\".join([example[\"Complex_CoT\"], example[\"Response\"]]) for example in dataset]\n",
    "\n",
    "        # Generate summaries\n",
    "        print(\"\\nGenerating Responses...\")\n",
    "        predictions = generate_summaries_with_model(endpoint_name, dataset)\n",
    "\n",
    "        # Calculate automated metrics using LightEval\n",
    "        print(\"\\nCalculating evaluation metrics with LightEval...\")\n",
    "        metrics = calculate_metrics(predictions, references)\n",
    "\n",
    "        # Format results\n",
    "        results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"endpoint_name\": endpoint_name,\n",
    "            \"num_samples\": len(dataset),\n",
    "            \"metrics\": metrics,\n",
    "            \"predictions\": predictions[:5],  # First 5 predictions\n",
    "            \"references\": references[:5]     # First 5 references\n",
    "        }\n",
    "\n",
    "        # Print key results\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        print(f\"ROUGE-1 F1: {metrics['rouge1_f']:.4f}\")\n",
    "        print(f\"ROUGE-2 F1: {metrics['rouge2_f']:.4f}\")\n",
    "        print(f\"ROUGE-L F1: {metrics['rougeL_f']:.4f}\")\n",
    "\n",
    "        return results, metrics['rouge1_f'], metrics['rouge2_f'], metrics['rougeL_f']\n",
    "\n",
    "    finetuned_model_results, rouge1_f, rouge2_f, rougeL_f = evaluate_model_on_dataset(model_to_evaluate, dataset)\n",
    "\n",
    "    return experiment_name, run_id, rouge1_f, rouge2_f, rougeL_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps. The lora_config is passed as a parameter, allowing for easy modification of LoRA settings between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = preprocess(\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    input_path=input_path,\n",
    ")\n",
    "\n",
    "training_step = train(\n",
    "    train_dataset_s3_path=preprocessing_step[2],\n",
    "    test_dataset_s3_path=preprocessing_step[3],\n",
    "    train_config_s3_path=train_config_s3_path,\n",
    "    experiment_name=preprocessing_step[0],\n",
    "    run_id=preprocessing_step[1],\n",
    "    model_id=model_s3_destination,\n",
    ")\n",
    "\n",
    "evaluate_step = evaluate(\n",
    "    experiment_name=training_step[0],\n",
    "    run_id=training_step[1],\n",
    "    model_artifacts_s3_path=training_step[2],\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        instance_type,\n",
    "    ],\n",
    "    steps=[preprocessing_step, training_step, evaluate_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 08:45:31,205 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-08-45-29-048/function\n",
      "2025-05-21 08:45:31,338 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-08-45-29-048/arguments\n",
      "2025-05-21 08:45:31,611 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmp7d7wq13m/requirements.txt'\n",
      "2025-05-21 08:45:31,638 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-08-45-29-048/pre_exec_script_and_dependencies'\n",
      "2025-05-21 08:45:31,642 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmpd4ri3qct/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-05-21 08:45:31,652 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmpd4ri3qct/workspace.zip'\n",
      "2025-05-21 08:45:31,683 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/sm_rf_user_ws/2025-05-21-08-45-29-048/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 08:45:33,527 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-08-45-29-048/function\n",
      "2025-05-21 08:45:33,582 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-08-45-29-048/arguments\n",
      "2025-05-21 08:45:33,636 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmp386ztkby/requirements.txt'\n",
      "2025-05-21 08:45:33,675 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-08-45-29-048/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-05-21 08:45:34,233 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-08-45-34-233/function\n",
      "2025-05-21 08:45:34,290 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-08-45-34-233/arguments\n",
      "2025-05-21 08:45:34,560 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmp_ks6i0xh/requirements.txt'\n",
      "2025-05-21 08:45:34,586 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-08-45-34-233/pre_exec_script_and_dependencies'\n",
      "2025-05-21 08:45:34,590 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmp8tt1aeyb/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-05-21 08:45:34,599 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmp8tt1aeyb/workspace.zip'\n",
      "2025-05-21 08:45:34,634 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/sm_rf_user_ws/2025-05-21-08-45-34-233/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-05-21 08:45:34,636 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-08-45-34-233/function\n",
      "2025-05-21 08:45:34,734 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-08-45-34-233/arguments\n",
      "2025-05-21 08:45:34,787 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpt3lb927f/requirements.txt'\n",
      "2025-05-21 08:45:34,817 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-08-45-34-233/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:891377369387:pipeline/deepseek-finetune-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '60048ada-b507-49ee-b774-4bd67b092954',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '60048ada-b507-49ee-b774-4bd67b092954',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '94',\n",
       "   'date': 'Wed, 21 May 2025 08:45:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution1 = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
