{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinating FMOps Steps into a Fine-Tuning and Model Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we stitch together the components of FMOps into a full FMOps pipeline on SageMaker AI. This capability creates a Directed-Acyclic Graph of steps, orchestrated by SageMaker AI and Managed MLFlow 3.0 on Amazon SageMaker.\n",
    "\n",
    "Running hundreds of experiments, comparing the results, and keeping a track of the ML lifecycle can become very complex. This is where MLflow can help streamline the ML lifecycle, from data preparation to model deployment. By integrating MLflow into your LLM workflow, you can efficiently manage experiment tracking, model versioning, and deployment, providing reproducibility of steps. With MLflow, you can track and compare the performance of multiple LLM experiments, identify the best-performing models, and deploy them to production environments with confidence. \n",
    "\n",
    "You can create workflows with SageMaker Pipelines that enable you to prepare data, fine-tune models, and evaluate model performance with simple Python code for each step. \n",
    "\n",
    "Now you can use SageMaker managed MLflow to run LLM fine-tuning and evaluation experiments at scale. Specifically:\n",
    "\n",
    "- MLflow can manage tracking of fine-tuning experiments, comparing evaluation results of different runs, model versioning, deployment, and configuration (such as data and hyperparameters)\n",
    "- SageMaker Pipelines can orchestrate multiple experiments based on the experiment configuration \n",
    "  \n",
    "\n",
    "The following figure shows the overview of the solution.\n",
    "\n",
    "![](./Task5-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- MLflow tracking server: If you're running this lab in a workshop environment, a MLflow tracking server has already been created for you. If you need to create a MLflow tracking server, follow the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies\n",
    "Restart the kernel after executing below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:56:50.196738Z",
     "iopub.status.busy": "2025-10-15T17:56:50.196483Z",
     "iopub.status.idle": "2025-10-15T17:56:58.331883Z",
     "shell.execute_reply": "2025-10-15T17:56:58.331192Z",
     "shell.execute_reply.started": "2025-10-15T17:56:50.196718Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:56:58.332985Z",
     "iopub.status.busy": "2025-10-15T17:56:58.332771Z",
     "iopub.status.idle": "2025-10-15T17:56:58.338882Z",
     "shell.execute_reply": "2025-10-15T17:56:58.338429Z",
     "shell.execute_reply.started": "2025-10-15T17:56:58.332962Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, which will be used to define the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:10:38.890122Z",
     "iopub.status.busy": "2025-10-15T18:10:38.889793Z",
     "iopub.status.idle": "2025-10-15T18:10:40.611213Z",
     "shell.execute_reply": "2025-10-15T18:10:40.610667Z",
     "shell.execute_reply.started": "2025-10-15T18:10:38.890099Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:29.689442Z",
     "iopub.status.busy": "2025-10-15T18:12:29.689175Z",
     "iopub.status.idle": "2025-10-15T18:12:30.791433Z",
     "shell.execute_reply": "2025-10-15T18:12:30.790896Z",
     "shell.execute_reply.started": "2025-10-15T18:12:29.689422Z"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "pipeline_name = \"qwen3-finetune-pipeline\"\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\").replace(\".\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management. **Update the ARN for the MLflow tracking server.**\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires a SageMaker with MLflow tracking server to track experiments and manage model artifacts. To create your own tracking server please refer to the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html). Once you have created your tracking server, please copy the tracking server ARN to the `mlflow_tracking server_arn` variable in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:30.838220Z",
     "iopub.status.busy": "2025-10-15T18:12:30.837950Z",
     "iopub.status.idle": "2025-10-15T18:12:31.008197Z",
     "shell.execute_reply": "2025-10-15T18:12:31.007542Z",
     "shell.execute_reply.started": "2025-10-15T18:12:30.838200Z"
    }
   },
   "outputs": [],
   "source": [
    "mlflow_tracking_server_arn = \"<REPLACE WITH YOUR ARN>\"\n",
    "\n",
    "try:\n",
    "    response = boto3.client('sagemaker').describe_mlflow_tracking_server(\n",
    "        TrackingServerName='genai-mlflow-tracker'\n",
    "    )\n",
    "    mlflow_tracking_server_arn = response['TrackingServerArn']\n",
    "    print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")\n",
    "except ClientError:\n",
    "    print(\"No MLflow Tracking Server Found, please input a value for mlflow_tracking_server_arn\")\n",
    "\n",
    "os.environ[\"mlflow_tracking_server_arn\"] = mlflow_tracking_server_arn\n",
    "os.environ[\"pipeline_name\"] = pipeline_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides blanket configuration for how remote functions should be executed in a SageMaker environment. This configuration helps to streamline remote function execution which is particularly useful for optimizing the execution of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:32.608898Z",
     "iopub.status.busy": "2025-10-15T18:12:32.608642Z",
     "iopub.status.idle": "2025-10-15T18:12:32.612632Z",
     "shell.execute_reply": "2025-10-15T18:12:32.612142Z",
     "shell.execute_reply.started": "2025-10-15T18:12:32.608878Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        InstanceType: ml.m5.xlarge\n",
    "        Dependencies: ./scripts/requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns: # files or directories to ignore\n",
    "          - \"*.ipynb\" # all notebook files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:33.245143Z",
     "iopub.status.busy": "2025-10-15T18:12:33.244889Z",
     "iopub.status.idle": "2025-10-15T18:12:33.247752Z",
     "shell.execute_reply": "2025-10-15T18:12:33.247285Z",
     "shell.execute_reply.started": "2025-10-15T18:12:33.245122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download Model Data from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:34.760569Z",
     "iopub.status.busy": "2025-10-15T18:12:34.760310Z",
     "iopub.status.idle": "2025-10-15T18:12:35.448363Z",
     "shell.execute_reply": "2025-10-15T18:12:35.447833Z",
     "shell.execute_reply.started": "2025-10-15T18:12:34.760549Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "# Simple function to check if file exists in S3\n",
    "def s3_file_exists(s3_client, bucket, key):\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False\n",
    "\n",
    "# Simple S3 upload function that checks if files exist before uploading\n",
    "def simple_s3_upload(local_dir, s3_bucket, s3_prefix, skip_existing=True):\n",
    "    \"\"\"\n",
    "    Upload files to S3, skipping files that already exist.\n",
    "    \n",
    "    Args:\n",
    "        local_dir (str): Local directory containing files to upload\n",
    "        s3_bucket (str): S3 bucket name\n",
    "        s3_prefix (str): S3 prefix (folder path)\n",
    "        skip_existing (bool): Whether to skip files that already exist in S3\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (uploaded_files, skipped_files, failed_files)\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    uploaded_files = []\n",
    "    skipped_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    # Get all local files\n",
    "    local_files = []\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            rel_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = os.path.join(s3_prefix, rel_path).replace('\\\\', '/')\n",
    "            local_files.append((local_path, s3_key))\n",
    "    \n",
    "    print(f\"Found {len(local_files)} files in {local_dir}\")\n",
    "    \n",
    "    # Process each file sequentially\n",
    "    for local_path, s3_key in local_files:\n",
    "        try:\n",
    "            # Check if file exists in S3\n",
    "            if skip_existing and s3_file_exists(s3_client, s3_bucket, s3_key):\n",
    "                print(f\"Skipping {s3_key} (file exists in S3)\")\n",
    "                skipped_files.append(s3_key)\n",
    "                continue\n",
    "            \n",
    "            # Upload the file\n",
    "            print(f\"Uploading {local_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "            s3_client.upload_file(\n",
    "                local_path, \n",
    "                s3_bucket, \n",
    "                s3_key,\n",
    "                ExtraArgs={'ACL': 'bucket-owner-full-control'}\n",
    "            )\n",
    "            uploaded_files.append(s3_key)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {local_path}: {str(e)}\")\n",
    "            failed_files.append((s3_key, str(e)))\n",
    "    \n",
    "    print(f\"\\nUpload Summary:\")\n",
    "    print(f\"  - Uploaded: {len(uploaded_files)} files\")\n",
    "    print(f\"  - Skipped: {len(skipped_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    return uploaded_files, skipped_files, failed_files\n",
    "\n",
    "# Set local and S3 model paths\n",
    "model_local_location = f\"../models/{model_id_filesafe}\"\n",
    "if default_prefix:\n",
    "    model_s3_destination = f\"s3://{bucket_name}/{default_prefix}/models/{model_id_filesafe}\"\n",
    "    prefix = f\"{default_prefix}/models/{model_id_filesafe}\"\n",
    "else:\n",
    "    model_s3_destination = f\"s3://{bucket_name}/models/{model_id_filesafe}\"\n",
    "    prefix = f\"models/{model_id_filesafe}\"\n",
    "\n",
    "print(\"Downloading model \", model_id)\n",
    "os.makedirs(model_local_location, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    snapshot_download(repo_id=model_id, local_dir=model_local_location)\n",
    "    print(f\"Model {model_id} downloaded under {model_local_location}\")\n",
    "    \n",
    "    print(f\"Beginning Model Upload to {model_s3_destination}...\")\n",
    "    \n",
    "    # Use the simple upload function without threads or batch processing\n",
    "    uploaded, skipped, failed = simple_s3_upload(\n",
    "        local_dir=model_local_location,\n",
    "        s3_bucket=bucket_name,\n",
    "        s3_prefix=prefix,\n",
    "        skip_existing=True\n",
    "    )\n",
    " \n",
    "    print(f\"Model successfully uploaded to: \\n {model_s3_destination}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model download or upload: {e}\")\n",
    "    raise\n",
    "\n",
    "os.environ[\"model_location\"] = model_s3_destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Configure Fine-Tuning Job\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code.\n",
    "\n",
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique that reduces the number of trainable parameters by adding low-rank decomposition matrices to existing weights rather than updating all model weights. This significantly reduces memory requirements and training time while maintaining performance comparable to full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:37.083014Z",
     "iopub.status.busy": "2025-10-15T18:12:37.082756Z",
     "iopub.status.idle": "2025-10-15T18:12:37.095488Z",
     "shell.execute_reply": "2025-10-15T18:12:37.094990Z",
     "shell.execute_reply.started": "2025-10-15T18:12:37.082991Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "\n",
    "# MLflow Config\n",
    "mlflow_uri: \"${mlflow_tracking_server_arn}\"                # The URI for the MLflow tracking server \n",
    "mlflow_experiment_name: \"${pipeline_name}\"  # Name of the MLflow experiment for organizing runs\n",
    "\n",
    "\n",
    "model_id: \"${model_location}\"              # Hugging Face model id, or S3 location of base model\n",
    "\n",
    "# SageMaker specific parameters \n",
    "output_dir: \"/opt/ml/model\"                # Path where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # Path where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"     # Path where FSx saves test dataset\n",
    "\n",
    "# Training parameters\n",
    "max_seq_length: 1500                       # Maximum sequence length for inputs (affects memory usage)\n",
    "                                           # Higher values allow for longer context but require more memory\n",
    "                                           # Range: 512-4096 depending on model architecture and hardware\n",
    "\n",
    "# LoRA parameters (Low-Rank Adaptation)\n",
    "lora_r: 8                                  # Rank of the LoRA update matrices\n",
    "                                           # Lower values (4-16) are more efficient, higher values (32-64) can improve quality\n",
    "                                           # Recommended range: 8-64 depending on task complexity\n",
    "lora_alpha: 16                             # Scaling factor for the LoRA update\n",
    "                                           # Generally set to 2x lora_r for good performance\n",
    "lora_dropout: 0.1                          # Dropout probability for LoRA layers\n",
    "                                           # Range: 0.0-0.5, helps prevent overfitting\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate: 2e-4                        # Learning rate for parameter updates\n",
    "                                           # Range: 1e-5 to 5e-4 for LoRA fine-tuning\n",
    "                                           # Too high: training instability, too low: slow convergence\n",
    "\n",
    "# Training loop parameters\n",
    "num_train_epochs: 1                        # Number of complete passes through the training dataset\n",
    "                                           # More epochs can improve performance but risk overfitting\n",
    "                                           # Range: 1-5 for LoRA fine-tuning\n",
    "per_device_train_batch_size: 2             # Number of samples per GPU during training\n",
    "                                           # Larger values improve training speed but require more memory\n",
    "                                           # Range: 1-8 for large models on common GPUs\n",
    "per_device_eval_batch_size: 1              # Number of samples per GPU during evaluation\n",
    "                                           # Can typically be larger than training batch size\n",
    "gradient_accumulation_steps: 2             # Accumulate gradients over multiple steps\n",
    "                                           # Effectively increases batch size by this factor\n",
    "                                           # Useful when limited by GPU memory\n",
    "\n",
    "# Memory optimization techniques\n",
    "gradient_checkpointing: true               # Reduces memory usage by recomputing activations during backward pass\n",
    "                                           # Trades computation for memory, ~20% slower but enables larger models/sequences\n",
    "fp16: true                                 # Use half-precision floating point (speeds up training, reduces memory)\n",
    "bf16: false                                # Use bfloat16 precision (better numerical stability than fp16)\n",
    "                                           # Also enables FlashAttention2 (requires Ampere/Hopper GPU+ eg:A10, A100, H100)\n",
    "tf32: false                                # Use TensorFloat-32 precision (NVIDIA Ampere+ GPUs only)\n",
    "\n",
    "#uncomment here for fsdp - start\n",
    "# fsdp: \"full_shard auto_wrap offload\"     # Fully Sharded Data Parallel training\n",
    "                                           # Splits model states across multiple GPUs\n",
    "# fsdp_config:                             # Configuration for FSDP\n",
    "#     backward_prefetch: \"backward_pre\"    # Prefetches parameters before backward pass\n",
    "#     cpu_ram_efficient_loading: true      # More memory-efficient parameter loading\n",
    "#     offload_params: true                 # Offloads parameters to CPU when not in use\n",
    "#     forward_prefetch: false              # Don't prefetch parameters for forward pass\n",
    "#     use_orig_params: true                # Use original parameter ordering\n",
    "#uncomment here for fsdp - end\n",
    "\n",
    "merge_weights: true                        # Merge adapter weights into the base model\n",
    "                                           # true: produces standalone model, false: keeps adapter separate\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:40.136027Z",
     "iopub.status.busy": "2025-10-15T18:12:40.135746Z",
     "iopub.status.idle": "2025-10-15T18:12:40.542117Z",
     "shell.execute_reply": "2025-10-15T18:12:40.541629Z",
     "shell.execute_reply.started": "2025-10-15T18:12:40.136005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/training_config/{model_id_filesafe}\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/training_config/{model_id_filesafe}\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps.\n",
    "\n",
    "1. Preprocessing Step -- Reformat all of the fine-tuning data to the prompt format required for the fine-tuning job.\n",
    "2. Training Step -- Execute the model fine-tuning job using the preprocessed data.\n",
    "3. Deploy Step -- Deploy the model to a SageMaker AI Managed Endpoint for testing fine-tuning performance.\n",
    "4. Quantitative Evaluation Step -- Evaluate the model's performance using ROUGE scores.\n",
    "5. Qualitative Evaluation Step -- Evaluate the model's performance using LLM-as-a-Judge.\n",
    "6. Conditionally Register Model -- Register the model if the quantitative and qualitative evaluations meet criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:12:43.868659Z",
     "iopub.status.busy": "2025-10-15T18:12:43.868403Z",
     "iopub.status.idle": "2025-10-15T18:12:44.216288Z",
     "shell.execute_reply": "2025-10-15T18:12:44.215721Z",
     "shell.execute_reply.started": "2025-10-15T18:12:43.868639Z"
    }
   },
   "outputs": [],
   "source": [
    "from steps import pipeline_utils\n",
    "guardrail_id, guardrail_version = pipeline_utils.get_or_create_guardrail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:13:22.801488Z",
     "iopub.status.busy": "2025-10-15T18:13:22.801207Z",
     "iopub.status.idle": "2025-10-15T18:13:25.970853Z",
     "shell.execute_reply": "2025-10-15T18:13:25.970278Z",
     "shell.execute_reply.started": "2025-10-15T18:13:22.801468Z"
    }
   },
   "outputs": [],
   "source": [
    "from steps import (\n",
    "    preprocess_step,\n",
    "    finetune_step,\n",
    "    deploy_step,\n",
    "    quantitative_eval_step,\n",
    "    qualitative_eval_step,\n",
    "    model_registration_step\n",
    ")\n",
    "from sagemaker.workflow.step_collections import StepCollection\n",
    "\n",
    "preprocessing_step = preprocess_step.preprocess(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    input_path=input_path\n",
    ")\n",
    "\n",
    "training_step = finetune_step.train(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=preprocessing_step[0],\n",
    "    train_dataset_s3_path=preprocessing_step[1],\n",
    "    test_dataset_s3_path=preprocessing_step[2],\n",
    "    train_config_s3_path=train_config_s3_path,\n",
    "    role=role,\n",
    "    model_id=model_s3_destination\n",
    ")\n",
    "run_id=training_step[0]\n",
    "model_artifacts_s3_path=training_step[2]\n",
    "\n",
    "deploy_step = deploy_step.deploy(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    model_artifacts_s3_path=model_artifacts_s3_path,\n",
    "    model_id=model_s3_destination,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=run_id,\n",
    ")\n",
    "endpoint_name=deploy_step\n",
    "\n",
    "mlflow_trace_attributes = {\n",
    "    \"model_id\": model_id,\n",
    "    \"guardrail_id\": guardrail_id,\n",
    "    \"guardrail_version\": guardrail_version\n",
    "}\n",
    "quantitative_eval_step = quantitative_eval_step.quantitative_evaluate(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=run_id,\n",
    "    endpoint_name=endpoint_name,\n",
    "    mlflow_trace_attributes=mlflow_trace_attributes\n",
    ")\n",
    "\n",
    "qualitative_eval_step = qualitative_eval_step.qualitative_evaluate(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=run_id,\n",
    "    endpoint_name=endpoint_name,\n",
    "    mlflow_trace_attributes=mlflow_trace_attributes\n",
    ")\n",
    "\n",
    "evaluation_gate = ConditionStep(\n",
    "    name=\"EvaluationGate\",\n",
    "    depends_on=[qualitative_eval_step],\n",
    "    conditions=[\n",
    "        ConditionGreaterThanOrEqualTo(\n",
    "            left=quantitative_eval_step[\"rougeL_f\"],\n",
    "            right=0.2\n",
    "        ),\n",
    "        ConditionGreaterThanOrEqualTo(\n",
    "            left=qualitative_eval_step[\"avg_medical_accuracy\"],\n",
    "            right=3.0\n",
    "        )\n",
    "    ],\n",
    "    if_steps=[\n",
    "        model_registration_step.register_model(\n",
    "            tracking_server_arn=mlflow_tracking_server_arn,\n",
    "            experiment_name=pipeline_name,\n",
    "            run_id=run_id,  # Assuming training_step returns run_id as first output\n",
    "            model_artifacts_s3_path=model_artifacts_s3_path,  # Assuming training_step returns artifacts path as second output\n",
    "            model_id=model_id,\n",
    "            model_name=f\"Fine-Tuned-Medical-Qwen3-4B-Instruct-2507\",\n",
    "            endpoint_name=endpoint_name,\n",
    "            evaluation_score=quantitative_eval_step[\"rougeL_f\"],  # Get the evaluation score\n",
    "            pipeline_name=pipeline_name,\n",
    "            model_description=\"Fine-tuned medical LLM for clinical reasoning and diagnostics\"\n",
    "        )\n",
    "    ],\n",
    "    else_steps=[\n",
    "        FailStep(\n",
    "            name=\"EvaluationFailed\",\n",
    "            error_message=\"Model evaluation failed to meet quality thresholds.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combining the steps into the pipeline definition\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        instance_type,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        training_step,\n",
    "        deploy_step,\n",
    "        quantitative_eval_step,\n",
    "        evaluation_gate\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:13:25.972041Z",
     "iopub.status.busy": "2025-10-15T18:13:25.971610Z",
     "iopub.status.idle": "2025-10-15T18:13:38.971434Z",
     "shell.execute_reply": "2025-10-15T18:13:38.970889Z",
     "shell.execute_reply.started": "2025-10-15T18:13:25.972020Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T18:13:38.972413Z",
     "iopub.status.busy": "2025-10-15T18:13:38.972199Z",
     "iopub.status.idle": "2025-10-15T18:13:39.429711Z",
     "shell.execute_reply": "2025-10-15T18:13:39.429119Z",
     "shell.execute_reply.started": "2025-10-15T18:13:38.972394Z"
    }
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint to avoid incurring charges\n",
    "import boto3\n",
    "import time\n",
    "import botocore\n",
    "\n",
    "def delete_endpoint_with_retry(endpoint_name, max_retries=3, wait_seconds=10):\n",
    "    \"\"\"\n",
    "    Delete a SageMaker endpoint with retry logic\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): Name of the SageMaker endpoint to delete\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        wait_seconds (int): Time to wait between retries in seconds\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if deletion was successful, False otherwise\n",
    "    \"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # First check if the endpoint exists\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        endpoint_exists = True\n",
    "    except sm_client.exceptions.ClientError as e:\n",
    "        if \"Could not find endpoint\" in str(e):\n",
    "            print(f\"Endpoint {endpoint_name} does not exist, no cleanup needed.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error checking endpoint existence: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # If we get here, the endpoint exists and we should delete it\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempting to delete endpoint {endpoint_name} (attempt {attempt + 1}/{max_retries})\")\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "            print(f\"Endpoint {endpoint_name} deletion initiated successfully\")\n",
    "            \n",
    "            # Wait for endpoint to be fully deleted\n",
    "            print(\"Waiting for endpoint to be fully deleted...\")\n",
    "            \n",
    "            # Poll until endpoint is deleted or max wait time is reached\n",
    "            total_wait_time = 0\n",
    "            max_wait_time = 300  # 5 minutes maximum wait\n",
    "            while total_wait_time < max_wait_time:\n",
    "                try:\n",
    "                    sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "                    print(f\"Endpoint still exists, waiting {wait_seconds} seconds...\")\n",
    "                    time.sleep(wait_seconds)\n",
    "                    total_wait_time += wait_seconds\n",
    "                except sm_client.exceptions.ClientError:\n",
    "                    print(f\"Endpoint {endpoint_name} successfully deleted\")\n",
    "                    return True\n",
    "            \n",
    "            # If we get here, the endpoint still exists after max_wait_time\n",
    "            print(f\"Warning: Endpoint deletion initiated but still exists after {max_wait_time} seconds\")\n",
    "            return False\n",
    "            \n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if \"ResourceInUse\" in str(e) or \"ResourceNotFound\" in str(e):\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "                print(f\"Retrying in {wait_seconds} seconds...\")\n",
    "                time.sleep(wait_seconds)\n",
    "            else:\n",
    "                print(f\"Unexpected error deleting endpoint: {e}\")\n",
    "                return False\n",
    "    \n",
    "    print(f\"Failed to delete endpoint {endpoint_name} after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "# Clean up endpoint\n",
    "try:\n",
    "    endpoint_name = f\"{model_id.replace('/', '-').replace('_', '-')}-sft-djl\"\n",
    "    \n",
    "    print(f\"Cleaning up endpoint: {endpoint_name}\")\n",
    "    if delete_endpoint_with_retry(endpoint_name):\n",
    "        print(\"Cleanup completed successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Endpoint cleanup may have failed, please check the SageMaker console\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during endpoint cleanup: {str(e)}\")\n",
    "    print(\"You may need to manually delete the endpoint from the SageMaker console\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
