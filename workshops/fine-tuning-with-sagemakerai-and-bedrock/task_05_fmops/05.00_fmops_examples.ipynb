{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Operations of FMOps\n",
    "\n",
    "The purpose of this notebook is to illustrate the capabilities SageMaker AI and Managed MLflow on SageMaker AI for FMOps tasks. In this notebook, we cover the foundational capabilities needed to develop an automated LLM fine-tuning and evaluation pipeline. We cover these components individually, without an orchestration service, to showcase the capabilities atomically. This notebook lays the groundwork for the next notebook, which stiches together these disparate components into a fully orchestrated fine-tuning and model evaluation pipeline powered by SageMaker AI Pipelines and Managed MLflow on SageMaker AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- MLflow tracking server: If you're running this lab in a workshop environment, a MLflow tracking server has already been created for you. If you need to create a MLflow tracking server, follow the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies\n",
    "Install dependencies and configure kernel.\n",
    "\n",
    "Restart the kernel after executing below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, which will be used to define the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import mlflow\n",
    "import tarfile\n",
    "import botocore\n",
    "import sagemaker\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources.\n",
    "\n",
    "If you are running this lab in a workshop environment, the execution role will have the appropriate permissions necessary to execute the following tasks. If not, you may need to check the permissions attached to your sagemaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration\n",
    "Here we setup our example execution environment.\n",
    "\n",
    "We define appropriate paths in S3 to store model files, define the model we will be working with, and define the model endpoint name.\n",
    "\n",
    "In this lab, we are working with [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507). It is easy to fine-tune as we will see in the next lab, and is small enough to fit on a reasonably sized GPU-accelerated hosting endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(bucket_name)\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\").replace(\".\", \"_\")\n",
    "model_name_safe = model_id.split('/')[-1].replace('.', '-').replace('_', '-')\n",
    "endpoint_name = f\"Example-{model_name_safe}\"\n",
    "instance_count = 1\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "health_check_timeout = 1800\n",
    "data_download_timeout = 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management. \n",
    "\n",
    "**Update the ARN for the MLflow tracking server.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires a SageMaker with MLflow tracking server to track experiments and manage model artifacts. To create your own tracking server please refer to the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html). Once you have created your tracking server, please copy the tracking server ARN to the `mlflow_tracking server_arn` variable in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_tracking_server_arn = \"<REPLACE WITH YOUR ARN>\"\n",
    "\n",
    "try:\n",
    "    response = boto3.client('sagemaker').describe_mlflow_tracking_server(\n",
    "        TrackingServerName='genai-mlflow-tracker'\n",
    "    )\n",
    "    mlflow_tracking_server_arn = response['TrackingServerArn']\n",
    "    print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")\n",
    "except botocore.exceptions.ClientError:\n",
    "    print(\"No MLflow Tracking Server Found, please input a value for mlflow_tracking_server_arn\")\n",
    "\n",
    "os.environ[\"mlflow_tracking_server_arn\"] = mlflow_tracking_server_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Deployment\n",
    "There are several approaches to deploying a model to a SageMaker AI managed endpoint. In this section, we explore the most direct option which downloads a model directly from HuggingFace to the managed endpoint via SageMaker JumpStart. We are still using Qwen3-4B-Instruct-2507, but we have not fine-tuned it. The purpose of this section is to illustrate the components required to customize a model deployment on SageMaker before fine-tuning it.\n",
    "\n",
    "![](./Task5-Deploy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image URI\n",
    "By default, images downloaded from HuggingFace use the [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/index) model serving toolkit. \n",
    "\n",
    "For this lab, we want to change the underlying model server to [Deep Java Library's Large Model Inference](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html) container, or DJL-LMI. This serving container offers [several performance benefits](https://aws.amazon.com/blogs/machine-learning/supercharge-your-llm-performance-with-amazon-sagemaker-large-model-inference-container-v15/) that we want to leverage for the production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\"\n",
    "print(f\"using image to host: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace + SageMaker JumpStart\n",
    "Here we download the model from SageMaker Jumpstart and create a `HuggingFaceModel` object. Notice how we define the `model_id` in the configuration, and specify the `image_uri` defined above in the instantiation of the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'SM_NUM_GPUS': json.dumps(1),\n",
    "    'OPTION_TRUST_REMOTE_CODE': 'true',\n",
    "    'OPTION_ROLLING_BATCH': \"vllm\",\n",
    "    'OPTION_DTYPE': 'bf16',\n",
    "    'OPTION_QUANTIZE': 'fp8',\n",
    "    'OPTION_TENSOR_PARALLEL_DEGREE': 'max',\n",
    "    'OPTION_MAX_ROLLING_BATCH_SIZE': '32',\n",
    "    'OPTION_MODEL_LOADING_TIMEOUT': '3600',\n",
    "    'OPTION_MAX_MODEL_LEN': '4096'\n",
    "}\n",
    "model = HuggingFaceModel(\n",
    "    image_uri=inference_image_uri,\n",
    "    env=model_config,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Deploy w/Managed MLFlow 3.0 on SageMaker AI\n",
    "Now we stitch the pieces together and use MLFlow to orchestrate the deployment of our model to a SageMaker AI managed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLFlow tracking data...\n",
    "mlflow.set_tracking_uri(mlflow_tracking_server_arn)\n",
    "mlflow.set_experiment(\"Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"example_model_deployment\"):\n",
    "    deployment_start_time = time.time()\n",
    "\n",
    "\n",
    "    # Log deployment parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": model_id,\n",
    "        \"instance_type\": instance_type,\n",
    "        \"instance_count\": instance_count,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"health_check_timeout\": health_check_timeout,\n",
    "        \"data_download_timeout\": data_download_timeout\n",
    "    })\n",
    "    mlflow.log_params({\"model_config_\" + k: v for k, v in model_config.items()})\n",
    "\n",
    "    try:\n",
    "        # deploy model to SageMaker Inference\n",
    "        predictor = model.deploy(\n",
    "            initial_instance_count=instance_count,\n",
    "            instance_type=instance_type,\n",
    "            container_startup_health_check_timeout=health_check_timeout,\n",
    "            model_data_download_timeout=data_download_timeout,\n",
    "            endpoint_name=f\"{endpoint_name}\"\n",
    "        )\n",
    "\n",
    "        # Log deployment metrics\n",
    "        deployment_time = time.time() - deployment_start_time\n",
    "        mlflow.log_metric(\"deployment_time_seconds\", deployment_time)\n",
    "        mlflow.log_metric(\"deployment_success\", 1)\n",
    "\n",
    "        # Log tags\n",
    "        mlflow.set_tags({\n",
    "            \"endpoint_status\": \"deployed\",\n",
    "            \"deployment_type\": \"sagemaker\",\n",
    "            \"framework\": \"djl-lmi\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log deployment failure\n",
    "        mlflow.log_metric(\"deployment_success\", 0)\n",
    "        mlflow.log_param(\"error_message\", str(e))\n",
    "        mlflow.set_tag(\"endpoint_status\", \"failed\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model Prediction\n",
    "Now we stitch the pieces together and use MLFlow to orchestrate the deployment of our model to a SageMaker AI managed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=f\"{endpoint_name}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "predictor.predict({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hi, what can you help me with?\"}\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_guardrail():\n",
    "    # Guardrail doesn't exist, create it\n",
    "    try:\n",
    "        guardrail = guardrail_client.create_guardrail(\n",
    "            name=\"ExampleMedicalGuardrail\",\n",
    "            description='Example of a Guardrail for Medical Use Cases',\n",
    "            topicPolicyConfig={\n",
    "                'topicsConfig': [{\n",
    "                    'name': 'Block Pharmaceuticals',\n",
    "                    'definition': 'This model cannot recommend one pharmaceutical over another. Generic prescriptions consistent with medical expertise and clinical diagnoses only.',\n",
    "                    'type': 'DENY',\n",
    "                    'inputAction': 'BLOCK',\n",
    "                    'outputAction': 'BLOCK',\n",
    "                }]        \n",
    "            },\n",
    "            sensitiveInformationPolicyConfig={\n",
    "                'piiEntitiesConfig': [\n",
    "                    {\n",
    "                        'type': 'UK_NATIONAL_HEALTH_SERVICE_NUMBER',\n",
    "                        'action': 'BLOCK',\n",
    "                        'inputAction': 'BLOCK',\n",
    "                        'outputAction': 'BLOCK'\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            contextualGroundingPolicyConfig={\n",
    "                'filtersConfig': [\n",
    "                    {\n",
    "                        'type': 'RELEVANCE',\n",
    "                        'threshold': 0.9,\n",
    "                        'action': 'BLOCK',\n",
    "                        'enabled': True\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            blockedInputMessaging=\"ExampleMedicalGuardrail has blocked this input.\",\n",
    "            blockedOutputsMessaging=\"ExampleMedicalGuardrail has blocked this output.\"\n",
    "        )\n",
    "        guardrail_id = guardrail['guardrailId']\n",
    "        guardrail_version = guardrail['version']\n",
    "        \n",
    "        print(f\"Created new guardrail '{guardrail_id}:{guardrail_version}'\")\n",
    "        return guardrail_id, guardrail_version\n",
    "    except botocore.exceptions.ClientError as create_error:\n",
    "        print(f\"Error creating guardrail: {create_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_client = boto3.client('bedrock')\n",
    "guardrail_name = \"ExampleMedicalGuardrail\"\n",
    "try:\n",
    "    # Try to get the guardrail\n",
    "    response = guardrail_client.list_guardrails()\n",
    "    guardrail_id = \"\"\n",
    "    for guardrail in response.get('guardrails', []):\n",
    "        if guardrail['name'] == guardrail_name:\n",
    "            guardrail_id = guardrail['id']\n",
    "    if guardrail_id != \"\":\n",
    "        response = guardrail_client.get_guardrail(\n",
    "            guardrailIdentifier=guardrail_id\n",
    "        )\n",
    "        guardrail_version = response[\"version\"]\n",
    "        print(f\"Found Guardrail {guardrail_id}:{guardrail_version}\")\n",
    "    else:\n",
    "        guardrail_id, guardrail_version = create_guardrail()\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    print(f\"Error checking guardrail: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Qualitative Model Evaluation\n",
    "Let's test the default Qwen3-4B-Instruct-2507 using MLFlow's LLM-as-a-Judge capability. We'll use [Anthropic's Claude Sonnet 4](https://www.anthropic.com/news/claude-4) model on [Amazon Bedrock](https://aws.amazon.com/bedrock/) as the judge. We'll also wrap our model endpoint invocation in a method making it easier to call in the evaluation. \n",
    "\n",
    "This particular endpoint is the [global cross-region inference endpoint](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html) name for Claude Sonnet 4.\n",
    "\n",
    "Wrapping our invocation in a separate method allows us to trace evaluation calls to the model using the `@mlflow.trace` annotation. These traces will appear in our MLFlow experiment under the \"Traces\" tab.\n",
    "\n",
    "![](Task5-Evaluate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge_llm = \"bedrock:/anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "judge_llm = \"bedrock:/global.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import SpanType\n",
    "\n",
    "@mlflow.trace(\n",
    "    name=\"call-local-llm\", span_type=SpanType.LLM, attributes={\n",
    "        \"model\": model_id,\n",
    "        \"guardrail_id\": guardrail_id,\n",
    "        \"guardrail_version\": guardrail_version\n",
    "    }\n",
    ")\n",
    "def invoke_sagemaker_endpoint(payload):\n",
    "\n",
    "    print(payload)\n",
    "\n",
    "    guardrail_response_input = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='INPUT',\n",
    "        content=[{'text': {'text': payload[\"messages\"][0][\"content\"]}}]\n",
    "    )\n",
    "    guardrailResult = guardrail_response_input[\"action\"]\n",
    "\n",
    "    if guardrailResult == \"GUARDRAIL_INTERVENED\":\n",
    "        reason = guardrail_response_input[\"assessments\"]\n",
    "        print(f\"Guardrail intervention: {reason}\")\n",
    "        return guardrail_response_input[\"outputs\"][0][\"text\"], -1\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = predictor.predict(payload)\n",
    "        inference_time = time.time() - start_time\n",
    "        return response, inference_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking endpoint {endpoint_name}: {str(e)}\")\n",
    "        return None, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Managed MLFlow 3.0 on Amazon SageMaker AI's `EvaluationExample` object to provide examples of good and bad model responses. This synthetic data will be used to evaluate our Example Qwen3-4B-Instruct-2507 along with several qualitative metrics. We create these qualitative metrics using `make_genai_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample, make_genai_metric\n",
    "\n",
    "medical_accuracy_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"What is the first-line treatment for hypertension?\",\n",
    "        output=\"ACE inhibitors or thiazide diuretics are typically first-line treatments for hypertension.\",\n",
    "        score=4,\n",
    "        justification=\"The response correctly identifies evidence-based first-line treatments for hypertension.\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What causes Type 1 diabetes?\",\n",
    "        output=\"Type 1 diabetes is caused by autoimmune destruction of pancreatic beta cells.\",\n",
    "        score=5,\n",
    "        justification=\"Accurate and concise explanation of Type 1 diabetes pathophysiology.\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do you treat a heart attack?\",\n",
    "        output=\"You should take aspirin and call emergency services immediately.\",\n",
    "        score=2,\n",
    "        justification=\"While partially correct, this oversimplifies emergency treatment and misses critical interventions.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "medical_accuracy = make_genai_metric(\n",
    "    name=\"medical_accuracy\",\n",
    "    definition=(\n",
    "        \"Medical accuracy measures how factually correct and evidence-based the medical information is. \"\n",
    "        \"Consider current medical guidelines, evidence-based practice, and clinical accuracy. \"\n",
    "        \"Score 1-5 where 5 is completely accurate and evidence-based.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Evaluate the medical accuracy of the response on a scale of 1-5:\\n\"\n",
    "        \"5: Completely accurate, evidence-based, follows current medical guidelines\\n\"\n",
    "        \"4: Mostly accurate with minor gaps or generalizations\\n\"\n",
    "        \"3: Generally accurate but missing important details or context\\n\"\n",
    "        \"2: Partially accurate but contains some medical inaccuracies\\n\"\n",
    "        \"1: Contains significant medical errors or misinformation\\n\\n\"\n",
    "        \"Question: {input}\\n\"\n",
    "        \"Response: {output}\\n\\n\"\n",
    "        \"Consider: Is the medical information factually correct? Does it align with current evidence-based practice? \"\n",
    "        \"Are there any dangerous inaccuracies or omissions?\\n\\n\"\n",
    "        \"Provide your score as a single integer from 1-5.\"\n",
    "    ),\n",
    "    examples=medical_accuracy_examples,\n",
    "    version=\"v1\",\n",
    "    model=judge_llm,\n",
    "    parameters={\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1000\n",
    "    },\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Clinical Reasoning Metric\n",
    "clinical_reasoning_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"A 65-year-old man presents with chest pain. What should be considered?\",\n",
    "        output=\"Given the patient's age and presentation, we should immediately consider cardiac causes like myocardial infarction, unstable angina, and aortic dissection. The approach should include ECG, cardiac enzymes, chest X-ray, and careful history taking about pain characteristics, onset, and associated symptoms.\",\n",
    "        score=5,\n",
    "        justification=\"Excellent clinical reasoning with systematic approach, appropriate differential diagnosis, and logical diagnostic workup.\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"Patient has fever and cough. What's the diagnosis?\",\n",
    "        output=\"The patient has pneumonia and needs antibiotics.\",\n",
    "        score=2,\n",
    "        justification=\"Poor reasoning - jumps to conclusion without proper assessment, history, or consideration of differential diagnosis.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "clinical_reasoning = make_genai_metric(\n",
    "    name=\"clinical_reasoning\",\n",
    "    definition=(\n",
    "        \"Clinical reasoning evaluates the logical thought process, differential diagnosis consideration, \"\n",
    "        \"and systematic approach to medical problems. Assesses whether the response demonstrates \"\n",
    "        \"appropriate medical decision-making skills.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Evaluate the clinical reasoning quality on a scale of 1-5:\\n\"\n",
    "        \"5: Excellent reasoning with systematic approach, appropriate differential diagnosis, logical steps\\n\"\n",
    "        \"4: Good reasoning with mostly sound logic and appropriate considerations\\n\"\n",
    "        \"3: Adequate reasoning but may miss some important considerations\\n\"\n",
    "        \"2: Poor reasoning with significant logical gaps or inappropriate approach\\n\"\n",
    "        \"1: No clear reasoning or completely inappropriate approach\\n\\n\"\n",
    "        \"Question: {input}\\n\"\n",
    "        \"Response: {output}\\n\\n\"\n",
    "        \"Consider: Does the response show logical thinking? Are differential diagnoses considered? \"\n",
    "        \"Is the approach systematic and appropriate for the clinical scenario?\\n\\n\"\n",
    "        \"Provide your score as a single integer from 1-5.\"\n",
    "    ),\n",
    "    examples=clinical_reasoning_examples,\n",
    "    version=\"v1\",\n",
    "    model=judge_llm,\n",
    "    parameters={\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1000\n",
    "    },\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Patient Safety Metric\n",
    "patient_safety_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"Patient allergic to penicillin needs antibiotic. What do you recommend?\",\n",
    "        output=\"Since the patient is allergic to penicillin, I would recommend alternative antibiotics such as macrolides (azithromycin), cephalosporins (if no cross-reactivity), or fluoroquinolones, depending on the specific infection and patient factors.\",\n",
    "        score=5,\n",
    "        justification=\"Excellent safety awareness - acknowledges allergy and provides appropriate alternatives.\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"Patient with chest pain asks if they can drive home.\",\n",
    "        output=\"Sure, if you're feeling better you can drive home.\",\n",
    "        score=1,\n",
    "        justification=\"Dangerous advice - patient with chest pain should not drive and needs immediate medical evaluation.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "patient_safety = make_genai_metric(\n",
    "    name=\"patient_safety\",\n",
    "    definition=(\n",
    "        \"Patient safety measures whether the response prioritizes patient wellbeing, avoids harmful advice, \"\n",
    "        \"considers contraindications, and promotes safe medical practices.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Evaluate patient safety considerations on a scale of 1-5:\\n\"\n",
    "        \"5: Prioritizes safety, considers contraindications, promotes safe practices\\n\"\n",
    "        \"4: Generally safe with minor safety considerations\\n\"\n",
    "        \"3: Mostly safe but may miss some safety considerations\\n\"\n",
    "        \"2: Some safety concerns or inappropriate advice\\n\"\n",
    "        \"1: Potentially dangerous advice or significant safety issues\\n\\n\"\n",
    "        \"Question: {input}\\n\"\n",
    "        \"Response: {output}\\n\\n\"\n",
    "        \"Consider: Is the advice safe? Are contraindications considered? Could following this advice harm the patient?\\n\\n\"\n",
    "        \"Provide your score as a single integer from 1-5.\"\n",
    "    ),\n",
    "    examples=patient_safety_examples,\n",
    "    version=\"v1\",\n",
    "    model=judge_llm,\n",
    "    parameters={\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1000\n",
    "    },\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "bedrock_judge_metrics = [medical_accuracy, clinical_reasoning, patient_safety]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method performs the qualitative evaluation using `mlflow.evaluate`. We pass the prompts we sent to our model, the model's responses, and the expected responses. The prompts and expected responses come from the [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset, available on HuggingFace. \n",
    "\n",
    "Our model's responses are compared to the expected responses and evaluated using the `EvaluationExample` objects and the grading prompt to determine the qualitative performance of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_qualitatively(model_config, dataset):\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model using LLM-as-a-judge metrics with fallback.\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "    endpoint_name = model_config[\"endpoint\"]\n",
    "    \n",
    "    print(f\"\\nPerforming qualitative evaluation for model: {model_name} on endpoint: {endpoint_name}\")\n",
    "    \n",
    "    predictions = []\n",
    "    questions = []\n",
    "    references = []\n",
    "    inference_times = []\n",
    "    failed_generations = 0\n",
    "    metric_results = {}\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Generating responses for evaluation\"):\n",
    "        question = example[\"Question\"]\n",
    "        reference = \"\\n\".join([example[\"Complex_CoT\"], example[\"Response\"]])\n",
    "        \n",
    "        \n",
    "        payload = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call the model endpoint\n",
    "        try:\n",
    "            response, inference_time = invoke_sagemaker_endpoint(payload)\n",
    "            \n",
    "            if response is None:\n",
    "                prediction = \"Error generating response.\"\n",
    "                failed_generations += 1\n",
    "            elif isinstance(response, list):\n",
    "                prediction = response[0].get('generated_text', '').strip()\n",
    "            elif isinstance(response, dict):\n",
    "                prediction = response.get('generated_text', '').strip()\n",
    "            else:\n",
    "                prediction = str(response).strip()\n",
    "            \n",
    "            prediction = prediction.split(\"<|eot_id|>\")[0] if \"<|eot_id|>\" in prediction else prediction\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "            prediction = \"Error generating response.\"\n",
    "            failed_generations += 1\n",
    "            inference_times.append(-1)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        questions.append(question)\n",
    "        references.append(reference)\n",
    "    \n",
    "    # Log basic generation metrics\n",
    "    mlflow.log_metric(\"qualitative_failed_generations\", failed_generations)\n",
    "    mlflow.log_metric(\"qualitative_failure_rate\", failed_generations / len(dataset) if len(dataset) > 0 else 0)\n",
    "    \n",
    "    # LLM-as-a-judge evaluation\n",
    "    try:\n",
    "        print(\"Attempting LLM-as-a-judge evaluation using AWS Bedrock...\")\n",
    "        \n",
    "        # Prepare data for MLflow evaluation\n",
    "        eval_data = pd.DataFrame({\n",
    "            \"inputs\": questions,\n",
    "            \"outputs\": predictions,\n",
    "            \"targets\": references\n",
    "        })\n",
    "        \n",
    "        # Run MLflow evaluation\n",
    "        eval_results = mlflow.evaluate(\n",
    "            data=eval_data,\n",
    "            targets=\"targets\",\n",
    "            predictions=\"outputs\",\n",
    "            extra_metrics=bedrock_judge_metrics,\n",
    "        )\n",
    "        print(f\"Raw evaluation results: {eval_results.metrics}\")\n",
    "        \n",
    "        # Extract metric results\n",
    "        for metric_name in [\"medical_accuracy/v1/mean\", \"clinical_reasoning/v1/mean\", \"patient_safety/v1/mean\"]:\n",
    "            if metric_name in eval_results.metrics:\n",
    "                base_name = metric_name.split('/')[0]\n",
    "                metric_results[base_name] = eval_results.metrics[metric_name]\n",
    "                if not np.isnan(metric_results[base_name]):\n",
    "                    mlflow.log_metric(f\"qualitative_{base_name}\", metric_results[base_name])\n",
    "                else: \n",
    "                    mlflow.log_metric(f\"qualitative_{base_name}\", 0.0)\n",
    "        \n",
    "        print(\"LLM-as-a-judge evaluation completed successfully!\")\n",
    "        # time.sleep(10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"LLM-as-a-judge evaluation failed: {str(e)}\")\n",
    "       \n",
    "    # Create evaluation summary\n",
    "    evaluation_details = []\n",
    "    for i, (pred, question, ref) in enumerate(zip(predictions[:5], questions[:5], references[:5])):\n",
    "        evaluation_details.append({\n",
    "            \"question\": question,\n",
    "            \"prediction\": pred[:500] + (\"...\" if len(pred) > 500 else \"\"),\n",
    "            \"reference\": ref[:500] + (\"...\" if len(ref) > 500 else \"\"),\n",
    "        })\n",
    "    \n",
    "    # Save detailed results\n",
    "    detailed_df = pd.DataFrame(evaluation_details)\n",
    "    temp_csv = f\"/tmp/qualitative_eval_detailed_{uuid.uuid4().hex[:8]}.csv\"\n",
    "    detailed_df.to_csv(temp_csv, index=False)\n",
    "    mlflow.log_artifact(temp_csv, \"qualitative_evaluation\")\n",
    "    \n",
    "    # Create simple visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metric_names = list(metric_results.keys())\n",
    "    metric_values = list(metric_results.values())\n",
    "    plt.bar(metric_names, metric_values, color=['blue', 'green', 'red', 'orange'])\n",
    "    plt.title('Qualitative Evaluation Scores')\n",
    "    plt.ylabel('Score (1-5)')\n",
    "    plt.ylim(1, 5)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/qualitative_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact('/tmp/qualitative_metrics.png', \"qualitative_evaluation\")\n",
    "    \n",
    "    avg_medical_accuracy = metric_results.get(\"medical_accuracy\", metric_results.get(\"overall_quality\", 3.0))\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name, \n",
    "        \"num_samples\": len(dataset),\n",
    "        \"metrics\": metric_results,\n",
    "        \"evaluation_details\": evaluation_details,\n",
    "        \"avg_medical_accuracy\": avg_medical_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the MLFlow run. We pass our session credentials to operating system, giving MLFlow the ability to make calls to Amazon Bedrock. This is required because we cannot configure MLFlow's connection to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from botocore.config import Config\n",
    "\n",
    "with mlflow.start_run(run_name=\"example_model_evaluation\"):\n",
    "    # Get AWS credentials from the SageMaker execution environment\n",
    "    retry_config = Config(\n",
    "        retries={\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'adaptive'  # or 'legacy', 'adaptive'\n",
    "        }\n",
    "    )\n",
    "    session = boto3.Session()\n",
    "    credentials = session.get_credentials()\n",
    "    \n",
    "    # Set as environment variables\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = credentials.access_key\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = credentials.secret_key\n",
    "    if credentials.token:\n",
    "        os.environ['AWS_SESSION_TOKEN'] = credentials.token\n",
    "    \n",
    "    # Set region - important for Bedrock\n",
    "    region = boto3.session.Session().region_name\n",
    "    os.environ['AWS_REGION'] = region\n",
    "\n",
    "    mlflow.set_tag(\"component\", \"qualitative_model_evaluation\")\n",
    "    \n",
    "    # Initialize the SageMaker client\n",
    "    sm_client = boto3.client('sagemaker-runtime', config=retry_config)\n",
    "    \n",
    "    # Define the model to evaluate\n",
    "    model_to_evaluate = {\n",
    "        \"name\": f\"Example-{model_name_safe}-sft-djl\", \n",
    "        \"endpoint\": f\"Example-{model_name_safe}-sft-djl\"\n",
    "        # \"endpoint\": endpoint_name\n",
    "    }\n",
    "    \n",
    "    # Limit samples for faster execution\n",
    "    num_samples = 10\n",
    "    \n",
    "    # Log evaluation parameters\n",
    "    mlflow.log_param(\"qualitative_evaluation_endpoint\", endpoint_name)\n",
    "    mlflow.log_param(\"qualitative_evaluation_num_samples\", num_samples)\n",
    "    mlflow.log_param(\"qualitative_evaluation_timestamp\", datetime.now().isoformat())\n",
    "    mlflow.log_param(\"llm_judge_model\", judge_llm)\n",
    "    \n",
    "    # Load the test dataset\n",
    "    try:\n",
    "        dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")\n",
    "        max_samples = len(dataset)\n",
    "        dataset = dataset.shuffle().select(range(min(num_samples, max_samples)))\n",
    "        print(f\"Loaded medical-o1-reasoning dataset with {len(dataset)} samples for qualitative evaluation\")\n",
    "        \n",
    "        mlflow.log_param(\"qualitative_dataset_name\", \"FreedomIntelligence/medical-o1-reasoning-SFT\") \n",
    "        mlflow.log_param(\"qualitative_dataset_actual_samples\", len(dataset))\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error loading dataset for qualitative evaluation: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        # Perform qualitative evaluation\n",
    "        qualitative_results = evaluate_model_qualitatively(model_to_evaluate, dataset)\n",
    "        \n",
    "        avg_medical_accuracy = qualitative_results[\"avg_medical_accuracy\"]\n",
    "        \n",
    "        print(f\"\\nQualitative evaluation completed!\")\n",
    "        print(f\"Average Medical Accuracy: {avg_medical_accuracy:.3f}\")\n",
    "        \n",
    "        print(f\"avg_medical_accuracy: {avg_medical_accuracy}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in qualitative model evaluation: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Templating a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next workshop we fine-tune Qwen3-4B-Instruct-2507 to become a medical expert. To accomplish this, we execute a fine-tuning job using Managed MLflow on SageMaker AI. We get our data from the [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset, available on HuggingFace.\n",
    "\n",
    "In this lab, we show a small example of what fine-tuning looks like for a single record of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINE_TUNING_DATA_SAMPLE = {\n",
    "    \"Question\": \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\", \n",
    "    \"Complex_CoT\": \"Okay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem.\\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal.\\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\",\n",
    "    \"Response\": \"Cystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.\"\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\"\"\"\n",
    "\n",
    "def convert_to_messages(sample, system_prompt=\"\"):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": sample[\"Question\"]},\n",
    "        {\"role\": \"assistant\", \n",
    "         \"content\": \n",
    "         f\"{sample[\"Complex_CoT\"]}\\n\\n{sample[\"Response\"]}\"}\n",
    "    ]\n",
    "\n",
    "    sample[\"messages\"] = messages\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "PROCESSED_SAMPLE = convert_to_messages(FINE_TUNING_DATA_SAMPLE)\n",
    "print(PROCESSED_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-Tuning Output\n",
    "The above output shows the templated prompt output to be used for fine-tuning. This pre-processing happens for every record in the fine-tuning dataset before fine-tuning actually takes place. This can be time-consuming for large fine-tuning datasets. We will show in the next lab how to orchestrate this with MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint_with_retry(endpoint_name, max_retries=3, wait_seconds=10):\n",
    "    \"\"\"\n",
    "    Delete a SageMaker endpoint with retry logic\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): Name of the SageMaker endpoint to delete\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        wait_seconds (int): Time to wait between retries in seconds\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if deletion was successful, False otherwise\n",
    "    \"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # First check if the endpoint exists\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        endpoint_exists = True\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if \"Could not find endpoint\" in str(e):\n",
    "            print(f\"Endpoint {endpoint_name} does not exist, no cleanup needed.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error checking endpoint existence: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # If we get here, the endpoint exists and we should delete it\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempting to delete endpoint {endpoint_name} (attempt {attempt + 1}/{max_retries})\")\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "            print(f\"Endpoint {endpoint_name} deletion initiated successfully\")\n",
    "            \n",
    "            # Wait for endpoint to be fully deleted\n",
    "            print(\"Waiting for endpoint to be fully deleted...\")\n",
    "            \n",
    "            # Poll until endpoint is deleted or max wait time is reached\n",
    "            total_wait_time = 0\n",
    "            max_wait_time = 300  # 5 minutes maximum wait\n",
    "            while total_wait_time < max_wait_time:\n",
    "                try:\n",
    "                    sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "                    print(f\"Endpoint still exists, waiting {wait_seconds} seconds...\")\n",
    "                    time.sleep(wait_seconds)\n",
    "                    total_wait_time += wait_seconds\n",
    "                except botocore.exceptions.ClientError:\n",
    "                    print(f\"Endpoint {endpoint_name} successfully deleted\")\n",
    "                    return True\n",
    "            \n",
    "            # If we get here, the endpoint still exists after max_wait_time\n",
    "            print(f\"Warning: Endpoint deletion initiated but still exists after {max_wait_time} seconds\")\n",
    "            return False\n",
    "            \n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if \"ResourceInUse\" in str(e) or \"ResourceNotFound\" in str(e):\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "                print(f\"Retrying in {wait_seconds} seconds...\")\n",
    "                time.sleep(wait_seconds)\n",
    "            else:\n",
    "                print(f\"Unexpected error deleting endpoint: {e}\")\n",
    "                return False\n",
    "    \n",
    "    print(f\"Failed to delete endpoint {endpoint_name} after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "# Clean up endpoint\n",
    "try:\n",
    "    print(f\"Cleaning up endpoint: {endpoint_name}\")\n",
    "    if delete_endpoint_with_retry(endpoint_name):\n",
    "        print(\"Cleanup completed successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Endpoint cleanup may have failed, please check the SageMaker console\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during endpoint cleanup: {str(e)}\")\n",
    "    print(\"You may need to manually delete the endpoint from the SageMaker console\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "In this notebook, we illustrated the building blocks for a fine-tuned LLM-deployment pipeline. We showed:\n",
    "\n",
    "1. How to prepare data for a fine-tuning job\n",
    "2. How to deploy a model to a SageMaker AI Managed Endpoint\n",
    "3. How to evaluate a model's performance\n",
    "4. Creating and applying Guardrails to our model\n",
    "5. Tracing model calls using MLFlow tracing\n",
    "\n",
    "Next, we show how to actually perform fine-tuning on this Qwen3 model to improve the model's performance in this domain. Moreover, we'll orchestrate all of these steps into a fine-tuning pipeline powered by Managed MLFlow and SageMaker AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
