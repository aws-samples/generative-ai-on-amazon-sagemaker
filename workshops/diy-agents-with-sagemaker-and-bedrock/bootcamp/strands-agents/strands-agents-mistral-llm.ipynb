{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a520cf-9179-4043-8a25-8529ecc98ee6",
   "metadata": {},
   "source": [
    "# Running Strands Agents with Sagemaker Endpoint using Mistral LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7321015-e901-4e68-bf20-5c7cfdebd3fc",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "We will use the Strands Agent SDK to query SageMaker AI Inference Endpoints. We use the LLM `Mistral-Small-24B-Instruct-2501` from Sagemaker JumpStart model hub. We create a Strands agent and use it to invoke a previously created inference component.\n",
    "\n",
    "Inference Components are a feature of SageMakerAI announced at re:Invent 2023. Inference Components allow models to be deployed and scaled independent of their hosting infrastructure. They are a more efficient way to use the hardware that hosts GPU-accelerated models. We can deploy the Mistral model we just registered to an Inference Component on our host using the below code.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To use SageMaker AI endpoints in these examples, you will need to first deploy a managed endpoint. In this example, you will leverage an already-deployed endpoint running Mistral LLM on Sagemaker AI. Below, you will create and use a Strands Agent to invoke the Mistral LLM, and use the agent code to reason about math.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\u26a0\ufe0f <b>Important:</b> (1) Make sure you've run the <code>0-setup/1-required-dependencies-strands.ipynb</code> notebook before proceeding. If you haven't, close this notebook, run that notebook first, then come back here.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\u26a0\ufe0f <b>Important:</b> (2) To use <b>Amazon SageMaker AI</b> for running the Inference Endpoint, make sure you've run the <code>0-setup/2-setup-mistral-sagemaker-endpoint.ipynb</code> notebook before proceeding. If you haven't, close this notebook, run that notebook first, then come back here.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b607abc-2377-45c2-882d-a6b32d9107c9",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f592f-0199-4863-a1c8-0c494f563357",
   "metadata": {},
   "source": [
    "### Run this cell to make sure the Strands Agent libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6136ff-d4c4-4522-bf47-059095eb8d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:17:49.418610Z",
     "iopub.status.busy": "2025-10-23T19:17:49.418291Z",
     "iopub.status.idle": "2025-10-23T19:17:50.826194Z",
     "shell.execute_reply": "2025-10-23T19:17:50.825519Z",
     "shell.execute_reply.started": "2025-10-23T19:17:49.418585Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip show strands-agents strands-agents-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad77170-f9e6-4693-8d7f-2ddfbc7e0c68",
   "metadata": {},
   "source": [
    "### If Strands Agents libraries do not show above, then install them by running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d9d50-df62-4eac-9ca5-f15e53d578a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment line below to run pip install\n",
    "# %pip install 'strands-agents[sagemaker]' strands-agents-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2d56d-6ee9-4892-ac90-ad947cc53926",
   "metadata": {},
   "source": [
    "### Restore names of Endpoint, Endpoint Config, and Inference Component\n",
    "\n",
    "Previously run notebook should have stored these variables into local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4041914-1db7-4379-bbcb-bbafd92726ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:17:55.193156Z",
     "iopub.status.busy": "2025-10-23T19:17:55.192788Z",
     "iopub.status.idle": "2025-10-23T19:17:55.199145Z",
     "shell.execute_reply": "2025-10-23T19:17:55.198569Z",
     "shell.execute_reply.started": "2025-10-23T19:17:55.193129Z"
    }
   },
   "outputs": [],
   "source": [
    "%store -r MISTRAL_ENDPOINT_NAME\n",
    "print(f\"Endpoint name: {MISTRAL_ENDPOINT_NAME}\")\n",
    "\n",
    "%store -r MISTRAL_ENDPOINT_CONFIG_NAME\n",
    "print(f\"Endpoint Config Name: {MISTRAL_ENDPOINT_CONFIG_NAME}\")\n",
    "\n",
    "%store -r MISTRAL_INFERENCE_COMPONENT_NAME\n",
    "print(f\"Inference Component Name: {MISTRAL_INFERENCE_COMPONENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b7db0-3116-4963-af9e-b5de8777cbc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:17:57.950206Z",
     "iopub.status.busy": "2025-10-23T19:17:57.949879Z",
     "iopub.status.idle": "2025-10-23T19:17:59.828217Z",
     "shell.execute_reply": "2025-10-23T19:17:59.827621Z",
     "shell.execute_reply.started": "2025-10-23T19:17:57.950185Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Setup role and sagemaker session\n",
    "iam_role = get_execution_role()\n",
    "boto_session = boto3.Session(region_name='us-west-2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19296faf-26f1-44fd-9faf-9ba5dd9a8fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:39:31.994160Z",
     "iopub.status.busy": "2025-09-10T18:39:31.993848Z",
     "iopub.status.idle": "2025-09-10T18:39:32.003937Z",
     "shell.execute_reply": "2025-09-10T18:39:32.003289Z",
     "shell.execute_reply.started": "2025-09-10T18:39:31.994139Z"
    }
   },
   "source": [
    "## Create Strands Agent and Sagemaker AI Model\n",
    "\n",
    "First, we create an instance of **SageMakerAIModel** based on the Mistral LLM endpoint previously deployed. \n",
    "Next, we create a Strand Agent that wraps that model and allows us to submit queries.\n",
    "\n",
    "More info: [see Strands Sagemaker Docs](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/sagemaker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5c860-b3b5-4a5a-89eb-533ef70c3cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:18:20.935239Z",
     "iopub.status.busy": "2025-10-23T19:18:20.934724Z",
     "iopub.status.idle": "2025-10-23T19:18:21.593629Z",
     "shell.execute_reply": "2025-10-23T19:18:21.593149Z",
     "shell.execute_reply.started": "2025-10-23T19:18:20.935216Z"
    }
   },
   "outputs": [],
   "source": [
    "import strands\n",
    "from strands import Agent\n",
    "from strands.models.sagemaker import SageMakerAIModel\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.getLogger(\"strands\").setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s | %(name)s | %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "model = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        'endpoint_name': MISTRAL_ENDPOINT_NAME,\n",
    "        'region_name': 'us-west-2',\n",
    "        'inference_component_name': MISTRAL_INFERENCE_COMPONENT_NAME,\n",
    "    },\n",
    "    payload_config={\n",
    "        'max_tokens': 4000,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.9,\n",
    "        'stream': False\n",
    "    },\n",
    "    boto_session=boto_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d94ed0-bce1-43e4-9e6f-e1940d625d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:24:49.616038Z",
     "iopub.status.busy": "2025-10-23T19:24:49.615457Z",
     "iopub.status.idle": "2025-10-23T19:24:49.619214Z",
     "shell.execute_reply": "2025-10-23T19:24:49.618690Z",
     "shell.execute_reply.started": "2025-10-23T19:24:49.616012Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant capable of explaining physics concepts.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the basics of Einstein's Special Theory of Relativity. Also explain how it was proven via actual measurements.\"}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 4000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329d2cd-597b-45af-ad33-fd364205f401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:24:53.697655Z",
     "iopub.status.busy": "2025-10-23T19:24:53.697325Z",
     "iopub.status.idle": "2025-10-23T19:25:14.402253Z",
     "shell.execute_reply": "2025-10-23T19:25:14.401636Z",
     "shell.execute_reply.started": "2025-10-23T19:24:53.697634Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=model,\n",
    "    system_prompt=messages[0][\"content\"]\n",
    ")\n",
    "\n",
    "result = agent(messages[1][\"content\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d8390-2618-416d-813e-5f20d3271f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}