{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586f2662-a568-4c29-af10-7bcb2eadbf30",
   "metadata": {},
   "source": [
    "# Deploying an LLM to Amazon SageMaker AI real-time endpoint\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To use SageMaker AI endpoints in these examples, you will need to first deploy a managed endpoint. In this example you will deploy an endpoint through SageMaker Jumpstart, a feature that helps machine learning practitioners quickly get started with hundreds of production-ready models in SageMaker AI.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\u26a0\ufe0f <b>Important:</b> Make sure you've run the <code>1-required-dependencies-strands.ipynb</code> notebook in this folder before proceeding. If you haven't, close this notebook, run the previous one first, then come back to this.\n",
    "</div>\n",
    "\n",
    "## Create Sagemaker Endpoint\n",
    "\n",
    "In this notebook, we first create an endpoint config that defines parameters for the endpoint. Then we specify an inference component that will field our requests. This component will create and kick-off the Sagemaker endpoint after some delay.\n",
    "\n",
    "**NOTE**: In order to deploy a Sagemaker Endpoint on a larger instance type, such as `ml.g5.48xlarge`, you may have to request a quota increase for your account. Please refer to AWS documentation on how to [increase quotas](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49d687-9592-4d6d-98e6-0e3a191f9fa1",
   "metadata": {},
   "source": [
    "### Run this cell to make sure the Strands Agent libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce612f-74cb-4c44-9483-a019bfa006d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:57:12.027566Z",
     "iopub.status.busy": "2025-10-23T18:57:12.027229Z",
     "iopub.status.idle": "2025-10-23T18:57:13.471878Z",
     "shell.execute_reply": "2025-10-23T18:57:13.471110Z",
     "shell.execute_reply.started": "2025-10-23T18:57:12.027543Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip show strands-agents strands-agents-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c2bba-157b-49f5-9cf0-aaacf8f6347a",
   "metadata": {},
   "source": [
    "### If Strands Agents libraries do not show above, then install them by running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3530b42-c71f-4d70-8856-60f8e24603e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:57:13.473416Z",
     "iopub.status.busy": "2025-10-23T18:57:13.473098Z",
     "iopub.status.idle": "2025-10-23T18:57:13.476502Z",
     "shell.execute_reply": "2025-10-23T18:57:13.475865Z",
     "shell.execute_reply.started": "2025-10-23T18:57:13.473393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment line below to run pip install\n",
    "# %pip install 'strands-agents[sagemaker]' strands-agents-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e0da6-0c35-484b-85dc-0510d0742b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:57:13.477542Z",
     "iopub.status.busy": "2025-10-23T18:57:13.477255Z",
     "iopub.status.idle": "2025-10-23T18:57:15.551862Z",
     "shell.execute_reply": "2025-10-23T18:57:15.551207Z",
     "shell.execute_reply.started": "2025-10-23T18:57:13.477526Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from sagemaker.core.helper.session_helper import get_execution_role\n",
    "\n",
    "# Setup role and sagemaker session\n",
    "iam_role = get_execution_role()\n",
    "boto_session = boto3.Session(region_name='us-west-2')\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\") \n",
    "\n",
    "# Names for endpoint config and endpoint\n",
    "endpointName='strands-endpoint-001'\n",
    "endpointConfigName='strands-endpoint-config'\n",
    "inferenceComponentName='mistral-24b-instruct-2501-ic'\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bdb040-cb4d-464c-9c58-58c938e5e283",
   "metadata": {},
   "source": [
    "## Delete Resources if they already exist\n",
    "\n",
    "**Note**: If you re-run this notebook, or see failures to create resources, run the next 3 cells to delete previously created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24591a94-1ff5-4d12-a5ad-f32f75ad84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to delete inference component (if exists)\n",
    "sagemaker_client.delete_inference_component(InferenceComponentName=inferenceComponentName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f59cb0-e525-4d3b-b2f7-faa551fa967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line if the endpoint already exists\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpointName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185be4a-8252-4951-a0c0-de9de0505112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line if the endpoint config already exists\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpointConfigName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8cfd4-b13a-4e4b-8dc3-48150f783536",
   "metadata": {},
   "source": [
    "## Create Endpoint Config, Endpoint, and Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0587aa-3647-44b0-a29b-9ce65239c05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:57:51.765826Z",
     "iopub.status.busy": "2025-10-23T18:57:51.765154Z",
     "iopub.status.idle": "2025-10-23T18:57:52.135235Z",
     "shell.execute_reply": "2025-10-23T18:57:52.134651Z",
     "shell.execute_reply.started": "2025-10-23T18:57:51.765801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create endpoint config\n",
    "\n",
    "endpoint_config = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpointConfigName,\n",
    "    EnableNetworkIsolation=False,\n",
    "    ExecutionRoleArn=iam_role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": 'AllTraffic',\n",
    "            \"InstanceType\": 'ml.g5.48xlarge',\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": 0,\n",
    "                \"MaxInstanceCount\": 2,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f574c2-6b35-4dc1-aeab-b35cf7214794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:57:58.123249Z",
     "iopub.status.busy": "2025-10-23T18:57:58.122746Z",
     "iopub.status.idle": "2025-10-23T18:57:58.634838Z",
     "shell.execute_reply": "2025-10-23T18:57:58.634214Z",
     "shell.execute_reply.started": "2025-10-23T18:57:58.123226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create endpoint\n",
    "endpoint = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpointName,\n",
    "    EndpointConfigName=endpointConfigName\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a7f7c-429b-47e4-9cdc-b2b57821a7c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:58:05.686983Z",
     "iopub.status.busy": "2025-10-23T18:58:05.686464Z",
     "iopub.status.idle": "2025-10-23T18:58:07.044029Z",
     "shell.execute_reply": "2025-10-23T18:58:07.043228Z",
     "shell.execute_reply.started": "2025-10-23T18:58:05.686961Z"
    }
   },
   "outputs": [],
   "source": [
    "# SDK v3: Using boto3 to create model from JumpStart artifacts\n",
    "# Get JumpStart model artifacts using SageMaker API\n",
    "import time\n",
    "\n",
    "# Mistral Small 24B model specs\n",
    "model_id = \"huggingface-llm-mistral-small-24B-Instruct-2501\"\n",
    "region = boto_session.region_name\n",
    "\n",
    "# Get the JumpStart model specs\n",
    "js_client = boto_session.client('sagemaker')\n",
    "\n",
    "# For JumpStart models in SDK v3, we retrieve the model package and create directly\n",
    "# Using the HuggingFace TGI container for Mistral\n",
    "tgi_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.4.0-tgi2.4.1-gpu-py311-cu124-ubuntu22.04\"\n",
    "\n",
    "model_name = f\"mistral-small-24b-{int(time.time())}\"\n",
    "\n",
    "env = {\n",
    "    'ENDPOINT_SERVER_TIMEOUT': '600',\n",
    "    'SM_NUM_GPUS': json.dumps(4),\n",
    "    'MESSAGES_API_ENABLED': 'true',\n",
    "    'HF_MODEL_ID': 'mistralai/Mistral-Small-24B-Instruct-2501',\n",
    "    'MAX_INPUT_LENGTH': '4096',\n",
    "    'MAX_TOTAL_TOKENS': '8192',\n",
    "}\n",
    "\n",
    "# Create model using boto3\n",
    "response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': tgi_image,\n",
    "        'Environment': env,\n",
    "    },\n",
    "    ExecutionRoleArn=iam_role,\n",
    "    EnableNetworkIsolation=False,\n",
    ")\n",
    "print(f\"Created model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca4816-91d8-4ce0-874a-96ba082585fa",
   "metadata": {},
   "source": [
    "### WAIT up to 5 minutes before creating Inference Component\n",
    "\n",
    "Typically, we need to wait up to 5 minutes for Endpoint creation before creating an Inference Component\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43aae8f-502f-4ab0-a4fc-fe8fbcbcdd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:08:58.723395Z",
     "iopub.status.busy": "2025-10-23T19:08:58.722664Z",
     "iopub.status.idle": "2025-10-23T19:08:59.327823Z",
     "shell.execute_reply": "2025-10-23T19:08:59.327222Z",
     "shell.execute_reply.started": "2025-10-23T19:08:58.723357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create inference component\n",
    "\n",
    "mistral_7Bv03_ic = sagemaker_client.create_inference_component(\n",
    "    EndpointName=endpointName,\n",
    "    InferenceComponentName=inferenceComponentName,\n",
    "    RuntimeConfig={\n",
    "        'CopyCount': 1\n",
    "    },\n",
    "    Specification={\n",
    "        'ModelName': model_name,\n",
    "        'StartupParameters': {\n",
    "            'ModelDataDownloadTimeoutInSeconds': 3600,\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 3600\n",
    "        },\n",
    "        'ComputeResourceRequirements': {\n",
    "            'MinMemoryRequiredInMb': 1024,\n",
    "            'NumberOfAcceleratorDevicesRequired': 4,\n",
    "        }\n",
    "    },\n",
    "    Tags=[{\n",
    "        'Key': 'Usage',\n",
    "        'Value': 'Strands Agents'\n",
    "    }],\n",
    "    VariantName=\"AllTraffic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7440010-016b-4d22-9532-a35426ce2c86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\u26a0\ufe0f <b>Note:</b> Deployment of the Sagemaker Endpoint Inference Component can take 5~10 minutes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1af96d-c8ff-4956-8272-c2bd9b6914cd",
   "metadata": {},
   "source": [
    "## Must WAIT up to 10 minutes for Endpoint AutoScaling to complete\n",
    "\n",
    "**Notice**: If you get this error message below, you must WAIT for the endpoint to complete it's auto-scaling\n",
    "\n",
    "`ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component\n",
    "has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to \n",
    "increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`\n",
    "\n",
    "If the cell below fails, you can simply retry after waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08268e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:17:30.026103Z",
     "iopub.status.busy": "2025-10-23T19:17:30.025372Z",
     "iopub.status.idle": "2025-10-23T19:17:32.901765Z",
     "shell.execute_reply": "2025-10-23T19:17:32.901217Z",
     "shell.execute_reply.started": "2025-10-23T19:17:30.026075Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant capable of performing math calculations.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Calculate the square root of 81. Briefly explain the significance of the result.\"}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 4000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "response_model = sagemaker_runtime.invoke_endpoint(\n",
    "    InferenceComponentName=inferenceComponentName,\n",
    "    EndpointName=endpointName,\n",
    "    Body=json.dumps({\n",
    "      \"messages\": messages,\n",
    "      \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 256,\n",
    "      }\n",
    "    }),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "response = json.loads(response_model['Body'].read().decode('utf-8'))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a39f67-2169-4251-abca-49a9c17614fa",
   "metadata": {},
   "source": [
    "### Save names of Endpoint, Endpoint Config, and Inference Component\n",
    "\n",
    "We save various attributes like endpoint name and inference component name using `store` magic command, as they will be needed in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6de49-11f7-4e36-b7bb-322282a51e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T19:17:39.004479Z",
     "iopub.status.busy": "2025-10-23T19:17:39.003837Z",
     "iopub.status.idle": "2025-10-23T19:17:39.009923Z",
     "shell.execute_reply": "2025-10-23T19:17:39.009369Z",
     "shell.execute_reply.started": "2025-10-23T19:17:39.004457Z"
    }
   },
   "outputs": [],
   "source": [
    "MISTRAL_ENDPOINT_NAME = endpointName\n",
    "MISTRAL_ENDPOINT_CONFIG_NAME = endpointConfigName\n",
    "MISTRAL_INFERENCE_COMPONENT_NAME = inferenceComponentName\n",
    "\n",
    "print(f\"Endpoint name: {MISTRAL_ENDPOINT_NAME}\")\n",
    "print(f\"Endpoint Config Name: {MISTRAL_ENDPOINT_CONFIG_NAME}\")\n",
    "print(f\"Inference Component Name: {MISTRAL_INFERENCE_COMPONENT_NAME}\")\n",
    "\n",
    "%store MISTRAL_ENDPOINT_NAME\n",
    "%store MISTRAL_ENDPOINT_CONFIG_NAME\n",
    "%store MISTRAL_INFERENCE_COMPONENT_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf1157-7f7e-4926-aa35-f78f66b1f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}