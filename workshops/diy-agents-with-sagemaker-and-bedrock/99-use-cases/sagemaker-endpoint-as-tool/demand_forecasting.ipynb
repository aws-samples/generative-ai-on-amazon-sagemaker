{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Forecasting with XGBoost on Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Prepare a synthetic demand forecasting dataset\n",
    "2. Train an XGBoost model on Amazon SageMaker using **SDK v3 (ModelTrainer)**\n",
    "3. Deploy the model to a SageMaker endpoint for real-time inference\n",
    "\n",
    "The workflow includes:\n",
    "- Generating synthetic time series data with trend, seasonality, and noise\n",
    "- Feature engineering for time series forecasting\n",
    "- Training an XGBoost model on SageMaker with the new ModelTrainer API\n",
    "- Deploying the model to a SageMaker endpoint\n",
    "- Testing the endpoint with sample data\n",
    "\n",
    "> **Note:** This notebook uses SageMaker Python SDK v3, which introduces the unified `ModelTrainer` class for training workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import InputData, Compute, OutputDataConfig\n",
    "from sagemaker.core.helper.session_helper import Session as SMSession, get_execution_role\n",
    "from sagemaker.core.image_uris import retrieve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json as json_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session (SDK v3)\n",
    "sagemaker_session = SMSession()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except:\n",
    "    role = input(\"> Enter the role ARN for your SageMaker execution role: \")\n",
    "    role = role.strip()\n",
    "\n",
    "print(f\"SageMaker session initialized in region: {region}\")\n",
    "print(f\"Using S3 bucket: {bucket}\")\n",
    "print(f\"Using IAM role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Demand Data\n",
    "\n",
    "We'll create synthetic time series data with trend, seasonality, and noise components to simulate demand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demand_data(n_periods=365*2, seasonality=True, trend=True, noise_level=0.2):\n",
    "    \"\"\"Generate synthetic demand data with trend, seasonality, and noise.\"\"\"\n",
    "    time_idx = np.arange(n_periods)\n",
    "    \n",
    "    # Base demand\n",
    "    base_demand = 100\n",
    "    \n",
    "    # Add trend component\n",
    "    trend_component = 0\n",
    "    if trend:\n",
    "        trend_component = time_idx * 0.1\n",
    "    \n",
    "    # Add seasonality component\n",
    "    seasonality_component = 0\n",
    "    if seasonality:\n",
    "        # Weekly seasonality\n",
    "        weekly = 10 * np.sin(2 * np.pi * time_idx / 7)\n",
    "        # Monthly seasonality\n",
    "        monthly = 20 * np.sin(2 * np.pi * time_idx / 30)\n",
    "        # Yearly seasonality\n",
    "        yearly = 50 * np.sin(2 * np.pi * time_idx / 365)\n",
    "        \n",
    "        seasonality_component = weekly + monthly + yearly\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, noise_level * base_demand, n_periods)\n",
    "    \n",
    "    # Combine components\n",
    "    demand = base_demand + trend_component + seasonality_component + noise\n",
    "    \n",
    "    # Ensure no negative values\n",
    "    demand = np.maximum(demand, 0)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    dates = pd.date_range(start='2021-01-01', periods=n_periods)\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'demand': demand\n",
    "    })\n",
    "    \n",
    "    # Add date features\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating synthetic demand forecasting data...\")\n",
    "df = generate_demand_data()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "Let's visualize the synthetic demand data to understand its patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    \"\"\"Visualize the demand data.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['date'], df['demand'])\n",
    "    plt.title('Synthetic Demand Data')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Visualize data\n",
    "visualize_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create lag and rolling window features for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Create lag and rolling features for time series forecasting.\"\"\"\n",
    "    # Create lag features (previous demand values)\n",
    "    for lag in [1, 7, 14, 30]:\n",
    "        df[f'lag_{lag}'] = df['demand'].shift(lag)\n",
    "\n",
    "    # Create rolling window features\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'rolling_mean_{window}'] = df['demand'].rolling(window=window).mean()\n",
    "        df[f'rolling_std_{window}'] = df['demand'].rolling(window=window).std()\n",
    "\n",
    "    # Drop rows with NaN values (due to lag and rolling features)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Prepare features\n",
    "df = prepare_features(df)\n",
    "print(f\"Dataset shape after feature engineering: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"Split data into features and target, then into train/val/test sets.\"\"\"\n",
    "    # Define features and target\n",
    "    features = [col for col in df.columns if col not in ['date', 'demand']]\n",
    "    X = df[features]\n",
    "    y = df['demand']\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(df)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to S3\n",
    "\n",
    "Save the datasets to CSV files and upload them to S3 for SageMaker training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_s3(X_train, y_train, X_val, y_val, X_test, y_test, bucket):\n",
    "    \"\"\"Save datasets to CSV and upload to S3.\"\"\"\n",
    "    # Create local directories for data\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "\n",
    "    # Save datasets to CSV\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    val_data = pd.concat([X_val, y_val], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_data.to_csv('data/train.csv', index=False, header=False)\n",
    "    val_data.to_csv('data/validation.csv', index=False, header=False)\n",
    "    test_data.to_csv('data/test.csv', index=False, header=False)\n",
    "\n",
    "    # Upload to S3 using boto3\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file('data/train.csv', bucket, 'demand-forecast/data/train.csv')\n",
    "    s3_client.upload_file('data/validation.csv', bucket, 'demand-forecast/data/validation.csv')\n",
    "    s3_client.upload_file('data/test.csv', bucket, 'demand-forecast/data/test.csv')\n",
    "    \n",
    "    train_s3_path = f's3://{bucket}/demand-forecast/data/train.csv'\n",
    "    val_s3_path = f's3://{bucket}/demand-forecast/data/validation.csv'\n",
    "    test_s3_path = f's3://{bucket}/demand-forecast/data/test.csv'\n",
    "    \n",
    "    return train_s3_path, val_s3_path, test_s3_path\n",
    "\n",
    "# Save data to S3\n",
    "train_s3_path, val_s3_path, test_s3_path = save_data_to_s3(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, bucket\n",
    ")\n",
    "print(f\"Training data uploaded to: {train_s3_path}\")\n",
    "print(f\"Validation data uploaded to: {val_s3_path}\")\n",
    "print(f\"Test data uploaded to: {test_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Model on SageMaker\n",
    "\n",
    "Define and train an XGBoost model using SageMaker's training infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_s3_path, val_s3_path, role, bucket, region, sagemaker_session, instance_type='ml.m5.large'):\n",
    "    \"\"\"Train XGBoost model on SageMaker using SDK v3 ModelTrainer.\"\"\"\n",
    "    # Get the XGBoost container image URI\n",
    "    xgboost_image_uri = retrieve(\n",
    "        framework='xgboost',\n",
    "        region=region,\n",
    "        version='1.7-1'\n",
    "    )\n",
    "    \n",
    "    # Define XGBoost hyperparameters (must be strings for built-in algorithm)\n",
    "    hyperparameters = {\n",
    "        'max_depth': '6',\n",
    "        'eta': '0.2',\n",
    "        'gamma': '4',\n",
    "        'min_child_weight': '6',\n",
    "        'subsample': '0.8',\n",
    "        'objective': 'reg:squarederror',\n",
    "        'num_round': '100',\n",
    "        'verbosity': '1'\n",
    "    }\n",
    "    \n",
    "    # Configure compute\n",
    "    compute_config = Compute(\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=30,\n",
    "        keep_alive_period_in_seconds=3600\n",
    "    )\n",
    "    \n",
    "    # Create ModelTrainer (SDK v3 pattern)\n",
    "    trainer = ModelTrainer(\n",
    "        training_image=xgboost_image_uri,\n",
    "        hyperparameters=hyperparameters,\n",
    "        role=role,\n",
    "        compute=compute_config,\n",
    "        base_job_name='demand-forecast-xgb',\n",
    "        output_data_config=OutputDataConfig(s3_output_path=f's3://{bucket}/demand-forecast/output'),\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # Define training inputs\n",
    "    train_input = InputData(\n",
    "        channel_name='train',\n",
    "        data_source=train_s3_path,\n",
    "        content_type='text/csv'\n",
    "    )\n",
    "    val_input = InputData(\n",
    "        channel_name='validation', \n",
    "        data_source=val_s3_path,\n",
    "        content_type='text/csv'\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training XGBoost model with SageMaker SDK v3...\")\n",
    "    trainer.train(\n",
    "        input_data_config=[train_input, val_input],\n",
    "        wait=True,\n",
    "        logs=True\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Train model (Note: This cell will take some time to execute)\n",
    "instance_type = 'ml.m5.large'\n",
    "trainer = train_model(train_s3_path, val_s3_path, role, bucket, region, sagemaker_session, instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Deploy the trained model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Model as SageMakerModel, EndpointConfig, Endpoint\n",
    "from sagemaker.core.shapes import ContainerDefinition, ProductionVariant\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def deploy_model(trainer, role, region, bucket, instance_type='ml.m5.xlarge'):\n",
    "    \"\"\"Deploy model to SageMaker endpoint using SDK v3.\"\"\"\n",
    "    print(\"Deploying model to SageMaker endpoint...\")\n",
    "    \n",
    "    # Get model artifacts from training job\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    latest_job = sm_client.list_training_jobs(\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=1,\n",
    "        NameContains='demand-forecast'\n",
    "    )['TrainingJobSummaries'][0]\n",
    "    \n",
    "    job_desc = sm_client.describe_training_job(TrainingJobName=latest_job['TrainingJobName'])\n",
    "    model_data = job_desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Get XGBoost inference container\n",
    "    xgboost_image_uri = retrieve(\n",
    "        framework='xgboost',\n",
    "        region=region,\n",
    "        version='1.7-1'\n",
    "    )\n",
    "    \n",
    "    # Create unique names\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    model_name = f'demand-forecast-mdl-{timestamp}'\n",
    "    endpoint_name = 'ml-models-as-tools'\n",
    "    \n",
    "    # Delete existing endpoint if exists\n",
    "    try:\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"Deleted existing endpoint: {endpoint_name}\")\n",
    "        time.sleep(30)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create container definition\n",
    "    container = ContainerDefinition(\n",
    "        image=xgboost_image_uri,\n",
    "        model_data_url=model_data,\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    sm_model = SageMakerModel.create(\n",
    "        model_name=model_name,\n",
    "        primary_container=container,\n",
    "        execution_role_arn=role,\n",
    "    )\n",
    "    print(f\"Created model: {model_name}\")\n",
    "    \n",
    "    # Create endpoint config\n",
    "    endpoint_config = EndpointConfig.create(\n",
    "        endpoint_config_name=endpoint_name,\n",
    "        production_variants=[\n",
    "            ProductionVariant(\n",
    "                variant_name='AllTraffic',\n",
    "                model_name=model_name,\n",
    "                initial_instance_count=1,\n",
    "                instance_type=instance_type,\n",
    "                initial_variant_weight=1.0,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create endpoint\n",
    "    endpoint = Endpoint.create(\n",
    "        endpoint_name=endpoint_name,\n",
    "        endpoint_config_name=endpoint_name,\n",
    "    )\n",
    "    print(f\"Creating endpoint: {endpoint_name}\")\n",
    "    print(\"Waiting for endpoint to be InService...\")\n",
    "    \n",
    "    # Wait for endpoint\n",
    "    while True:\n",
    "        status = sm_client.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n",
    "        print(f\"  Status: {status}\")\n",
    "        if status == 'InService':\n",
    "            print(\"Endpoint is ready!\")\n",
    "            break\n",
    "        elif status == 'Failed':\n",
    "            raise Exception(\"Endpoint deployment failed\")\n",
    "        time.sleep(30)\n",
    "    \n",
    "    return endpoint_name\n",
    "\n",
    "# Deploy model\n",
    "deploy_instance_type = 'ml.m5.xlarge'\n",
    "endpoint_name = deploy_model(trainer, role, region, bucket, deploy_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Endpoint\n",
    "\n",
    "Test the deployed endpoint with sample data from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sample = X_test.iloc[:5].values.tolist()\n",
    "actual = y_test.iloc[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "\n",
    "def generate_prediction_with_boto3(endpoint_name, test_sample):\n",
    "    sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(test_sample),\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\"\n",
    "    )\n",
    "    predictions = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    return np.array(predictions)\n",
    "\n",
    "generate_prediction_with_boto3(\"ml-models-as-tools\", test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_endpoint(endpoint_name, test_sample, actual):\n",
    "    \"\"\"Test the endpoint with sample data using boto3.\"\"\"\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Make predictions using boto3\n",
    "    print(\"Testing the endpoint with sample data...\")\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='text/csv',\n",
    "        Accept='application/json',\n",
    "        Body=','.join(map(str, test_sample[0]))  # Send first sample as CSV\n",
    "    )\n",
    "    \n",
    "    # For multiple samples, send each separately\n",
    "    predictions = []\n",
    "    for sample in test_sample:\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='text/csv',\n",
    "            Accept='application/json',\n",
    "            Body=','.join(map(str, sample))\n",
    "        )\n",
    "        pred = json_module.loads(response['Body'].read().decode())\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predicted = np.array(predictions)\n",
    "\n",
    "    # Compare predictions with actual values\n",
    "    print(\"Sample predictions:\")\n",
    "    for i in range(len(actual)):\n",
    "        print(f\"Actual: {actual[i]:.2f}, Predicted: {predicted[i]:.2f}\")\n",
    "\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "# Test endpoint\n",
    "mae, rmse = test_endpoint(endpoint_name, test_sample, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions\n",
    "\n",
    "Let's visualize the predictions against actual values for a larger portion of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for more test samples using boto3\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "n_samples = 50  # Number of samples to predict\n",
    "test_samples = X_test.iloc[:n_samples].values.tolist()\n",
    "\n",
    "predictions = []\n",
    "for sample in test_samples:\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='text/csv',\n",
    "        Accept='application/json',\n",
    "        Body=','.join(map(str, sample))\n",
    "    )\n",
    "    pred = json_module.loads(response['Body'].read().decode())\n",
    "    predictions.append(pred)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actual = y_test.iloc[:n_samples].values\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(actual)), actual, label='Actual')\n",
    "plt.plot(range(len(predictions)), predictions, label='Predicted')\n",
    "plt.title('Actual vs Predicted Demand')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Demand')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Resources (Optional)\n",
    "\n",
    "Delete the endpoint to avoid incurring charges when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to delete the endpoint\n",
    "# WARNING: This will delete the endpoint!\n",
    "\n",
    "# sm_client = boto3.client('sagemaker')\n",
    "# print(\"Cleaning up: deleting endpoint...\")\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# print(\"Endpoint deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "1. Generate synthetic demand data with trend, seasonality, and noise\n",
    "2. Create time series features using lag and rolling window techniques\n",
    "3. Train an XGBoost model on Amazon SageMaker\n",
    "4. Deploy the model to a SageMaker endpoint\n",
    "5. Test the endpoint with real-time predictions\n",
    "\n",
    "This workflow can be adapted for real-world demand forecasting applications by replacing the synthetic data with actual historical demand data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}