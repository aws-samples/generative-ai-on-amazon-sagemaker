{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ed5371",
   "metadata": {},
   "source": [
    "# LLM evaluation on Amazon SageMaker AI\n",
    "\n",
    "In this lab, we are going to evaluate the fine-tuned model by using Statistical metrics, such as `BERT`, `ROUGE`, and qualitative metrics through `LLM as a Judge`. Those metrics are evaluated against the base model. For the purpose of this lab, the base model was pre-deployed and accessible in the AWS account.\n",
    "\n",
    "![image.png](./images/llm_eval_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c467cf",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d330e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U bert-score \\\n",
    "    \"datasets>=4.0.0\" \\\n",
    "    mlflow \\\n",
    "    numpy \\\n",
    "    pandas \\\n",
    "    rouge-score \\\n",
    "    sagemaker-mlflow==0.1.0 \\\n",
    "    scikit-learn \\\n",
    "    sentence-transformers \\\n",
    "    textstat \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa725a22",
   "metadata": {},
   "source": [
    "Define the SageMaker endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "eval_dataset = \"pranavvmurthy26/DoclingMatix_500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffdb25-7f50-44f4-8754-d737d5320bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_endpoint_name = \"base-Qwen3-VL-2B-Instruct-ep\"\n",
    "tuned_endpoint_name = \"tuned-Qwen3-VL-2B-Instruct-ep\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c434bbf",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to analyze the dataset used for evaluation\n",
    "\n",
    "```json\n",
    "[\n",
    "    [\n",
    "        {\"images\": [<PIL_IMAGE_1>, ..., <PIL_IMAGE_N>]},\n",
    "        {\"texts\": [\n",
    "                {\"user\": \"<CONTENT>\",\"assistant\": \"<CONTENT>\",\"source\": \"<IMAGE_NAME>\"},\n",
    "                ...\n",
    "                {\"user\": \"<CONTENT>\",\"assistant\": \"<CONTENT>\",\"source\": \"<IMAGE_NAME>\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    ...\n",
    "    [\n",
    "        {\"images\": [<PIL_IMAGE_1>, ..., <PIL_IMAGE_N>]},\n",
    "        {\"texts\": [\n",
    "                {\"user\": \"<CONTENT>\",\"assistant\": \"<CONTENT>\",\"source\": \"<IMAGE_NAME>\"},\n",
    "                ...\n",
    "                {\"user\": \"<CONTENT>\",\"assistant\": \"<CONTENT>\",\"source\": \"<IMAGE_NAME>\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b59e43-d0c9-4fd0-89de-0144f2b3377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.session import get_execution_role\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from nb_local_utils.helpers import (\n",
    "    pretty_print_html,\n",
    "    get_mlflow_server_arn, \n",
    "    get_tracking_server_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a9a4b-9384-4eb7-866a-01d7eb7efb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "sess = Session(boto3.Session(region_name=region))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(eval_dataset, split=\"test\", streaming=True).shuffle(\n",
    "    buffer_size=1000\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_generator(lambda: dataset, features=dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a5450",
   "metadata": {},
   "source": [
    "The sample dataset contains 100 rows taken from [HuggingFaceM4/DoclingMatix](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix/viewer/default/train), a multi-turn and multi-modal dataset for document intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03d472-8038-4245-8a95-f2972b495d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a527214-ac3a-4e71-824b-be5921365caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(dataset[0][\"images\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "random_index = random.randrange(len(dataset[0][\"texts\"]))\n",
    "\n",
    "pretty_print_html(\n",
    "    f\"### First conversation:### \\n\\n{json.dumps(dataset[0][\"texts\"][random_index], indent=2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b69736",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848af33e",
   "metadata": {},
   "source": [
    "## Create dataset for evaluation\n",
    "\n",
    "In the following steps, we are going to generate the dataset for evaluation, by invoking the deployed model, saving the answers and the ground truth in a proper dataset used for the evaluation later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ca7bd-3b2d-4eb0-a5c8-9920477626d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_base64(pil_img, resize_perc=0.5):\n",
    "    \"\"\"Convert a PIL image to base64-encoded PNG string.\"\"\"\n",
    "    pil_img = pil_img.resize([int(resize_perc * s) for s in pil_img.size])\n",
    "    buffer = BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf80a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = []\n",
    "\n",
    "for row in dataset:\n",
    "    images = []\n",
    "    for img in row[\"images\"]:  # Fix: use row['images']\n",
    "        image_base64 = pil_to_base64(img)\n",
    "        images.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    random_index = random.randrange(len(row[\"texts\"]))\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": images + [\n",
    "                {\"type\": \"text\", \"text\": row[\"texts\"][random_index][\"user\"]}\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inference_dataset.append(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"question\": row[\"texts\"][random_index][\"user\"],\n",
    "            \"ground_truth\": row[\"texts\"][random_index][\"assistant\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into a df\n",
    "df = pd.DataFrame(inference_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256838b-c7c2-4494-a93b-a50889298a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset to dict\n",
    "os.makedirs(\"./data/eval\", exist_ok=True)\n",
    "df.to_json(\"./data/eval/dataset.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b648d13",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9490ff6",
   "metadata": {},
   "source": [
    "## Run model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ee7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import time\n",
    "import time\n",
    "from collections import deque\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6c797-0213-45c9-b240-548476797217",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(service_name=\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd69c9c",
   "metadata": {},
   "source": [
    "### Iterator class for streaming inference\n",
    "\n",
    "Utility class to parse streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                continue\n",
    "\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d1e98",
   "metadata": {},
   "source": [
    " Utility function to compute inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    request_start, \n",
    "    request_end, \n",
    "    request_times, \n",
    "    first_token_time, \n",
    "    output_tokens\n",
    "):\n",
    "    \"\"\"Compute latency, RPM, throughput, and TTFT for a single request.\"\"\"\n",
    "    # Calculate metrics\n",
    "    latency_ms = (request_end - request_start)\n",
    "    request_times.append(request_end)\n",
    "    ttft_seconds = (first_token_time - request_start) if first_token_time else 0\n",
    "\n",
    "    current_time = time.time()\n",
    "    recent_requests = [t for t in request_times if current_time - t <= 60]\n",
    "    rpm = len(recent_requests)\n",
    "    throughput = (\n",
    "        output_tokens / (request_end - request_start)\n",
    "        if (request_end - request_start) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Latency: {latency_ms:.0f} s, RPM: {rpm}, Throughput: {throughput:.2f} tokens/s, TTFT: {ttft_seconds:.2f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813c715",
   "metadata": {},
   "source": [
    "Utility function to parse model answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_streaming_response(line_str):\n",
    "    \"\"\"Parse a single streaming response line and return content if found.\"\"\"\n",
    "    if not line_str.strip() or line_str.strip() == \"data: [DONE]\":\n",
    "        return None\n",
    "\n",
    "    if line_str.startswith(\"data: \"):\n",
    "        line_str = line_str[6:]\n",
    "\n",
    "    try:\n",
    "        data = json.loads(line_str)\n",
    "        if \"choices\" in data:\n",
    "            for choice in data[\"choices\"]:\n",
    "                if \"delta\" in choice and \"content\" in choice[\"delta\"]:\n",
    "                    return choice[\"delta\"][\"content\"]\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a337bdc-2a90-48bc-a0d2-f79ffb1ea1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_streaming_eval(\n",
    "    inference_dataset,\n",
    "    sagemaker_client,\n",
    "    endpoint_name,\n",
    "    request_times,\n",
    "):\n",
    "    \"\"\"Run streaming inference over a dataset for a given endpoint.\"\"\"\n",
    "    eval_dataset = []\n",
    "\n",
    "    for index, el in enumerate(inference_dataset, start=1):\n",
    "        print(\"Processing el\", index)\n",
    "\n",
    "        request_start = time.time()\n",
    "        first_token_time = None\n",
    "        generated_text = \"\"\n",
    "        output_tokens = 0\n",
    "\n",
    "        request_body = {\n",
    "            \"messages\": el[\"messages\"],\n",
    "            \"max_tokens\": len(el[\"ground_truth\"]),\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop\": [\"<|im_end|>\"],\n",
    "            \"stream\": True,\n",
    "        }\n",
    "\n",
    "        response = sagemaker_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(request_body),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "        for line in LineIterator(response[\"Body\"]):\n",
    "            if line:\n",
    "                content = parse_streaming_response(line.decode(\"utf-8\"))\n",
    "                if content:\n",
    "                    if first_token_time is None:\n",
    "                        first_token_time = time.time()\n",
    "                    generated_text += content\n",
    "                    output_tokens += len(content.split())\n",
    "\n",
    "        request_end = time.time()\n",
    "\n",
    "        compute_metrics(\n",
    "            request_start=request_start,\n",
    "            request_end=request_end,\n",
    "            request_times=request_times,\n",
    "            first_token_time=first_token_time,\n",
    "            output_tokens=output_tokens,\n",
    "        )\n",
    "\n",
    "        eval_dataset.append(\n",
    "            {\n",
    "                \"messages\": el[\"messages\"],\n",
    "                \"question\": el[\"question\"],\n",
    "                \"output_text\": generated_text,\n",
    "                \"ground_truth\": el[\"ground_truth\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480bc39-10c5-4496-b9ba-0191630f3450",
   "metadata": {},
   "source": [
    "**Run Inference with Base Model Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd65c1-8cae-4fa2-ad96-872f7170384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_times_base = deque()\n",
    "eval_base = run_streaming_eval(inference_dataset, sagemaker_client, base_endpoint_name, request_times_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52759a35-5e64-498e-9803-a1628b1dcd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.DataFrame(eval_base)\n",
    "df_base.to_json(\"./data/eval/base_dataset.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7161481-cb7c-473c-94df-6c8a746aa139",
   "metadata": {},
   "source": [
    "**Run Inference with Fine-tuned Model Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c74af-f843-4930-86e5-c1b0c86678de",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_times_tuned = deque()\n",
    "eval_tuned = run_streaming_eval(inference_dataset, sagemaker_client, tuned_endpoint_name, request_times_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edc269",
   "metadata": {},
   "source": [
    "Save evaluation dataset to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bf8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tuned = pd.DataFrame(eval_tuned)\n",
    "df_tuned.to_json(\"./data/eval/tuned_dataset.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ffc37",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d9c5e",
   "metadata": {},
   "source": [
    "## Statistical evaluation using Managed MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0002f",
   "metadata": {},
   "source": [
    "Define the MLflow tracking ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd942f-d0ce-4a19-a8f0-ece5e4ab6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78963915-d0c6-4240-a8f6-4a3d4f283692",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_SERVER_ARN = get_mlflow_server_arn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8ad36-e97a-49e5-afc4-27c1a9f4af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_link = get_tracking_server_uri(sess, MLFLOW_TRACKING_SERVER_ARN)\n",
    "pretty_print_html(f'<a href=\"{mlflow_link}\" target=\"_blank\">ðŸ”— [Click Me to Open MLflow] ðŸ”—</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e63ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_SERVER_ARN)\n",
    "    mlflow.set_experiment(f\"eval-{model_id.split('/')[-1].replace('.', '-')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd6acd-734c-4381-85ce-085a1dd74547",
   "metadata": {},
   "source": [
    "**Statistical Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb8f73-669b-4065-98c5-0c075faf7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pathlib import Path\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalEvaluator:\n",
    "    \"\"\"Handles statistical evaluation metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rouge = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
    "        )\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def add_metrics(self, df):\n",
    "        \"\"\"Add statistical metrics to dataframe.\"\"\"\n",
    "        preds, refs = df.outputs.tolist(), df.ground_truth.tolist()\n",
    "\n",
    "        # ROUGE scores\n",
    "        rouge_scores = [self.rouge.score(r, p) for p, r in zip(preds, refs)]\n",
    "        df[\"rouge1\"] = [s[\"rouge1\"].fmeasure for s in rouge_scores]\n",
    "        df[\"rouge2\"] = [s[\"rouge2\"].fmeasure for s in rouge_scores]\n",
    "        df[\"rougeL\"] = [s[\"rougeL\"].fmeasure for s in rouge_scores]\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_score(preds, refs, lang=\"en\", verbose=False)\n",
    "        df[\"bert_p\"], df[\"bert_r\"], df[\"bert_f1\"] = P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "        # Semantic similarity\n",
    "        P_emb = self.embedder.encode(preds)\n",
    "        R_emb = self.embedder.encode(refs)\n",
    "        sims = [cosine_similarity([p], [r])[0][0] for p, r in zip(P_emb, R_emb)]\n",
    "        df[\"sem_sim\"] = sims\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de2c20",
   "metadata": {},
   "source": [
    "Utility function to run evaluation with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(df, run_name):\n",
    "    \"\"\"Run MLflow evaluation for a single model.\"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "        def dummy_model(x):\n",
    "            return df.outputs.tolist()\n",
    "\n",
    "        results = mlflow.evaluate(\n",
    "            model=dummy_model,\n",
    "            data=df,\n",
    "            model_type=\"text\",\n",
    "            evaluators=\"default\",\n",
    "            evaluator_config={\n",
    "                \"col_mapping\": {\n",
    "                    \"inputs\": \"inputs\",\n",
    "                    \"outputs\": \"outputs\",\n",
    "                    \"targets\": \"ground_truth\",\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6a821",
   "metadata": {},
   "source": [
    "Utility function to print result metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(combined_results: pd.DataFrame, metrics_to_show: Sequence[str]) -> None:\n",
    "    \"\"\"Print evaluation summary and show a grouped bar chart.\"\"\"\n",
    "    print(\"\\n===================================\")\n",
    "    print(\"          EVALUATION SUMMARY       \")\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    print(\"Available columns:\", \", \".join(combined_results.columns))\n",
    "    print(\"\\n----------------------\\n\")\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for model_type in [\"finetuned\", \"unfinetuned\"]:\n",
    "        sub = combined_results[combined_results[\"model_type\"] == model_type]\n",
    "\n",
    "        if sub.empty:\n",
    "            print(f\"{model_type.upper()}: No rows found\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"{model_type.upper()}:\")\n",
    "\n",
    "        for metric in metrics_to_show:\n",
    "            if metric not in sub.columns:\n",
    "                print(f\"  - {metric}: [column not found]\")\n",
    "                continue\n",
    "\n",
    "            values = sub[metric].dropna()\n",
    "            if values.empty:\n",
    "                print(f\"  - {metric}: [no data]\")\n",
    "                continue\n",
    "\n",
    "            mean_val = values.mean()\n",
    "            print(f\"  - {metric:<20}: {mean_val:>8.3f}\")\n",
    "\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"model_type\": model_type,\n",
    "                    \"metric\": metric,\n",
    "                    \"value\": mean_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print()  # blank line between model types\n",
    "\n",
    "    if not summary_rows:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # Build ordered, unique metric list, filtered to whatâ€™s actually present\n",
    "    present_metrics = list(summary_df[\"metric\"].unique())\n",
    "    ordered_unique_metrics = []\n",
    "    for m in metrics_to_show:\n",
    "        if m in present_metrics and m not in ordered_unique_metrics:\n",
    "            ordered_unique_metrics.append(m)\n",
    "\n",
    "    if ordered_unique_metrics:\n",
    "        summary_df[\"metric\"] = pd.Categorical(\n",
    "            summary_df[\"metric\"],\n",
    "            categories=ordered_unique_metrics,\n",
    "            ordered=True,\n",
    "        )\n",
    "\n",
    "    fig = px.bar(\n",
    "        summary_df,\n",
    "        x=\"metric\",\n",
    "        y=\"value\",\n",
    "        color=\"model_type\",\n",
    "        barmode=\"group\",\n",
    "        text=\"value\",\n",
    "        title=\"Evaluation Metrics by Model Type\",\n",
    "        labels={\n",
    "            \"metric\": \"Metric\",\n",
    "            \"value\": \"Mean Score\",\n",
    "            \"model_type\": \"Model Type\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        texttemplate=\"%{text:.3f}\",\n",
    "        textposition=\"outside\",\n",
    "        hovertemplate=\"<b>%{customdata[0]}</b><br>\"\n",
    "                      \"Metric: %{x}<br>\"\n",
    "                      \"Mean: %{y:.3f}<extra></extra>\",\n",
    "        customdata=summary_df[[\"model_type\"]],\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        uniformtext_minsize=10,\n",
    "        uniformtext_mode=\"hide\",\n",
    "        xaxis=dict(title=\"Metric\"),\n",
    "        yaxis=dict(title=\"Mean Score\", rangemode=\"tozero\"),\n",
    "        legend=dict(\n",
    "            title=\"Model Type\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "        ),\n",
    "        margin=dict(l=40, r=40, t=60, b=80),\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd0b93",
   "metadata": {},
   "source": [
    "Utility function to format the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(df, model_type):\n",
    "    \"\"\"Create evaluation dataframe from JSONL rows.\"\"\"\n",
    "    outputs = []\n",
    "    inputs = []\n",
    "    gts = []\n",
    "\n",
    "    for _, el in df.iterrows():\n",
    "        # Combine reasoning + final output\n",
    "        outputs.append(el[\"output_text\"])\n",
    "        inputs.append(json.dumps(el[\"messages\"]))\n",
    "        gts.append(el[\"ground_truth\"])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"inputs\": inputs,\n",
    "            \"outputs\": outputs,\n",
    "            \"ground_truth\": gts,\n",
    "            \"model_type\": model_type,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3932f",
   "metadata": {},
   "source": [
    "### Run Evaluation: Base Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d807730-0c42-4861-ab03-5a9726bc8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_for_base = StatisticalEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_final = create_dataframe(df_base, \"unfinetuned\")\n",
    "df_base_final = evaluator_for_base.add_metrics(df_base_final)\n",
    "\n",
    "# run evaluation\n",
    "results_base_final = run_evaluation(df_base_final, \"unfinetuned_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd29e3-6506-493f-a70b-686e6b66cf2c",
   "metadata": {},
   "source": [
    "### Run Evaluation: Fine-Tuned Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c75e6-e7e0-478d-b9f9-6e71012d03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_for_tuned = StatisticalEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7cf039-3644-4ab5-9384-d110301b506d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tuned_final = create_dataframe(df_tuned, \"finetuned\")\n",
    "df_tuned_final = evaluator_for_tuned.add_metrics(df_tuned_final)\n",
    "\n",
    "# run evaluation\n",
    "results_tuned_final = run_evaluation(df_tuned_final, \"finetuned_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbbc3a-fe02-403b-b56e-277285318379",
   "metadata": {},
   "source": [
    "### Display Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0f0b9-bbc4-4d70-9e1d-0fa145a27724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat both base and tuned results\n",
    "results_all = pd.concat(\n",
    "    [results_base_final, results_tuned_final], \n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b8ff7-a1bd-45ee-8885-3ce1f8e8fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./evaluation_results\"\n",
    "\n",
    "Path(path).mkdir(exist_ok=True)\n",
    "out_path = Path(path) / \"statistical_eval_results.csv\"\n",
    "results_all.to_csv(out_path, index=False)\n",
    "\n",
    "print_summary(\n",
    "    results_all, \n",
    "    metrics_to_show=[\"toxicity/v1/score\", \"rouge1\", \"rouge2\", \"rougeL\", \"bert_p\", \"bert_r\", \"bert_f1\", \"sem_sim\", \"bert_f1\", \"sem_sim\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3112e7b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43633e6",
   "metadata": {},
   "source": [
    "## Qualitative evaluation using LLM as a judge\n",
    "\n",
    "We are now going to evaluate the model with the LLM as a Judge evaluation task. First, let's define the Bedrock client\n",
    "\n",
    "![llm_judge.png](./images/llm_judge.png)\n",
    "\n",
    "LLM-as-a-Judge is a framework where a large language model (LLM) acts as an evaluator, scoring the coherence of AI-generated outputs based on given inputs. The the following cells, we are going to score outputs generated by both the base and the fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import textwrap\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import sagemaker\n",
    "from tqdm import tqdm\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sagemaker_session.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc17c03",
   "metadata": {},
   "source": [
    "To invoke the fine-tuned model for inference, we'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8050bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_base = f\"\"\"\n",
    "You are an expert evaluator for document Q&A systems. Evaluate the generated answer against the ground truth.\n",
    "\n",
    "## EVALUATION CRITERIA\n",
    "\n",
    "1. **Factual Accuracy** (40 points): Is the information correct based on the document?\n",
    "2. **Completeness** (30 points): Does it fully answer the question?\n",
    "3. **Relevance** (20 points): Is the response directly related to the question?\n",
    "4. **Clarity** (10 points): Is the answer clear and well-structured?\n",
    "\n",
    "## EVALUATION\n",
    "Provide scores for each criterion and explain your reasoning.\n",
    "\n",
    "**Factual Accuracy**: X/40 (explanation)\n",
    "**Completeness**: X/30 (explanation)  \n",
    "**Relevance**: X/20 (explanation)\n",
    "**Clarity**: X/10 (explanation)\n",
    "**Total Score**: X/100\n",
    "\n",
    "At the end of your evaluation, you MUST put your final preference in the tags <final_score>...</final_score> like:\n",
    "<final_score>\n",
    "Score 90/100\n",
    "</final_score>\n",
    "\n",
    "You MUST put the details in the tags <details>...</details>\n",
    "<details>\n",
    "Factual Accuracy: 35/40\n",
    "Completeness: 30/30\n",
    "Relevance: 20/20\n",
    "Clarity: 5/10\n",
    "</details>\n",
    "\n",
    "This is the baseline to evaluate:\n",
    "\n",
    "## QUESTION\n",
    "{{question}}\n",
    "\n",
    "## GROUND TRUTH ANSWER\n",
    "{{ground_truth}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbba140",
   "metadata": {},
   "source": [
    "### Use Amazon Nova Pro as Judge\n",
    "\n",
    "We invoke Amazon Nova Pro on the generated dataset to evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_score(text):\n",
    "    pattern = r\"<final_score>(.*?)</final_score>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def extract_details(text):\n",
    "    pattern = r\"<details>(.*?)</details>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def run_evaluator(\n",
    "    df,\n",
    "    system_prompt_base: str,\n",
    "    rate_limit: int = 5,\n",
    "    window_seconds: int = 60,\n",
    "):\n",
    "    \"\"\"Run model-based evaluation for each row in a DataFrame with built-in rate limiting.\"\"\"\n",
    "\n",
    "    request_times = []\n",
    "    results = []\n",
    "\n",
    "    for index, el in df.iterrows():\n",
    "        print(f\"Processing el: {index + 1}\")\n",
    "\n",
    "        current_time = time.time()\n",
    "        request_times = [t for t in request_times if current_time - t < window_seconds]\n",
    "\n",
    "        if len(request_times) >= rate_limit:\n",
    "            wait_time = window_seconds - (current_time - request_times[0])\n",
    "            print(f\"Rate limit reached â†’ waiting {wait_time:.2f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "            request_times = request_times[1:]\n",
    "\n",
    "        request_times.append(time.time())\n",
    "\n",
    "        prompt = system_prompt_base.format(\n",
    "            question=el[\"question\"],\n",
    "            ground_truth=el[\"ground_truth\"],\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [{\"text\": el[\"output_text\"]}]},\n",
    "        ]\n",
    "        \n",
    "        response = generate(\n",
    "            model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "            system_prompt=textwrap.dedent(prompt).strip(),\n",
    "            messages=messages,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        model_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "        final_score = extract_final_score(model_text)\n",
    "        details = extract_details(model_text)\n",
    "\n",
    "        results.append(\n",
    "            [\n",
    "                index,\n",
    "                el[\"question\"],\n",
    "                final_score,\n",
    "                details,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281c0cd-8359-482f-9a73-6b1beecdc697",
   "metadata": {},
   "source": [
    "**Run Judge Evaluation for Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0d14e-a1ba-48e7-8a46-2c3015e06e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_base = run_evaluator(df=df_base, system_prompt_base=system_prompt_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71f42f-9e90-4be3-bb3c-444a46c74fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_df = pd.DataFrame(\n",
    "    results_for_base, columns=[\"index\", \"question\", \"final_score\", \"details\"]\n",
    ")\n",
    "\n",
    "results_base_df.to_json(\"./evaluation_results/base_llm_judge_eval_results.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef78c89-68e6-4348-85b4-c8381fcf2498",
   "metadata": {},
   "source": [
    "**Run Judge Evaluation for Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e3add-eded-4c46-9313-50a550fd412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_tuned = run_evaluator(df=df_tuned, system_prompt_base=system_prompt_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf5b2d",
   "metadata": {},
   "source": [
    "Save results into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3487e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tuned_df = pd.DataFrame(\n",
    "    results_for_tuned, columns=[\"index\", \"question\", \"final_score\", \"details\"]\n",
    ")\n",
    "\n",
    "results_tuned_df.to_json(\"./evaluation_results/tuned_llm_judge_eval_results.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7510c",
   "metadata": {},
   "source": [
    "## Compare evaluation results\n",
    "\n",
    "We are now comparing the evaluation results between the base and the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both files\n",
    "with open(\"./evaluation_results/tuned_llm_judge_eval_results.json\", \"r\") as f:\n",
    "    data_finetuned = json.load(f)\n",
    "\n",
    "with open(\"./evaluation_results/base_llm_judge_eval_results.json\", \"r\") as f:\n",
    "    data_base = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee045e11",
   "metadata": {},
   "source": [
    "Utility function to extract scores and categorize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(score_str):\n",
    "    match = re.search(r\"(\\d+)/100\", score_str)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def categorize(score):\n",
    "    if score >= 90:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 80:\n",
    "        return \"Good\"\n",
    "    elif score >= 70:\n",
    "        return \"Fair\"\n",
    "    else:\n",
    "        return \"Poor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = pd.DataFrame(data_finetuned)\n",
    "df_ft = df_ft.dropna()\n",
    "df_base = pd.DataFrame(data_base)\n",
    "df_base = df_base.dropna()\n",
    "\n",
    "df_ft[\"score\"] = df_ft[\"final_score\"].apply(extract_score)\n",
    "df_base[\"score\"] = df_base[\"final_score\"].apply(extract_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713b4a4",
   "metadata": {},
   "source": [
    "Side-by-side comparison histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.hist(df_ft[\"score\"], bins=20, edgecolor=\"black\", alpha=0.7, color=\"blue\")\n",
    "ax1.set_title(\"Fine-tuned Model Scores\")\n",
    "ax1.set_xlabel(\"Score\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "\n",
    "ax2.hist(df_base[\"score\"], bins=20, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
    "ax2.set_title(\"Base Model Scores\")\n",
    "ax2.set_xlabel(\"Score\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b5ba1",
   "metadata": {},
   "source": [
    "Overlapping distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03eea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_ft[\"score\"], bins=20, alpha=0.5, label=\"Fine-tuned\", color=\"blue\")\n",
    "plt.hist(df_base[\"score\"], bins=20, alpha=0.5, label=\"Base\", color=\"orange\")\n",
    "plt.title(\"Score Distribution Comparison\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3. Box plot comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([df_ft[\"score\"], df_base[\"score\"]], labels=[\"Fine-tuned\", \"Base\"])\n",
    "plt.title(\"Score Distribution Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb96ed5",
   "metadata": {},
   "source": [
    "Statistics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== STATISTICS COMPARISON ===\\n\")\n",
    "print(f\"Fine-tuned Model:\")\n",
    "print(f\"  Average: {df_ft['score'].mean():.1f}\")\n",
    "print(f\"  Median: {df_ft['score'].median():.1f}\")\n",
    "print(f\"  Min: {df_ft['score'].min()}\")\n",
    "print(f\"  Max: {df_ft['score'].max()}\")\n",
    "print(f\"\\nBase Model:\")\n",
    "print(f\"  Average: {df_base['score'].mean():.1f}\")\n",
    "print(f\"  Median: {df_base['score'].median():.1f}\")\n",
    "print(f\"  Min: {df_base['score'].min()}\")\n",
    "print(f\"  Max: {df_base['score'].max()}\")\n",
    "print(f\"\\nImprovement: {df_ft['score'].mean() - df_base['score'].mean():.1f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea032a",
   "metadata": {},
   "source": [
    "Score categories comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft[\"category\"] = df_ft[\"score\"].apply(categorize)\n",
    "df_base[\"category\"] = df_base[\"score\"].apply(categorize)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df_ft[\"category\"].value_counts().plot.pie(ax=ax1, autopct=\"%1.1f%%\")\n",
    "ax1.set_title(\"Fine-tuned Model Categories\")\n",
    "df_base[\"category\"].value_counts().plot.pie(ax=ax2, autopct=\"%1.1f%%\")\n",
    "ax2.set_title(\"Base Model Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af3225-6780-48eb-b1d0-9a290f36d698",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "END OF LAB 3\n",
    "--- \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d80da3-edff-4760-908e-a3d778e84511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
