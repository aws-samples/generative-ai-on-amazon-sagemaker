{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize and Deploy `Qwen/Qwen3-VL-2B-Instruct` on Amazon SageMaker AI\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we explore **Qwen3-VL-2B-Instruct**, a ~2-billion-parameter vision-language instruction-tuned model from Alibabaâ€™s Qwen3 â€œVLâ€ series. Youâ€™ll learn how to fine-tune it with image + text instruction workflows, evaluate its vision-language output performance, and deploy it via SageMaker for multimodal assistant use-cases.\n",
    "\n",
    "**What is Qwen3-VL-2B-Instruct?**\n",
    "Qwen3-VL-2B-Instruct is the instruction-tuned vision-language variant of the Qwen3 family at ~2B parameters. The â€œVLâ€ denotes support for both image and text input, while â€œInstructâ€ indicates that the model has been fine-tuned to follow human instructions. The Qwen3 family is released under the **Apache-2.0 license**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Specifications**\n",
    "\n",
    "| Feature                   | Details                                                                               |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **Parameters**            | ~2 billion                                                                            |\n",
    "| **Modalities**            | Image + Text input â†’ Text output                                                      |\n",
    "| **Context / Window Size** | Large text context (e.g., tens of thousands of tokens) combined with vision input     |\n",
    "| **Architecture**          | Dense Transformer backbone with vision encoder front-end and instruction-tuned output |\n",
    "| **License**               | Apache-2.0                                                                            |\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarks & Behavior**\n",
    "\n",
    "* The Qwen3-VL models demonstrate strong capabilities in vision-language reasoning, structured output generation, and instruction following.\n",
    "* At ~2B size, Qwen3-VL-2B-Instruct offers an efficient footprint for multimodal tasks where modest compute is available.\n",
    "* Ideal for tasks such as image-guided question answering, vision-augmented instruction following, and assistant workflows that take images + text prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq \"datasets==4.3.0\" \\\n",
    "    \"sagemaker==2.253.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.session import get_execution_role\n",
    "from nb_local_utils.helpers import (\n",
    "    pretty_print_html,\n",
    "    get_mlflow_server_arn, \n",
    "    get_tracking_server_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "sess = Session(boto3.Session(region_name=region))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5a299-1c89-4881-8d51-9538d95d007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_html(f\"sagemaker role arn: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8786c-04b1-4029-b459-fb9c5f1afb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_html(f\"sagemaker bucket: {sagemaker_session_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9786901-b012-41ff-a98a-b16e5f60ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_html(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2bf05-a59f-43f5-ad2a-8279d3ab8f1c",
   "metadata": {},
   "source": [
    "## 01. Data Preparation for Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad12e3-c97a-4fde-a16e-3db093b318c1",
   "metadata": {},
   "source": [
    "### [DoclingMatix](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix)\n",
    "\n",
    "**DoclingMatix** is a large-scale, multimodal dataset designed for document-intelligence tasks involving vision + instruction-tuned structured output. Built by augmenting the original Docmatix dataset, DoclingMatix converts each document image and QA pair into an instruction-based format for document conversion or visual question answering.\n",
    "\n",
    "**Data Format & Structure**:\n",
    "\n",
    "* Distributed in **Parquet** and/or **JSONL** formats.\n",
    "* Contains a single `train` split with approximately **1,270,911** samples.\n",
    "* Each record includes:\n",
    "\n",
    "  * `images` â€“ one or more images of the document page(s)\n",
    "  * `instruction` â€“ a natural-language prompt specifying the desired conversion or QA task (e.g., â€œConvert this page into structured DocTag formatâ€ or â€œAnswer the question about the documentâ€)\n",
    "  * `response` â€“ the target output (structured text or answer)\n",
    "\n",
    "**License**: Released under the terms indicated on the Hugging Face dataset page (please verify before use).\n",
    "\n",
    "**Applications**:\n",
    "\n",
    "The dataset can support a variety of document-intelligence tasks, including:\n",
    "\n",
    "* Instruction-tuned document conversion (image + instruction â†’ structured text)\n",
    "* Document visual question answering (DocVQA) and document understanding workflows\n",
    "* Training visionâ€“language models that parse document layouts and generate structured outputs\n",
    "* Fine-tuning multimodal assistants or agents for enterprises, knowledge work, and document automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6608-a09f-4521-8ac8-36ec94cc76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "import json\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from datasets import Dataset, load_dataset\n",
    "from sagemaker.modules.configs import InputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a6564-4a41-4d05-a550-e43784cb2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parent_path = os.path.join(os.getcwd(), \"tmp_cache_local_dataset\")\n",
    "os.makedirs(dataset_parent_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d27052-23fd-4a8b-863b-06b325be7502",
   "metadata": {},
   "source": [
    "**Preparing Your Dataset in `messages` format**\n",
    "\n",
    "This section walks you through creating a conversation-style datasetâ€”the required `messages` formatâ€”for directly training LLMs using SageMaker AI.\n",
    "\n",
    "**What Is the `messages` Format?**\n",
    "\n",
    "The `messages` format structures instances as chat-like exchanges, wrapping each conversation turn into a role-labeled JSON array. Itâ€™s widely used by frameworks like TRL.\n",
    "\n",
    "Example entry:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"How do I bake sourdough?\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"First, you need to create a starter by...\" }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c962155-197b-447a-99ad-de6642153d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"pranavvmurthy26/DoclingMatix_500\"\n",
    "dataset_train = load_dataset(dataset_name, split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c87fc-597c-4ee7-9181-27c9a101299e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pp(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48951960-8766-4f89-9e44-ae736f370ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_html(f\"total number of fine-tunable samples: {len(dataset_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889d18f-a156-4b0e-975f-51684421aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_base64(pil_img, resize_perc=0.2):\n",
    "    \"\"\"Convert a PIL image to base64-encoded PNG string.\"\"\"\n",
    "    if not isinstance(pil_img, Image.Image):\n",
    "        raise ValueError(\"Input must be a PIL Image.\")\n",
    "    new_size = [int(resize_perc * s) for s in pil_img.size]\n",
    "    pil_img = pil_img.resize(new_size)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def convert_to_messages_multimodal(row):\n",
    "    \"\"\"Explode a multi-turn document QA row into multiple single-turn multimodal conversations.\"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a multimodal document reasoning assistant. Given an image of a document page \"\n",
    "        \"and an instruction in natural language, generate a structured text output or answer \"\n",
    "        \"that follows the instruction precisely. Process the visual and textual content in the image \"\n",
    "        \"and provide a concise, well-formatted result. Explain briefly how the document image was used.\"\n",
    "    )\n",
    "\n",
    "    # Encode all images as base64 strings once per row\n",
    "    images = []\n",
    "    img_list = row.get(\"images\", [])\n",
    "    if img_list:\n",
    "        for img in img_list if isinstance(img_list, list) else [img_list]:\n",
    "            if hasattr(img, \"save\"):\n",
    "                b64_img = pil_to_base64(img)\n",
    "                images.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{b64_img}\"}\n",
    "                })\n",
    "\n",
    "    # Prepare outputs\n",
    "    for t in row.get(\"texts\", []):\n",
    "        user_text = t.get(\"user\", \"\").strip()\n",
    "        assistant_text = t.get(\"assistant\", \"\").strip()\n",
    "\n",
    "        # Skip empty entries\n",
    "        if not user_text or not assistant_text:\n",
    "            continue\n",
    "\n",
    "        yield {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "                {\"role\": \"user\", \"content\": images + [{\"type\": \"text\", \"text\": user_text}]},\n",
    "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_text}]}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "def convert_to_messages(dataset):\n",
    "    \"\"\"Iteratively run conversion on multi-turn conversation and flatten to messages\"\"\"\n",
    "    records = []\n",
    "    # Unroll your generator for every dataset row\n",
    "    for row in tqdm(\n",
    "        dataset, \n",
    "        total=len(dataset), \n",
    "        desc=\"Converting to messages\"\n",
    "    ):\n",
    "        for example in convert_to_messages_multimodal(row):\n",
    "            records.append(example)\n",
    "    # Convert list of dicts â†’ Hugging Face Dataset and return\n",
    "    return Dataset.from_list(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458717e-5122-4246-b61d-c619991fd7d4",
   "metadata": {},
   "source": [
    "#### Prepare a `training` dataset and upload file to `s3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd52a37-8bd7-4031-ad86-22e262767f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = convert_to_messages(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470cad1-5bd4-4c8c-89df-d73fff34b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename_train = os.path.join(dataset_parent_path, f\"{dataset_name.replace('/', '--').replace('.', '-')}--train.jsonl\")\n",
    "dataset_train.to_json(dataset_filename_train, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1ffa4-c27d-4f58-94a4-e3db4805f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_uri = f\"s3://{sess.default_bucket()}/dataset/{dataset_name}\"\n",
    "\n",
    "uploaded_train_s3_uri = S3Uploader.upload(\n",
    "    local_path=dataset_filename_train,\n",
    "    desired_s3_uri=data_s3_uri\n",
    ")\n",
    "print(f\"Uploaded:\\n--Source: {dataset_filename_train}\\n>\\n--Destination:{uploaded_train_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cdab7-fa8e-4d12-88eb-22c3adc93a40",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "END OF LAB 1\n",
    "--- \n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e320-cb5c-4cce-8d71-ff7152ac9f95",
   "metadata": {},
   "source": [
    "## 02. Fine-Tune LLMs using SageMaker `Estimator`/`ModelTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c12ae-a6d4-4225-af55-89776bf09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a530ca5-253a-4202-a144-7203a207b8d2",
   "metadata": {},
   "source": [
    "### Training using `PyTorch` SageMaker `ModelTrainer`\n",
    "\n",
    "**Training Using `PyTorch` Estimator**\n",
    "Leverages the official PyTorch SageMaker container to run a custom training script using the Accelerate and DeepSpeed libraries. This option is ideal for users who want full control over the training pipeline \n",
    "\n",
    "---\n",
    "**Observability**: SageMaker AI has [SageMaker MLflow](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) which enables you to accelerate generative AI by making it easier to track experiments and monitor performance of models and AI applications using a single tool.\n",
    "\n",
    "You can choose to include MLflow as a part of your training workflow to track your model fine-tuning metrics in realtime by simply specifying a **mlflow** tracking arn.\n",
    "\n",
    "Optionally you can also report to : **tensorboard**, **wandb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automate tracking server fetch\n",
    "MLFLOW_TRACKING_SERVER_ARN = get_mlflow_server_arn()\n",
    "\n",
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    reports_to = \"mlflow\"\n",
    "else:\n",
    "    reports_to = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08e26c-6eb8-4d26-a10d-b0a70305a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_link = get_tracking_server_uri(sess, MLFLOW_TRACKING_SERVER_ARN)\n",
    "pretty_print_html(f'<a href=\"{mlflow_link}\" target=\"_blank\">ðŸ”— [Click Me to Open MLflow] ðŸ”—</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dea7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'Qwen--Qwen3-VL-2B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182e8ef-1cb0-4ca7-89d9-0ab229991d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MLFLOW_TRACKING_SERVER_ARN:\n",
    "    training_env = {\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": f\"oss-recipe-{job_name}\",\n",
    "        \"MLFLOW_TAGS\": '{\"source.job\": \"sm-training-jobs\", \"source.type\": \"sft\", \"source.framework\": \"pytorch\"}',\n",
    "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_SERVER_ARN,\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\"\n",
    "    }\n",
    "else:\n",
    "    training_env = {\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50358b-84f8-46fb-8096-f0d28088e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mechanism\n",
    "attention_mechanism = \"flash_attention_2\" # eager | flash_attention_2 | kernels-community/vllm-flash-attn3\n",
    "# data path inside the container\n",
    "datapath = f\"/opt/ml/input/data/training/{os.path.basename(dataset_filename_train)}\"\n",
    "# max seq lengths\n",
    "max_seq_lengths = 512\n",
    "# training epochs\n",
    "epochs = 1\n",
    "# training batch size\n",
    "training_batch_size = 1\n",
    "# run name\n",
    "run_name = f\"Qwen3-VL-2B-Instruct-peft-qlora-{dataset_name.replace('/', '--').replace('.', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccf1b3-9003-451a-bfa1-6b053821d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_template = \"\"\"# Model arguments\n",
    "model_name_or_path: Qwen/Qwen3-VL-2B-Instruct\n",
    "processor_name_or_path: Qwen/Qwen3-VL-2B-Instruct\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: {{ attention_mechanism }}\n",
    "use_liger: false\n",
    "bf16: true\n",
    "tf32: false\n",
    "output_dir: /opt/ml/checkpoints/Qwen/Qwen3-VL-2B-Instruct/peft-qlora/\n",
    "\n",
    "# Dataset arguments\n",
    "dataset_id_or_path: {{ datapath }}\n",
    "max_seq_length: {{ max_seq_lengths }}\n",
    "packing: false\n",
    "\n",
    "# LoRA arguments\n",
    "use_peft: true\n",
    "load_in_4bit: true\n",
    "lora_target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "# lora_modules_to_save: [\"lm_head\", \"embed_tokens\"]\n",
    "lora_r: 4\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# Modality type\n",
    "modality_type: \"image\"\n",
    "\n",
    "# Training arguments\n",
    "num_train_epochs: {{ epochs }}\n",
    "per_device_train_batch_size: {{ training_batch_size }}\n",
    "gradient_accumulation_steps: 1\n",
    "gradient_checkpointing: true\n",
    "gradient_checkpointing_kwargs:\n",
    "  use_reentrant: true\n",
    "learning_rate: 1.0e-4 \n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "remove_unused_columns: false\n",
    "\n",
    "# Logging arguments\n",
    "logging_strategy: steps\n",
    "logging_steps: 1\n",
    "report_to:\n",
    "- {{ reports_to }}\n",
    "run_name: {{ run_name }}\n",
    "save_strategy: \"epoch\"\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "config_filename = \"Qwen3-VL-2B-Instruct-vanilla-peft-qlora.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe60e54-90a5-45e7-b898-c441f980f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render template and plug dynamic values\n",
    "rendered_yaml = Template(yaml_template).render(\n",
    "    attention_mechanism=attention_mechanism,\n",
    "    datapath=datapath,\n",
    "    max_seq_lengths=max_seq_lengths,\n",
    "    epochs=epochs,\n",
    "    training_batch_size=training_batch_size,\n",
    "    reports_to=reports_to,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "# Save to file (optional)\n",
    "with open(os.path.join(\"./sagemaker_code/hf_recipes/Qwen\", config_filename), \"w\") as f:\n",
    "    f.write(rendered_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1313cbe-0828-4588-a62f-e2b81944b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_html(rendered_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0cad0-ee23-4e64-87bd-1df486578d35",
   "metadata": {},
   "source": [
    "#### Training strategy: `PeFT/LoRA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"2.7.1\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"training\",\n",
    ")\n",
    "\n",
    "pretty_print_html(f\"Running training on: {image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import (\n",
    "    CheckpointConfig,\n",
    "    Compute,\n",
    "    OutputDataConfig,\n",
    "    SourceCode,\n",
    "    StoppingCondition,\n",
    ")\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "\n",
    "args = [\n",
    "    \"--config\",\n",
    "    \"hf_recipes/Qwen/Qwen3-VL-2B-Instruct-vanilla-peft-qlora.yaml\",\n",
    "]\n",
    "\n",
    "# Define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./sagemaker_code\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    command=f\"bash sm_accelerate_train.sh {' '.join(args)}\",\n",
    ")\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    keep_alive_period_in_seconds=0,\n",
    ")\n",
    "\n",
    "\n",
    "# define OutputDataConfig path\n",
    "output_path = f\"s3://{sagemaker_session_bucket}/{job_name}\"\n",
    "\n",
    "# Define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
    "    environment=training_env,\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=output_path,\n",
    "        compression_type=\"NONE\",\n",
    "    ),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        s3_uri=output_path + \"/checkpoint\", local_path=\"/opt/ml/checkpoints\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"training\",\n",
    "    data_source=uploaded_train_s3_uri,\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "data = [train_input]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65a077-d4fc-48bf-8788-de47e017f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit or train\n",
    "model_trainer.train(input_data_config=data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad066686-43b1-42e2-be6f-ef5efa471bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_job_name = model_trainer._latest_training_job.training_job_name\n",
    "pretty_print_html(f\"Your training job name ðŸ‘‰ {full_training_job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ab639-ea48-4f04-9b14-43e5134557f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store full_training_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fd0d9-cb3a-4b65-b965-1630cc556171",
   "metadata": {},
   "source": [
    "#### Get Training Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41024d3a-84cf-47f6-9817-6b4e5aef7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ea458-2b4f-42f0-85b5-5c1ae90ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_state = Estimator.attach(\n",
    "    training_job_name=full_training_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98bece-58bf-446c-aee3-7e5fcbd83e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## print training job log (uncomment before execution)\n",
    "# estimator_state.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7c62-e6e7-4255-8383-d14e980b40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = estimator_state.model_data\n",
    "pretty_print_html(f\"Fine-tuned model location: {s3_model_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f633a-1361-4930-82b6-79cea6a8985b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "END OF LAB 2\n",
    "--- \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d24d7e-6405-466c-b6e1-055bf3abdd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
