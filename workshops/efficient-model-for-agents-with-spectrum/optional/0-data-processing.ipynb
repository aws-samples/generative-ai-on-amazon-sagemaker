{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd3e9d3",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "For the sake of the workshop, we have created a small dataset, which is available on [HuggingFace](https://huggingface.co/datasets/riv25-aim410/riv25_aim410_nl2sql_toolcall). This dataset is generated from two publicly available datasets. In this notebook, we show you how we did it.\n",
    "\n",
    "## Access data from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3187e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker==2.254.1 datasets transformers boto3 --quiet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable xet in huggingface because of bug with ipykernel\n",
    "# https://github.com/huggingface/xet-core/issues/526\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a49699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "nl2sql_text = load_dataset(\"gretelai/synthetic_text_to_sql\").shuffle(seed=42)\n",
    "ds = load_dataset(\"interstellarninja/hermes_reasoning_tool_use\").shuffle(\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c548f",
   "metadata": {},
   "source": [
    "## Prepare data using `apply_chat_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccca9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo - Sample the dataset (remove for better performances)\n",
    "ds['train'] = ds['train'].select(range(600))\n",
    "nl2sql_text['train'] = nl2sql_text['train'].select(range(500))\n",
    "nl2sql_text['test'] = nl2sql_text['test'].select(range(100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceef007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_tokenize_hermes_reasoning_tool_use(sample, tokenizer):\n",
    "    # Replace \"from\" key with \"role\", and \"value\" with \"content\"\n",
    "    for message in sample['conversations']:\n",
    "        message[\"role\"] = message.pop(\"from\")\n",
    "        message[\"content\"] = message.pop(\"value\")\n",
    "\n",
    "    # Replace \"human\" value with \"user\", and \"gpt\" with \"assistant\"\n",
    "    for message in sample['conversations']:\n",
    "        if message[\"role\"] == \"human\":\n",
    "            message[\"role\"] = \"user\"\n",
    "        elif message[\"role\"] == \"gpt\":\n",
    "            message[\"role\"] = \"assistant\"\n",
    "\n",
    "    # Apply the chat template\n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(\n",
    "        sample['conversations'], tokenize=False, enable_thinking=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "\n",
    "ds_v2 = ds[\"train\"].map(\n",
    "    convert_and_tokenize_hermes_reasoning_tool_use,\n",
    "    remove_columns=list(ds[\"train\"].features),\n",
    "    fn_kwargs={\"tokenizer\": tok},\n",
    ")\n",
    "ds_v2 = ds_v2.train_test_split(test_size=0.2)\n",
    "tool_call_train_dataset = ds_v2['train']\n",
    "tool_call_test_dataset = ds_v2['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa227993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_tokenize_synthetic_text_to_sql(sample, tokenizer):\n",
    "    system = f\"\"\"\n",
    "        You are an expert SQL developer. Given the provided database schema and the following user question, generate a syntactically correct SQL query. \n",
    "        Only reply with the SQL query, nothing else. Do NOT use the backticks to identify the code, just reply with the pure SQL query.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        -- Schema --\n",
    "        {sample[\"sql_context\"]}\n",
    "        -- Query --\n",
    "        {sample[\"sql_prompt\"]}\n",
    "        -- SQL --\n",
    "    \"\"\"\n",
    "    # reasoning = sample[\"sql_explanation\"]\n",
    "    answer = sample[\"sql\"]\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        # {\"role\": \"assistant\", \"reasoning_content\": reasoning, \"content\": answer},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, enable_thinking=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "\n",
    "nl2sql_train_dataset = nl2sql_text[\"train\"].map(\n",
    "    convert_and_tokenize_synthetic_text_to_sql,\n",
    "    remove_columns=list(nl2sql_text[\"train\"].features),\n",
    "    fn_kwargs={\"tokenizer\": tok},\n",
    ")\n",
    "nl2sql_test_dataset = nl2sql_text[\"test\"].map(\n",
    "    convert_and_tokenize_synthetic_text_to_sql,\n",
    "    remove_columns=list(nl2sql_text[\"test\"].features),\n",
    "    fn_kwargs={\"tokenizer\": tok},\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21de72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = concatenate_datasets(\n",
    "    [tool_call_train_dataset, nl2sql_train_dataset]\n",
    ")\n",
    "test_dataset = concatenate_datasets(\n",
    "    [tool_call_test_dataset, nl2sql_test_dataset]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbb4f7",
   "metadata": {},
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de836af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/finetuning-modeltrainer-accelerate\"\n",
    "else:\n",
    "    input_path = f\"datasets/finetuning-modeltrainer-accelerate\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.json\"\n",
    "\n",
    "# Save datasets to s3\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "train_dataset.to_json(\"./data/train/dataset.json\", orient=\"records\")\n",
    "test_dataset.to_json(\"./data/val/dataset.json\", orient=\"records\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.json\", bucket_name, f\"{input_path}/train/dataset.json\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/dataset.json\", bucket_name, f\"{input_path}/val/dataset.json\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
