{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4cb574-f12e-4f8a-8b2c-1047f611d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragas datacompy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bf6ad-559f-46ab-ad06-2bf6e4edd226",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <center>⚠️️ Restart the notebook kernel before proceeding!</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936a4d4-fd63-4887-8b27-3777aab8dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r db_name s3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1b395-2a52-458e-9efc-9c8c988453fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0372b4-7453-4b6f-80be-10055497d952",
   "metadata": {},
   "source": [
    "# Evaluate your SLM using Ragas\n",
    "\n",
    "## Non-Execution Evaluation: SQL Query Semantic equivalence\n",
    "\n",
    "**OUTPUT:** 0 if semantically different, 1 if semantically equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a249b-85bb-4611-b4b4-0dbf9193e4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_results = \"../results/eval_sql_qwen_base.json\"\n",
    "base_df = pd.read_json(base_results, orient=\"columns\")\n",
    "\n",
    "ft_results = \"../results/eval_sql_qwen_ft.json\"\n",
    "ft_df = pd.read_json(ft_results, orient=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf1f75-73d8-4fe2-bb47-2f0767a67e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Bedrock as the Evaluator LLM\n",
    "from langchain_aws import ChatBedrock\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import LLMSQLEquivalence\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_id = \"us.amazon.nova-pro-v1:0\"   # Choose your desired model\n",
    "region_name = \"us-east-1\"              # Choose your desired AWS region\n",
    "\n",
    "bedrock_llm = ChatBedrock(model_id=model_id, region_name=region_name)\n",
    "evaluator_llm = LangchainLLMWrapper(bedrock_llm)\n",
    "scorer = LLMSQLEquivalence(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc239c95",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <center><b>⚠️️ Important ⚠️️</b> The cell below takes <b>~10 minutes to run</b>!</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81372fab-0bc8-450c-a587-2185dc380cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = open(\"../utils/data_schema.md\").read()\n",
    "\n",
    "# Evaluate Base Model\n",
    "base_scores = []\n",
    "for test in tqdm(base_df.iterrows(), total=len(base_df)):\n",
    "    sample = SingleTurnSample(\n",
    "        response=test[1]['qwen_base_sql_query'],\n",
    "        reference=test[1]['sql_query'],\n",
    "        reference_contexts=[schema]\n",
    "    )\n",
    "    base_scores.append(await scorer.single_turn_ascore(sample))\n",
    "base_score = statistics.mean(base_scores)\n",
    "\n",
    "# Evaluate Fine-Tuned Model\n",
    "ft_scores = []\n",
    "for test in tqdm(ft_df.iterrows(), total=len(ft_df)):\n",
    "    sample = SingleTurnSample(\n",
    "        response=test[1]['qwen_ft_sql_query'],\n",
    "        reference=test[1]['sql_query'],\n",
    "        reference_contexts=[schema]\n",
    "    )\n",
    "    ft_scores.append(await scorer.single_turn_ascore(sample))\n",
    "ft_score = statistics.mean(ft_scores)\n",
    "\n",
    "print(f\"Base Model Score: {base_score}\")\n",
    "print(f\"Fine-Tuned Model Score: {ft_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4d474-60fa-4b19-a944-861005b255fb",
   "metadata": {},
   "source": [
    "## Execution Evaluation: DataCompy Score\n",
    "\n",
    "**OUTPUT:** F1 score of row-wise comparison - 1 if the provided Pandas DataFrames are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e8a4b-ba35-456a-96a2-e2bb91d7e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import collect_athena_metrics\n",
    "import json\n",
    "\n",
    "# Load evaluation dataset\n",
    "data = []\n",
    "with open('../eval_sql.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} evaluation queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45de3c3-7b21-4aa2-8b82-336c5094ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute queries and collect metrics\n",
    "print(\"Executing ground truth queries and collecting metrics ... \")\n",
    "ground_truth_metrics = []\n",
    "for item in tqdm(data, total=len(data)):\n",
    "    metrics = collect_athena_metrics(\n",
    "        sql_query=item[\"sql_query\"],\n",
    "        db_name=db_name,\n",
    "        s3_output=s3_output,\n",
    "        query_id=item[\"id\"],\n",
    "    )\n",
    "    ground_truth_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c052961-2ceb-4e2b-8492-cba9014b6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_results = \"../results/qwen3_gt_results.json\"\n",
    "ground_truth_metrics = json.load(open(ground_truth_results))\n",
    "\n",
    "base_results = \"../results/qwen3_base_results.json\"\n",
    "base_metrics = json.load(open(base_results))\n",
    "\n",
    "ft_results = \"../results/qwen3_ft_results.json\"\n",
    "ft_metrics = json.load(open(ft_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaec5f0-997e-4361-9c21-18d5a64bba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import DataCompyScore\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scorer = DataCompyScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519a9c5-8d62-4e8e-b648-44d49249a96e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "base_scores = []\n",
    "for response, reference  in tqdm(zip(base_metrics, ground_truth_metrics), total=len(base_metrics)):\n",
    "    sample = SingleTurnSample(\n",
    "        response=pd.DataFrame(response['result']).to_string(index=False),\n",
    "        reference=pd.DataFrame(reference['result']).to_string(index=False)\n",
    "    )\n",
    "    base_scores.append(await scorer.single_turn_ascore(sample))\n",
    "base_score = np.mean(np.nan_to_num(base_scores))\n",
    "\n",
    "# Evaluate custom model\n",
    "ft_scores = []\n",
    "for response, reference  in tqdm(zip(ft_metrics, ground_truth_metrics), total=len(ft_metrics)):\n",
    "    sample = SingleTurnSample(\n",
    "        response=pd.DataFrame(response['result']).to_string(index=False),\n",
    "        reference=pd.DataFrame(reference['result']).to_string(index=False)\n",
    "    )\n",
    "    ft_scores.append(await scorer.single_turn_ascore(sample))\n",
    "ft_score = np.mean(np.nan_to_num(ft_scores))\n",
    "\n",
    "print(f\"Base Model Score: {base_score}\")\n",
    "print(f\"Fine-Tuned Model Score: {ft_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
