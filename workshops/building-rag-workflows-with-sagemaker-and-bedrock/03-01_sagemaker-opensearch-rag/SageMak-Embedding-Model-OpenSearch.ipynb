{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082aa845",
   "metadata": {},
   "source": [
    "# Lab 3: Rag with Amazon SageMaker AI endpoint and Amazon OpenSearch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c50906",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook demonstrates how to implement a Retrieval Augmented Generation (RAG) solution using:\n",
    "- Amazon SageMaker for hosting embedding and LLM models\n",
    "- Amazon OpenSearch for vector search\n",
    "- LangChain for orchestrating the RAG pipeline\n",
    "\n",
    "In this notebook, Question Answering solution with Large Language Models (LLMs) and Amazon OpenSearch Service. An application using the RAG(Retrieval Augmented Generation) approach retrieves information most relevant to the userâ€™s request from the enterprise knowledge base or content, bundles it as context along with the userâ€™s request as a prompt, and then sends it to the LLM to get a GenAI response.\n",
    "\n",
    "LLMs have limitations around the maximum word count for the input prompt, therefore choosing the right passages among thousands or millions of documents in the enterprise, has a direct impact on the LLMâ€™s accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d37f81",
   "metadata": {},
   "source": [
    "<H2>Part 1: Build conversational search with OpenSearch Service</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7b731",
   "metadata": {},
   "source": [
    "The vector dataset used in this part of the lab is comprised of a predefined content resource from the [PubMedQA](https://pubmedqa.github.io/) dataset.\n",
    "\n",
    "You will use OpenSearch ingest pipeline with embedding processor to generate text embeddings for the dataset. Using the neural plugin in OpenSearch will allow you to generate the embeddings of the search query as well.\n",
    "You will then use the large language model (LLM) hosted on Amazon SageMaker endpoints with the RAG processor in the search pipeline to generate text. The RAG processor will combine the retrieved search results from OpenSearch with the generated answer from the LLM to send back to the end user.\n",
    "\n",
    "Follow step 1 to step 5 to complete part 1 of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ae065",
   "metadata": {},
   "source": [
    "### The key steps in part 1 of this lab are as follow:\n",
    "\n",
    "1. Get pre-requisites installed and libraries imported.\n",
    "1. Deploy the embedding model to a SageMaker endpoint, create a KNN-enabled index and ingest the catalog items into the index.\n",
    "1. Build the end-to-end pipeline with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be245f",
   "metadata": {},
   "source": [
    "# 1. Lab Pre-requisites\n",
    "This notebook is designed to be run as part of the larger workshop [placeholder for workshop].\n",
    "Before proceeding with this notebook, you should complete all of the steps.\n",
    "\n",
    "## Prerequisites\n",
    "- Required Python libraries: opensearch-py, langchain, boto3, requests_aws4auth\n",
    "- Access to Amazon SageMaker and OpenSearch\n",
    "- Appropriate IAM roles and permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df92a75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Import libraries & initialize resources\n",
    "The code blocks below will install and import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35fc57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opensearch-py -q\n",
    "!pip install opensearch_py_ml -q\n",
    "!pip install deprecated -q\n",
    "!pip install requests_aws4auth -q\n",
    "!pip install langchain boto3 -q\n",
    "print(\"Installs completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fdd173",
   "metadata": {},
   "source": [
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d38375",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import boto3\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "import os\n",
    "from os import path\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from ruamel.yaml import YAML\n",
    "from PIL import Image\n",
    "import base64\n",
    "import re\n",
    "import time as t\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "import requests\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a9721-d612-4701-a4ba-f8b4447a6f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    \n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed452d3d-d89e-48ad-bdcf-a2c9acacc820",
   "metadata": {},
   "source": [
    "# 2. Deploy the embedding model to a SageMaker endpoint & build retrieval integration with OpenSearch\n",
    "\n",
    "We have taken the PubMedQA dataset and prepared it to include the contexts in the `extracted_context.json` file.\n",
    "\n",
    "The following cells will perform the steps to generate embeddings with the dataset and ingest into the OpenSearch vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61449be0-4aae-49e4-81b9-f7cf0ec8379f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Establish a connection to the OpenSearch Service domain\n",
    "\n",
    "### OpenSearch Configuration\n",
    "- Establish connection to OpenSearch domain\n",
    "- Create index with KNN vector search capabilities\n",
    "- Define mapping for document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c32082-19b3-4f24-8260-d51118f409d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the Amazon OpenSearch Service domain endpoint info from DynamoDB\n",
    "session = boto3.Session()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = session.region_name\n",
    "\n",
    "aos_host = \"<TODO>\" # replace with the output opensearch cluster name, you can find it from the cloudformation output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797317a5-f6df-4c08-9a98-b81df5e1a456",
   "metadata": {},
   "source": [
    "### ðŸš¨ Authentication cell below ðŸš¨ \n",
    "The below cell establishes an authenticated connection to our OpenSearch Service domain. The connection will periodically expire.\n",
    "If you see an `AuthorizationException` error later in this notebook it means that the connection has expired and you just need to re-run the cell to get a new security tokken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10616439-4ed6-4faf-8157-da56df28d69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to OpenSearch using the IAM Role of this notebook\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    'es',\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "# Create OpenSearch client\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[f'https://{aos_host}'],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=60\n",
    ")\n",
    "print(\"Connection details: \")\n",
    "aos_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec7693-3a0b-41c7-b154-fed58a7502ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 Create the index with defined mappings.\n",
    "\n",
    "It is important to define the 'knn_vector' fields as without the propper definitions dynamic mapping would type these as simple float fields.\n",
    "\n",
    "A **k-NN (k-Nearest Neighbors)** enabled index is created in OpenSearch to store vector embeddings. The index schema defines:\n",
    "\n",
    "A **knn_vector** field (`context_vector`) for storing embeddings.\n",
    "\n",
    "To learn more about OpenSearch service, you can refer to the [document](https://aws.amazon.com/opensearch-service/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa795f-c62c-4fb3-9207-af0ffce9029f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create the k-NN index\n",
    "# Check if the index exists. Delete and recreate if it does. \n",
    "if aos_client.indices.exists(index='opensearch-rag-index'):\n",
    "    print(\"The index exists. Deleting...\")\n",
    "    response = aos_client.indices.delete(index='opensearch-rag-index')\n",
    "    \n",
    "payload = { \n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": True\n",
    "    }\n",
    "  },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"context_vector\": {\n",
    "              \"type\": \"knn_vector\",\n",
    "              \"dimension\": 384,\n",
    "              \"method\": {\n",
    "                \"engine\": \"faiss\",\n",
    "                \"space_type\": \"l2\",\n",
    "                \"name\": \"hnsw\",\n",
    "                \"parameters\": {}\n",
    "              }\n",
    "            },\n",
    "            \"template\": {\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "}\n",
    "\n",
    "print(\"Creating index...\")\n",
    "response = aos_client.indices.create(index='opensearch-rag-index',body=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7145ba-f124-4294-a518-b2dfa03d2fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Create SageMaker Embedding Endpoint\n",
    "A **Hugging Face text embedding model (all-MiniLM-L6-v2)** is deployed via SageMaker JumpStart to a SageMaker real-time endpoint. This model converts text into 384-dimensional vectors for semantic search.\n",
    "### Embedding Model Deployment\n",
    "- Deploy Hugging Face embedding model (all-MiniLM-L6-v2) on SageMaker\n",
    "- Create embedding endpoint for text vectorization\n",
    "- Configure content handlers for model input/output processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36c86e-8789-44e3-8f30-13e2638bbeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve the image uri based on instance type\n",
    "def get_image_uri(instance_type):\n",
    "    key = \"huggingface-tei\" if instance_type.startswith(\"ml.g\") or instance_type.startswith(\"ml.p\") else \"huggingface-tei-cpu\"\n",
    "    return get_huggingface_llm_image_uri(key, version=\"1.4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8cc50-c774-43c6-8782-ea3c14b1bf06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textembedding-all-MiniLM-L6-v2\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22e77e-24ef-4756-8c1f-59532311dce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2168a02-c548-4d83-b83b-f236586c2c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sagemaker config\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "emb_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=get_image_uri(instance_type),\n",
    "  model_data=model.model_data['S3DataSource']['S3Uri'],\n",
    "  env={'HF_MODEL_ID': \"/opt/ml/model\"}     # Path to the model in the container\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5e698-8eb4-4125-8058-98b196f92841",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy the model onto a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36119bab-9e34-4283-b904-1e46566009e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4299f-c79a-41bc-bf1f-911783db1ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_endpoint_name = predictor.endpoint_name\n",
    "print(f\"Successfully deployed embedding model to the SageMaker endpoint: {embed_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0974ac-ab63-4804-84b1-acd8a54960e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_text = \"Is adjustment for reporting heterogeneity necessary in sleep disorders?\"\n",
    "# invoke the embedding model\n",
    "input_str = {\"inputs\": query_text}\n",
    "output = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName=embed_endpoint_name,\n",
    "    Body=json.dumps(input_str),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "embeddings = output[\"Body\"].read().decode(\"utf-8\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda04dc-9ae0-41b2-b002-99a56ca7ee37",
   "metadata": {},
   "source": [
    "We can wrap up our SageMaker endpoints for embedding model into `langchain.embeddings.SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding model and can be use with other LangChain functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257c6fb-19a1-4add-84f5-f5b78f113f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            inputs: List of input strings.\n",
    "            model_kwargs: Additional keyword arguments to be passed to the endpoint.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        # Example: inference.py expects a JSON string with a \"inputs\" key:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        # Example: inference.py returns a JSON string with the list of\n",
    "        # embeddings in a \"vectors\" key:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # print(len(response_json))\n",
    "        return response_json\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "\n",
    "embeddings_function = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    region_name=region,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "query_result = embeddings_function.embed_query(query_text)\n",
    "print(\"Output:\\n\", query_result, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc8991-83b2-4ee4-a8e2-fb3fb2dcc9b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.4 Load data into the new index\n",
    "\n",
    "### Data Processing\n",
    "- Load and process input data\n",
    "- Generate embeddings for documents\n",
    "- Index documents with their embeddings in OpenSearch\n",
    "\n",
    "We will use the [bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) to load all of the products into our newly created index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d374d4-4453-4ba9-88fc-45e0b9945071",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, embed_endpoint_name, model_kwargs=None):\n",
    "    \"\"\"\n",
    "    Call the SageMaker embedding model to embed the given text.\n",
    "    Adjust the payload and response parsing according to your model's API.\n",
    "    \"\"\"\n",
    "    embeddings = SagemakerEndpointEmbeddings(\n",
    "        endpoint_name=embed_endpoint_name,\n",
    "        region_name=region,\n",
    "        content_handler=content_handler,\n",
    "    )\n",
    "\n",
    "    return embeddings.embed_query(text)\n",
    "\n",
    "get_embedding(query_text, embed_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920811c-b15e-4b58-8699-598cf9992e8e",
   "metadata": {},
   "source": [
    "- **Chunking**: Long documents are split into smaller passages (max 256 tokens) using LangChain's `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "- **Embedding Generation**: Each chunk is converted into a vector using the SageMaker embedding endpoint.\n",
    "\n",
    "- **Bulk Ingestion**: The embeddings and text are indexed into OpenSearch for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7077fed-8c54-4162-bde5-974713781a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Tokenizer\n",
    "\n",
    "# Initialize tokenizer matching your embedding model (e.g., \"sentence-transformers/all-mpnet-base-v2\")\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "\n",
    "# Configure splitter with model-aware tokenization\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=250,  # 256 - safety buffer\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\"],  # FIRST try splitting at paragraphs, then lines\n",
    "    keep_separator=True,  # Preserve paragraph/line breaks in chunks\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "def validate_chunk(chunk: str) -> bool:\n",
    "    \"\"\"Ensure chunk doesn't exceed token limit with model's actual tokenization\"\"\"\n",
    "    tokens = tokenizer.encode(chunk, add_special_tokens=True)\n",
    "    return len(tokens) <= 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25960198-b021-4717-80ac-8066c36f67ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_filename = \"extracted_context.json\"\n",
    "output_filename = \"output_embedded.jsonl\"  # Line-delimited JSON\n",
    "\n",
    "\n",
    "# Load the input JSON file (mapping IDs to lists of context strings)\n",
    "with open(input_filename, \"r\", encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "# Open the output file for writing line-delimited JSON objects\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for key, contexts in data.items():\n",
    "        embeddings = []\n",
    "        all_chunks = []\n",
    "        for ctx_idx, context in enumerate(contexts):\n",
    "                # First attempt: split at paragraphs/lines only\n",
    "                chunks = text_splitter.split_text(context)\n",
    "\n",
    "                # Second pass: check and fix any chunks that still exceed limits\n",
    "                final_chunks = []\n",
    "                for chunk in chunks:\n",
    "                    if validate_chunk(chunk):\n",
    "                        final_chunks.append(chunk)\n",
    "                    else:\n",
    "                        # Force split at sentences ONLY if absolutely necessary\n",
    "                        emergency_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                            tokenizer=tokenizer,\n",
    "                            chunk_size=250,\n",
    "                            chunk_overlap=50,\n",
    "                            separators=[\". \"],  # Only split sentences when forced\n",
    "                            keep_separator=True\n",
    "                        )\n",
    "                        final_chunks.extend(emergency_splitter.split_text(chunk))\n",
    "\n",
    "                # Embed validated chunks\n",
    "                for chunk_idx, chunk in enumerate(final_chunks):\n",
    "                    if not validate_chunk(chunk):\n",
    "                        continue  # Skip invalid chunks or handle differently\n",
    "\n",
    "                    embedding = get_embedding(chunk, embed_endpoint_name)\n",
    "                    output_obj = {\n",
    "                        \"id\": f\"{key}-{ctx_idx}-{chunk_idx}\",\n",
    "                        \"contexts\": chunk,\n",
    "                        \"context_vector\": embedding\n",
    "                    }\n",
    "                    outfile.write(json.dumps(output_obj) + \"\\n\")\n",
    "\n",
    "print(f\"Embeddings saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fdfb9-dadb-4635-9624-4ff715508e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read all JSON objects from the JSONL file\n",
    "with open(\"output_embedded.jsonl\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    json_objects = [json.loads(line) for line in infile]\n",
    "\n",
    "# Write the objects as a JSON array into a new .txt file\n",
    "with open(\"merged_output.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(json_objects, outfile, indent=4)\n",
    "\n",
    "print(\"Merged JSON objects have been saved to merged_output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9818c-929a-418d-970b-3fcb48ac4852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_file(input_filename, output_filename):\n",
    "    # Load the merged file, which is expected to be a JSON array\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
    "        records = json.load(infile)\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        # Process each record in the array\n",
    "        for record in records:\n",
    "            contexts = record.get(\"contexts\", [])\n",
    "            vectors = record.get(\"context_vector\", [])\n",
    "            # For each pair of context string and corresponding embedding vector:\n",
    "            # Create a new object without the \"id\" field.\n",
    "            new_obj = {\n",
    "                \"contexts\": contexts,\n",
    "                \"context_vector\": vectors\n",
    "            }\n",
    "            # Write the JSON object as a single line\n",
    "            outfile.write(json.dumps(new_obj) + \"\\n\")\n",
    "\n",
    "transform_file(\"merged_output.txt\", \"final_output_oneline.txt\")\n",
    "print(\"Transformation complete. Check final_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8332f3-8a4b-4036-b548-3181e8be0233",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Index TEXT file into index: opensearch-rag-index\n",
    "batch = 0\n",
    "count = 0\n",
    "batch_size = 5\n",
    "body_ = ''\n",
    "action = json.dumps({ 'index': { '_index': 'opensearch-rag-index' } })\n",
    "errors = []\n",
    "with open('final_output_oneline.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        if count > 5000:\n",
    "            break # Use this to run a limited number of items.\n",
    "        body_ = body_ + action + \"\\n\" + line + \"\\n\"\n",
    "        # print(f\"body: {body_}\")\n",
    "        if count % batch_size == 0 and count != 0:\n",
    "            batch+=1\n",
    "            if count % (batch_size*30) == 0:\n",
    "                print(\"Batch: \" + str(batch) + \", count: \" + str(count)+ \", errors: \" + str(len(errors)))\n",
    "            response = aos_client.bulk(\n",
    "                index = 'opensearch-rag-index',\n",
    "                body = body_\n",
    "            )\n",
    "            body_ = ''\n",
    "            if response['errors'] == True:\n",
    "                for item in response['items']:\n",
    "                    if item['index']['status'] != 201:\n",
    "                        errors.append(item['index']['error']) \n",
    "        # print(response)\n",
    "        # break \n",
    "        count += 1\n",
    "if body_ !=\"\":\n",
    "    response = aos_client.bulk(\n",
    "        index = 'opensearch-rag-index',\n",
    "        body = body_\n",
    "    )\n",
    "if response['errors'] == True:\n",
    "    for item in response['items']:\n",
    "        if item['index']['status'] != 201:\n",
    "            errors.append(item['index']['error'])\n",
    "print(\"Last batch: \" + str(batch) + \", documet count: \" + str(count)+ \", errors: \" + str(len(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7dfdad-d5ec-48f2-8a71-16bfc7700c00",
   "metadata": {},
   "source": [
    "## 2.5 Query OpenSearch Database\n",
    "\n",
    "Once the vectors are ingested into the database, we can run queries to retrieve relevant contexts based on the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb85e3e-5197-448a-b9f6-3aa5100b35c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your natural language query\n",
    "query_vector = get_embedding(query_text, embed_endpoint_name)\n",
    "\n",
    "# Now, use the embedding in a k-NN query\n",
    "knn_query = {\n",
    "    \"size\": 5,  # adjust how many results you want to retrieve\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"context_vector\": {\n",
    "                \"vector\": query_vector,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response_knn = aos_client.search(\n",
    "    index=\"opensearch-rag-index\",\n",
    "    body=knn_query\n",
    ")\n",
    "\n",
    "print(\"KNN Query Results:\")\n",
    "for hit in response_knn['hits']['hits']:\n",
    "    print(hit['_source'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a1587-fb44-417a-a4d5-9ed0ca54c8e9",
   "metadata": {},
   "source": [
    "# 3. Build end-to-end RAG pipeline with LLM models hosted on SageMaker AI and LangChain\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "1. **Generate embedings for each of document in the knowledge library with SageMaker hosted embedding model.**\n",
    "2. **Identify top K most relevant documents based on user query.**\n",
    "    - 2.1 **For a query of your interest, generate the embedding of the query using the same embedding model.**\n",
    "    - 2.2 **Search the indexes of top K most relevant documents in the embedding space using in-memory Faiss search.**\n",
    "    - 2.3 **Use the indexes to retrieve the corresponded documents.**\n",
    "3. **Combine the retrieved documents with prompt and question and send them into SageMaker LLM.**\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens. \n",
    "\n",
    "---\n",
    "To build a simiplied QA application with LangChain, we need: \n",
    "1. Wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. (We have already created the embedding SageMaker wrapper class in the previous section.\n",
    "2. Prepare the dataset to build the knowledge data base. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04f987-5478-40b4-be60-a978b7e9164f",
   "metadata": {},
   "source": [
    "Now we need to deploy a **Llama 3.1 8B LLM** onto a SageMaker real-time endpoint and prepare the SageMaker Endpoint class for LangChain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c61ebb-c73b-42e6-b6b5-3ebff6e25bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id_llm, model_version = \"meta-textgeneration-llama-3-1-8b-instruct\", \"*\"\n",
    "accept_eula = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56e839-dc01-4cfe-9212-3e46044b03b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = JumpStartModel(model_id=model_id_llm, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd3b77-7000-41c4-af67-18f7dcbf83fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714efbb9-200c-462c-8b70-d18ccb2bb499",
   "metadata": {},
   "source": [
    "Invoke the LLM endpoint for a quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1b459-3d6b-4372-b29c-5172e2a3a936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name = predictor.endpoint_name\n",
    "input_str = { \"inputs\": query_text, \n",
    "            \"parameters\": { \n",
    "                \"max_new_tokens\": 100, \n",
    "                \"top_p\": 0.9, \n",
    "                \"temperature\": 0.6 \n",
    "            }\n",
    "        }\n",
    "output = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName=llm_endpoint_name,\n",
    "    Body=json.dumps(input_str),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "embeddings = output[\"Body\"].read().decode(\"utf-8\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fbbc2-eccb-4297-b0a9-eb1fd11ac1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5c343-08c6-47dd-86a8-4182246413be",
   "metadata": {},
   "source": [
    "Next, we wrap up our SageMaker endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0d57c9dc-7439-40ce-a9cc-bfb0ecaca4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/30/25 08:38:10] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/30/25 08:38:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=118680;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=739362;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        self.len_prompt = len(prompt)\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = output.read()\n",
    "        res = json.loads(response_json)\n",
    "        \n",
    "        ans = res['generated_text']\n",
    "        # print(ans)\n",
    "        return ans \n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8f2eb-7a9c-4de3-a2f1-cd58183950a9",
   "metadata": {},
   "source": [
    "We combine the retrieved documents with prompt and question and send them into SageMaker LLM.\n",
    "\n",
    "We define a customized prompt as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbdbe9-79e3-4386-8b81-9d878f68f857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.:\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "opensearch_url = f\"https://{aos_host}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffe1a9-0e21-42f6-98cc-9b7f98a78674",
   "metadata": {},
   "source": [
    "For this example, we have created the OpenSearch cluster with this user-name and password. But in real application, we suggest you store the user name and password using services that can securely store the value, for example SecretsManager as [shown here](https://github.com/aws-samples/rag-with-amazon-opensearch-and-sagemaker/blob/main/app/opensearch_retriever_llama2.py#L89)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de73a3-cb00-4e96-9826-9a0d790e9bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "http_auth = (\"master\", \"ML-Search123!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74602217-4745-4a94-83a4-a27643a67daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opensearch_vector_search = OpenSearchVectorSearch(\n",
    "    opensearch_url=opensearch_url,\n",
    "    index_name='opensearch-rag-index',\n",
    "    embedding_function=embeddings_function,\n",
    "    http_auth=http_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f1c66-7358-41ac-8a88-04a4db8e3d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = opensearch_vector_search.as_retriever(\n",
    "    search_kwargs={\"k\": 3, \"vector_field\": \"context_vector\", \"text_field\": \"contexts\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aceb1b-764b-4c33-a80f-9120a0b87d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": True}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    sm_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    # return_source_documents=True, ## you can uncomment this line to see the detailed retrieved data source\n",
    "    # verbose=True, #DEBUG\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5c950-1a5f-4524-a287-908f6a26c11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736ab13-ab88-4bc3-9a4a-ab37ea551094",
   "metadata": {},
   "source": [
    "### Key Workflow Summary\n",
    "- Data Preparation: Text is split into chunks and embedded.\n",
    "- OpenSearch Setup: A vector index is created and populated.\n",
    "- Model Deployment: Embedding and LLM models are hosted on SageMaker.\n",
    "- RAG Pipeline: Queries retrieve relevant context, and the LLM generates answers.\n",
    "\n",
    "This notebook provides an end-to-end example of building a production-ready RAG system using AWS services. The same approach can be adapted for other domains by replacing the dataset and fine-tuning the models.\n",
    "\n",
    "# Congratulations for finishing Lab 3. Now please continue on to the next Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c772362-75ae-47c0-b5f5-be04a2b46f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
