{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693c66ec",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with Serverless customization on SageMaker AI\n",
    "\n",
    "## Lab 3 - LLM Evaluation\n",
    "\n",
    "In this notebook, we are going to run an Evaluation job on the fine-tuned model by using LLM as a Judge with Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d6578",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc40c5a",
   "metadata": {},
   "source": [
    "### Prerequistes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25caddc",
   "metadata": {},
   "source": [
    "#### Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee71cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "sess = Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sess = Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=sess.boto_region_name)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7d13b",
   "metadata": {},
   "source": [
    "Edit model package group name and model package version if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.ai_registry.dataset import DataSet\n",
    "from sagemaker.core.resources import ModelPackageGroup\n",
    "\n",
    "base_model_id = \"huggingface-llm-qwen2-5-7b-instruct\"\n",
    "\n",
    "model_package_group_name = f\"{base_model_id}-mpg\"\n",
    "model_package_version = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DataSet.get(name=\"medical-o1-reasoning-sft-test\")\n",
    "print(f\"Test dataset for evaluation: {test_dataset}\")\n",
    "model_package_group = ModelPackageGroup.get(model_package_group_name)\n",
    "\n",
    "fine_tuned_model_package_group_arn = model_package_group.model_package_group_arn\n",
    "print(f\"Fine-tuned Model Package Group ARN: {fine_tuned_model_package_group_arn}\")\n",
    "\n",
    "fine_tuned_model_package_arn = f\"{model_package_group.model_package_group_arn.replace(\"model-package-group\", \"model-package\", 1)}/{model_package_version}\"\n",
    "print(f\"Fine-tuned Model Package ARN: {fine_tuned_model_package_arn}\")\n",
    "\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{base_model_id}/evaluation\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{base_model_id}/evaluation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802fa62",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb8bc4",
   "metadata": {},
   "source": [
    "### Create custom metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3905a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATOR_MODEL = \"amazon.nova-pro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILTIN_METRICS = [\"Correctness\", \"Completeness\", \"Faithfulness\", \"Coherence\"]\n",
    "\n",
    "custom_metrics_list = [\n",
    "    {\n",
    "        \"customMetricDefinition\": {\n",
    "            \"name\": \"MedicalReasoningQuality\",\n",
    "            \"instructions\": (\n",
    "                \"Evaluate if the response demonstrates sound medical reasoning. \"\n",
    "                \"Check if the thinking process logically connects symptoms, conditions, and conclusions. \"\n",
    "                \"Prompt: {{prompt}}\\nResponse: {{prediction}}\"\n",
    "            ),\n",
    "            \"ratingScale\": [\n",
    "                {\n",
    "                    \"definition\": \"Excellent - Clear logical chain from symptoms to diagnosis/answer\",\n",
    "                    \"value\": {\"floatValue\": 3},\n",
    "                },\n",
    "                {\n",
    "                    \"definition\": \"Adequate\",\n",
    "                    \"value\": {\"floatValue\": 2},\n",
    "                },\n",
    "                {\n",
    "                    \"definition\": \"Poor\",\n",
    "                    \"value\": {\"floatValue\": 1},\n",
    "                },\n",
    "                {\n",
    "                    \"definition\": \"Incorrect\",\n",
    "                    \"value\": {\"floatValue\": 0},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"customMetricDefinition\": {\n",
    "            \"name\": \"ClinicalAccuracy\",\n",
    "            \"instructions\": (\n",
    "                \"Assess if the medical facts, terminology, and clinical conclusions are accurate. \"\n",
    "                \"Consider diagnoses, treatments, mechanisms, and medical concepts mentioned. \"\n",
    "                \"Prompt: {{prompt}}\\nResponse: {{prediction}}\\nReference: {{ground_truth}}\"\n",
    "            ),\n",
    "            \"ratingScale\": [\n",
    "                {\n",
    "                    \"definition\": \"Good\",\n",
    "                    \"value\": {\"floatValue\": 1},\n",
    "                },\n",
    "                {\n",
    "                    \"definition\": \"Bad\",\n",
    "                    \"value\": {\"floatValue\": 0},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"customMetricDefinition\": {\n",
    "            \"name\": \"ThinkTagStructure\",\n",
    "            \"instructions\": (\n",
    "                \"Check if the response follows the expected format with reasoning in <think> tags \"\n",
    "                \"followed by a clear final answer outside the tags. \"\n",
    "                \"Prompt: {{prompt}}\\nResponse: {{prediction}}\"\n",
    "            ),\n",
    "            \"ratingScale\": [\n",
    "                {\n",
    "                    \"definition\": \"Good\",\n",
    "                    \"value\": {\"floatValue\": 1},\n",
    "                },\n",
    "                {\n",
    "                    \"definition\": \"Bad\",\n",
    "                    \"value\": {\"floatValue\": 0},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "custom_metrics_json = json.dumps(custom_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb79b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.evaluate import LLMAsJudgeEvaluator\n",
    "\n",
    "evaluator = LLMAsJudgeEvaluator(\n",
    "    model=fine_tuned_model_package_arn,\n",
    "    model_package_group=fine_tuned_model_package_group_arn,\n",
    "    evaluator_model=EVALUATOR_MODEL,  # Required\n",
    "    dataset=test_dataset,  # Required: S3 URI or Dataset ARN\n",
    "    builtin_metrics=BUILTIN_METRICS,  # Optional: Can combine with custom metrics\n",
    "    custom_metrics=custom_metrics_json,  # Optional: JSON string of custom metrics\n",
    "    s3_output_path=output_path,  # Required\n",
    "    evaluate_base_model=False,  # Skip base model evaluation to evaluate only custom model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14198b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f80fa",
   "metadata": {},
   "source": [
    "### Analyze evaluation results\n",
    "\n",
    "In this section we will further analyze the LLMAJ evaluation results produced by SageMaker AI serverless evaluation jobs, which are still accessible on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d068c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "from sagemaker.train.common_utils import show_results_utils\n",
    "from sagemaker.train.evaluate import EvaluationPipelineExecution\n",
    "from sagemaker.train.evaluate.constants import EvalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a04539",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_succeeded = next(\n",
    "    (\n",
    "        e\n",
    "        for e in EvaluationPipelineExecution.get_all(eval_type=EvalType.LLM_AS_JUDGE)\n",
    "        if e.status.overall_status == \"Succeeded\"\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "pprint(latest_succeeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42db7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_original_format = show_results_utils._format_score\n",
    "show_results_utils._format_score = lambda s: (\n",
    "    f\"{s * 100:.1f}%\" if s is not None else \"N/A\"\n",
    ")\n",
    "\n",
    "latest_succeeded.show_results(limit=5, offset=0, show_explanations=False)\n",
    "\n",
    "show_results_utils._format_score = _original_format  # restore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d55b34",
   "metadata": {},
   "source": [
    "#### Download results\n",
    "\n",
    "First we download the results from S3 as JSONL files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = urlparse(latest_succeeded.s3_output_path)\n",
    "bucket = parsed.netloc\n",
    "prefix = parsed.path.lstrip(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket, Prefix=f\"{prefix}/custom-llmaj-eval-{latest_succeeded.name}\"\n",
    ")\n",
    "\n",
    "# Find the jsonl file\n",
    "jsonl_key = next(\n",
    "    obj[\"Key\"] for obj in response[\"Contents\"] if obj[\"Key\"].endswith(\"_output.jsonl\")\n",
    ")\n",
    "\n",
    "os.mkdir(\"./tmp\", exist_ok=True)\n",
    "s3_client.download_file(bucket, jsonl_key, \"./tmp/evaluation_results.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eae723",
   "metadata": {},
   "source": [
    "#### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb1623",
   "metadata": {},
   "source": [
    "Utility functions used to create different charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ff136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_results(filepath):\n",
    "    \"\"\"Load evaluation results from JSONL file into DataFrame.\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        for score in r[\"automatedEvaluationResult\"][\"scores\"]:\n",
    "            rows.append({\"metric\": score[\"metricName\"], \"score\": score[\"result\"]})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_metrics_bar(df):\n",
    "    \"\"\"Horizontal bar chart of average scores by metric.\"\"\"\n",
    "    agg = df.groupby(\"metric\")[\"score\"].mean().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.barh(agg.index, agg.values, color=\"steelblue\")\n",
    "    plt.xlabel(\"Average Score\")\n",
    "    plt.title(\"LLM-as-Judge Evaluation Results\")\n",
    "    plt.xlim(0, 1)\n",
    "\n",
    "    for bar, val in zip(bars, agg.values):\n",
    "        plt.text(\n",
    "            val + 0.02, bar.get_y() + bar.get_height() / 2, f\"{val:.1%}\", va=\"center\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_radar(df):\n",
    "    \"\"\"Radar chart showing all metrics.\"\"\"\n",
    "    agg = df.groupby(\"metric\")[\"score\"].mean()\n",
    "    metrics = agg.index.tolist()\n",
    "    values = agg.values.tolist() + [agg.values[0]]\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist() + [0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, values, \"o-\", linewidth=2, color=\"steelblue\")\n",
    "    ax.fill(angles, values, alpha=0.25, color=\"steelblue\")\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([m.replace(\"Builtin.\", \"\") for m in metrics], size=9)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\"Evaluation Metrics Overview\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_bullet(df, target=0.8):\n",
    "    \"\"\"Bullet chart comparing scores against target.\"\"\"\n",
    "    agg = df.groupby(\"metric\")[\"score\"].mean().sort_values()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    y_pos = range(len(agg))\n",
    "    ax.barh(y_pos, [1] * len(agg), color=\"#eee\", height=0.6)\n",
    "    ax.barh(y_pos, [target] * len(agg), color=\"#ddd\", height=0.6)\n",
    "    ax.barh(y_pos, agg.values, color=\"steelblue\", height=0.3)\n",
    "    ax.axvline(target, color=\"red\", linestyle=\"--\", label=f\"Target ({target:.0%})\")\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(agg.index)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_title(\"Metrics vs Target\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_evaluation_results(\"evaluation_results.jsonl\")\n",
    "plot_metrics_bar(df)\n",
    "plot_metrics_radar(df)\n",
    "plot_metrics_bullet(df, target=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312-sm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
