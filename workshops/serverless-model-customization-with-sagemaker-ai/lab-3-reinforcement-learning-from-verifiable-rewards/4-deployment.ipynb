{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e853a14",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from Verifiable Rewards (RLVR) with SageMaker\n",
    "\n",
    "## Lab 4 — LLM Deployment\n",
    "\n",
    "In this lab, you will deploy the RLVR fine-tuned model to a **SageMaker real-time endpoint** and test it with a math question.\n",
    "\n",
    "### Deployment architecture\n",
    "\n",
    "SageMaker real-time endpoints use an **Inference Component** pattern that decouples the endpoint infrastructure from the model:\n",
    "\n",
    "1. **Endpoint Configuration** — defines the instance type and routing strategy\n",
    "2. **Endpoint** — provisions the compute infrastructure\n",
    "3. **Model** — registers the fine-tuned weights with a serving container (DJL LMI + vLLM)\n",
    "4. **Inference Component** — attaches the model to the endpoint with specific resource requirements\n",
    "\n",
    "This separation lets you host multiple models on a single endpoint or swap models without recreating the infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "### Set up the SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from rich.pretty import pprint\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "sess = Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sess = Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=sess.boto_region_name)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_retrieval_header",
   "metadata": {},
   "source": [
    "### Retrieve the fine-tuned model\n",
    "\n",
    "We look up the Model Package from Lab 2 and extract the S3 URI of the merged model weights. We also generate unique resource names to avoid collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4206486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker.core.resources import ModelPackage, ModelPackageGroup\n",
    "from sagemaker.core import s3\n",
    "\n",
    "base_model_id = \"huggingface-llm-qwen2-5-7b-instruct\"\n",
    "model_name = f\"{base_model_id}-rlvr-{random.randint(100, 100000)}\"\n",
    "\n",
    "model_package_group_name = f\"{base_model_id}-rlvr\"\n",
    "model_package_version = \"1\"\n",
    "\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "ic_name = f\"{model_name}-ic\"\n",
    "region = sess.boto_region_name\n",
    "\n",
    "model_package_group = ModelPackageGroup.get(model_package_group_name)\n",
    "\n",
    "fine_tuned_model_package_arn = f\"{model_package_group.model_package_group_arn.replace('model-package-group', 'model-package', 1)}/{model_package_version}\"\n",
    "print(f\"Fine-tuned Model Package ARN: {fine_tuned_model_package_arn}\")\n",
    "\n",
    "model_package = ModelPackage.get(fine_tuned_model_package_arn)\n",
    "\n",
    "merged_model_s3_uri = s3.s3_path_join(\n",
    "    model_package.inference_specification.containers[0].model_data_source.s3_data_source.s3_uri,\n",
    "    \"checkpoints\", \"hf_merged\"\n",
    ") + \"/\"\n",
    "print(f\"Merged model S3 URI: {merged_model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endpoint_config_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Create the endpoint\n",
    "\n",
    "We first create an **Endpoint Configuration** that specifies the instance type (`ml.g5.2xlarge` — a single NVIDIA A10G GPU) and routing strategy, then create the **Endpoint** itself.\n",
    "\n",
    "> **⏱ Expected duration:** The endpoint takes a few minutes to reach `InService` status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8664f29-9626-4d77-8275-fb32134bd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Endpoint, EndpointConfig\n",
    "from sagemaker.core.shapes import ProductionVariant\n",
    "\n",
    "print(f\"Creating EndpointConfig: {endpoint_config_name}\")\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=endpoint_config_name,\n",
    "    execution_role_arn=role,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"AllTraffic\",\n",
    "            instance_type=\"ml.g5.2xlarge\",\n",
    "            initial_instance_count=1,\n",
    "            model_data_download_timeout_in_seconds=700,\n",
    "            routing_config={\"routing_strategy\": \"LEAST_OUTSTANDING_REQUESTS\"}\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81084de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating Endpoint: {endpoint_name}\")\n",
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_config_name=endpoint_config_name\n",
    ")\n",
    "endpoint.wait_for_status(\"InService\")\n",
    "print(f\"Endpoint {endpoint_name} is InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Create the model and inference component\n",
    "\n",
    "We register the fine-tuned model using the **DJL LMI** (Large Model Inference) container with **vLLM** as the serving backend. The environment variables configure:\n",
    "\n",
    "- `OPTION_MAX_MODEL_LEN` — maximum sequence length (16K tokens)\n",
    "- `OPTION_TENSOR_PARALLEL_DEGREE` — set to `max` to use all available GPUs\n",
    "- `OPTION_ENTRYPOINT` — the vLLM async serving engine\n",
    "\n",
    "Then we create an **Inference Component** that attaches the model to the endpoint with specific memory and accelerator requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = \"0.36.0-lmi18.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "\n",
    "lmi_env = {\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"16384\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecb663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Model\n",
    "from sagemaker.core.shapes import ContainerDefinition, ModelDataSource, S3ModelDataSource\n",
    "\n",
    "fine_tuned_model = Model.create(\n",
    "    model_name=model_name,\n",
    "    primary_container=ContainerDefinition(\n",
    "        image=inference_image,\n",
    "        model_data_source=ModelDataSource(\n",
    "            s3_data_source=S3ModelDataSource(\n",
    "                s3_uri=merged_model_s3_uri,\n",
    "                s3_data_type=\"S3Prefix\",\n",
    "                compression_type=\"None\"\n",
    "            )\n",
    "        ),\n",
    "        environment=lmi_env\n",
    "    ),\n",
    "    execution_role_arn=role\n",
    ")\n",
    "\n",
    "pprint(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import InferenceComponent\n",
    "from sagemaker.core.shapes import (\n",
    "    InferenceComponentSpecification,\n",
    "    InferenceComponentComputeResourceRequirements,\n",
    "    InferenceComponentRuntimeConfig,\n",
    ")\n",
    "\n",
    "inference_component = InferenceComponent.create(\n",
    "    inference_component_name=ic_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    variant_name=\"AllTraffic\",\n",
    "    specification=InferenceComponentSpecification(\n",
    "        model_name=model_name,\n",
    "        compute_resource_requirements=InferenceComponentComputeResourceRequirements(\n",
    "            min_memory_required_in_mb=10240,\n",
    "            number_of_accelerator_devices_required=1,\n",
    "        )\n",
    "    ),\n",
    "    runtime_config=InferenceComponentRuntimeConfig(\n",
    "        copy_count=1\n",
    "    ),\n",
    "    region=region\n",
    ")\n",
    "\n",
    "print(f\"InferenceComponent created: {inference_component.inference_component_name}\")\n",
    "print(f\"Endpoint ARN: {endpoint.endpoint_arn}\")\n",
    "inference_component.wait_for_status(\"InService\")\n",
    "print(f\"InferenceComponent {ic_name} is InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Test the endpoint\n",
    "\n",
    "Let's send a math question to the deployed model using streaming inference. The helper functions below handle the SageMaker Runtime API call and parse the streaming response tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3970d-4f6e-4268-bcbd-fe11562d17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "\n",
    "\n",
    "def execute_inference(prompt, endpoint_name, inference_component_name, stream=True):\n",
    "    sm_rt_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.1, \"top_p\": 0.9},\n",
    "    }\n",
    "\n",
    "    if stream:\n",
    "        payload[\"stream\"] = True\n",
    "        result = sm_rt_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            InferenceComponentName=inference_component_name,\n",
    "            CustomAttributes='accept_eula=true',\n",
    "            Body=json.dumps(payload),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "        return result['Body']\n",
    "    else:\n",
    "        result = sm_rt_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            InferenceComponentName=inference_component_name,\n",
    "            CustomAttributes='accept_eula=true',\n",
    "            Body=json.dumps(payload),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "        return result[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "\n",
    "\n",
    "def print_stream(stream):\n",
    "    for line in LineIterator(stream):\n",
    "        try:\n",
    "            if line != b'':\n",
    "                resp = json.loads(line)\n",
    "                print(resp[\"token\"].get(\"text\"), end='')\n",
    "        except:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f55f2-ae88-48db-ae24-a68a26c68f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is 25 * 4 + 10? Show your reasoning step by step.\"\n",
    "\n",
    "stream = execute_inference(prompt, endpoint_name, ic_name, stream=True)\n",
    "print_stream(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Clean up resources\n",
    "\n",
    "Uncomment and run the cells below to delete the endpoint and avoid ongoing charges. Resources must be deleted in reverse order: inference component → model → endpoint → endpoint config.\n",
    "\n",
    "> **⚠️ Important:** Remember to clean up after completing the workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_component.delete()\n",
    "# fine_tuned_model.delete()\n",
    "# endpoint.delete()\n",
    "# endpoint_config.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
