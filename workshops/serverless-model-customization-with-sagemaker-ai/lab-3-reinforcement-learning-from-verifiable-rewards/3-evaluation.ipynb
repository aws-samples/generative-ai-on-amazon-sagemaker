{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693c66ec",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from Verifiable Rewards (RLVR) with SageMaker\n",
    "\n",
    "## Lab 3 — Benchmark Evaluation\n",
    "\n",
    "In this lab, you will evaluate the RLVR fine-tuned model against the **MATH benchmark** and compare it to the base model to measure the improvement in mathematical reasoning.\n",
    "\n",
    "### Why benchmark evaluation?\n",
    "\n",
    "RLVR trains models on tasks with objectively verifiable answers, so it makes sense to evaluate them the same way — using a standardized benchmark with known correct answers rather than subjective LLM-as-a-Judge scoring.\n",
    "\n",
    "### What you'll do in this notebook\n",
    "\n",
    "1. Retrieve the fine-tuned model from the Model Registry\n",
    "2. Explore available benchmarks and create a `BenchMarkEvaluator`\n",
    "3. Run evaluation on both the fine-tuned and base model\n",
    "4. Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "### Set up the SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee71cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "sess = Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sess = Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=sess.boto_region_name)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "### Retrieve the fine-tuned model\n",
    "\n",
    "We look up the Model Package Group created in Lab 2 and construct the ARN for the first model version. We also set an S3 output path for the evaluation artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import ModelPackageGroup\n",
    "\n",
    "base_model_id = \"huggingface-llm-qwen2-5-7b-instruct\"\n",
    "model_package_group_name = f\"{base_model_id}-rlvr\"\n",
    "model_package_version = \"1\"\n",
    "\n",
    "model_package_group = ModelPackageGroup.get(model_package_group_name)\n",
    "\n",
    "fine_tuned_model_package_arn = f\"{model_package_group.model_package_group_arn.replace('model-package-group', 'model-package', 1)}/{model_package_version}\"\n",
    "print(f\"Fine-tuned Model Package ARN: {fine_tuned_model_package_arn}\")\n",
    "\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{base_model_id}/evaluation\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{base_model_id}/evaluation\"\n",
    "\n",
    "print(f\"Evaluation output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Explore available benchmarks\n",
    "\n",
    "SageMaker provides several built-in benchmarks. Let's list them and inspect the properties of the **MATH** benchmark we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list_benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.evaluate import BenchMarkEvaluator, get_benchmarks, get_benchmark_properties\n",
    "from rich.pretty import pprint\n",
    "\n",
    "Benchmark = get_benchmarks()\n",
    "pprint(list(Benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark_props",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(get_benchmark_properties(benchmark=Benchmark.MATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluator_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Create and run the benchmark evaluator\n",
    "\n",
    "The `BenchMarkEvaluator` runs the selected benchmark against your model. Setting `evaluate_base_model=True` also evaluates the original base model so you can directly compare the two.\n",
    "\n",
    "> **⏱ Expected duration:** 15–30 minutes when evaluating both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb79b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BenchMarkEvaluator(\n",
    "    benchmark=Benchmark.MATH,\n",
    "    model=fine_tuned_model_package_arn,\n",
    "    model_package_group=model_package_group_name,\n",
    "    s3_output_path=output_path,\n",
    "    evaluate_base_model=True,\n",
    ")\n",
    "\n",
    "pprint(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = evaluator.evaluate()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. View evaluation results\n",
    "\n",
    "We retrieve the latest succeeded benchmark evaluation and display the results. The output shows the MATH benchmark scores for both the base model and the RLVR fine-tuned model side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a04539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.evaluate import EvaluationPipelineExecution\n",
    "from sagemaker.train.evaluate.constants import EvalType\n",
    "\n",
    "latest_succeeded = next(\n",
    "    (\n",
    "        e\n",
    "        for e in EvaluationPipelineExecution.get_all(eval_type=EvalType.BENCHMARK)\n",
    "        if e.status.overall_status == \"Succeeded\"\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "pprint(latest_succeeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42db7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_succeeded:\n",
    "    latest_succeeded.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've completed the RLVR lab! Here's what you accomplished:\n",
    "\n",
    "1. **Lab 1** — Prepared the GSM8K dataset in RLVR format and registered it in the AI Registry\n",
    "2. **Lab 2** — Launched a serverless RLVR training job with the `RLVRTrainer`\n",
    "3. **Lab 3** — Evaluated the fine-tuned model against the MATH benchmark and compared it to the base model\n",
    "\n",
    "The benchmark scores show how RLVR training improved the model's mathematical reasoning — without any human feedback or a separate reward model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
