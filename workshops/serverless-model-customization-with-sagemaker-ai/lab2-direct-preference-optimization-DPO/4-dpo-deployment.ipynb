{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e853a14",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO) Training with SageMaker\n",
    "\n",
    "## Lab 4 - LLM Deployment\n",
    "\n",
    "In this notebook, we are going to deploy the fine-tuned LLM using SageMaker Real-time endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6949b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0a20b",
   "metadata": {},
   "source": [
    "### Prerequistes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f431a9f",
   "metadata": {},
   "source": [
    "#### Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from rich.pretty import pprint\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "sess = Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sess = Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=sess.boto_region_name)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89088cc1",
   "metadata": {},
   "source": [
    "Edit model package group name and model package version if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4206486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker.core.resources import ModelPackage, ModelPackageGroup\n",
    "\n",
    "base_model_id = \"meta-textgeneration-llama-3-2-1b-instruct\"\n",
    "model_name = f\"{base_model_id}-dpo-{random.randint(100, 100000)}\"\n",
    "\n",
    "model_package_group_name = f\"{base_model_id}-dpo\"\n",
    "model_package_version = \"1\"\n",
    "\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "ic_name = f\"{model_name}-ic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core import s3\n",
    "\n",
    "model_package_group = ModelPackageGroup.get(model_package_group_name)\n",
    "\n",
    "fine_tuned_model_package_group_arn = model_package_group.model_package_group_arn\n",
    "print(f\"Fine-tuned Model Package Group ARN: {fine_tuned_model_package_group_arn}\")\n",
    "\n",
    "fine_tuned_model_package_arn = f\"{model_package_group.model_package_group_arn.replace(\"model-package-group\", \"model-package\", 1)}/{model_package_version}\"\n",
    "print(f\"Fine-tuned Model Package ARN: {fine_tuned_model_package_arn}\")\n",
    "\n",
    "model_package = ModelPackage.get(fine_tuned_model_package_arn)\n",
    "\n",
    "# get the merged model artifact and deploy it\n",
    "merged_model_s3_uri = s3.s3_path_join(model_package.inference_specification.containers[0].model_data_source.s3_data_source.s3_uri, \"checkpoints\", \"hf_merged\")+ \"/\"\n",
    "print(merged_model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42771136",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee351ee0",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "\n",
    "Utility functions to check the creation status of endpoints and inference components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a65760",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f461ff",
   "metadata": {},
   "source": [
    "### Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e1659",
   "metadata": {},
   "source": [
    "Define inference configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8664f29-9626-4d77-8275-fb32134bd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker.core.resources import Endpoint, EndpointConfig\n",
    "from sagemaker.core.shapes import ProductionVariant\n",
    "\n",
    "print(f\"Creating EndpointConfig: {endpoint_config_name}\")\n",
    "endpoint_config=EndpointConfig.create(\n",
    "    endpoint_config_name=endpoint_config_name,\n",
    "    execution_role_arn=role,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"AllTraffic\",\n",
    "            instance_type=instance_type,\n",
    "            initial_instance_count=1,\n",
    "            model_data_download_timeout_in_seconds=health_check_timeout,\n",
    "            routing_config={\"routing_strategy\": \"LEAST_OUTSTANDING_REQUESTS\"}\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07dafe",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "A SageMaker Endpoint is a fully managed, always-on HTTPS API that hosts your deployed model and serves real-time inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81084de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating Endpoint: {endpoint_name}\")\n",
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_config_name=endpoint_config_name\n",
    ")\n",
    "endpoint.wait_for_status(\"InService\")\n",
    "print(f\"Endpoint {endpoint_name} is InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb6a0d",
   "metadata": {},
   "source": [
    "### Create Model from Model Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99d052",
   "metadata": {},
   "source": [
    "Get the image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ac311-ba69-4380-9861-dcc6d1c18c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sess.boto_region_name\n",
    "CONTAINER_VERSION = \"0.36.0-lmi18.0.0-cu128\"\n",
    "\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_env = {\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"16384\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecb663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Model\n",
    "from sagemaker.core.resources import TrainingJob\n",
    "from sagemaker.core.shapes import ContainerDefinition, ModelDataSource, S3ModelDataSource\n",
    "\n",
    "fine_tuned_model = Model.create(\n",
    "    model_name=model_name,\n",
    "    primary_container=ContainerDefinition(\n",
    "        image=inference_image,\n",
    "        model_data_source=ModelDataSource(\n",
    "            s3_data_source=S3ModelDataSource(\n",
    "                s3_uri=merged_model_s3_uri ,\n",
    "                s3_data_type=\"S3Prefix\",\n",
    "                compression_type=\"None\"\n",
    "            )\n",
    "        ),\n",
    "        environment=lmi_env\n",
    "    ),\n",
    "    execution_role_arn=role\n",
    ")\n",
    "\n",
    "pprint(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171776f",
   "metadata": {},
   "source": [
    "### Create Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import InferenceComponent\n",
    "from sagemaker.core.shapes import (\n",
    "    InferenceComponentSpecification,\n",
    "    InferenceComponentContainerSpecification,\n",
    "    InferenceComponentComputeResourceRequirements,\n",
    "    InferenceComponentRuntimeConfig,\n",
    ")\n",
    "\n",
    "# Step 3: Create InferenceComponent\n",
    "inference_component = InferenceComponent.create(\n",
    "    inference_component_name=ic_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    variant_name=\"AllTraffic\",\n",
    "    specification=InferenceComponentSpecification(\n",
    "        model_name=model_name,\n",
    "        compute_resource_requirements=InferenceComponentComputeResourceRequirements(\n",
    "            min_memory_required_in_mb=10240,\n",
    "            number_of_accelerator_devices_required=1,\n",
    "        )\n",
    "    ),\n",
    "    runtime_config=InferenceComponentRuntimeConfig(\n",
    "        copy_count=1\n",
    "    ),\n",
    "    region=region\n",
    ")\n",
    "\n",
    "print(f\"InferenceComponent created: {inference_component.inference_component_name}\")\n",
    "print(f\"Endpoint ARN: {endpoint.endpoint_arn}\")\n",
    "inference_component.wait_for_status(\"InService\")\n",
    "print(f\"Endpoint {ic_name} is InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58db98",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed6e4c",
   "metadata": {},
   "source": [
    "### Test endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df90b8-7cd8-4915-a9f9-299eb92b7159",
   "metadata": {},
   "source": [
    "### Utility interface for interacting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4eabbf-a716-43a8-84a7-bdfff800cf95",
   "metadata": {},
   "source": [
    "Utility function to invoke the model and parse the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3970d-4f6e-4268-bcbd-fe11562d17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import io\n",
    "\n",
    "def execute_inference(prompt, endpoint_name, inference_component_name, stream=True):\n",
    "    sm_rt_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    if stream:\n",
    "        result = sm_rt_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            InferenceComponentName=inference_component_name,\n",
    "            CustomAttributes='accept_eula=true',\n",
    "            Body=json.dumps(\n",
    "                {\n",
    "                    \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\n{0}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\".format(prompt),\n",
    "                    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.1, \"top_p\": 0.9},\n",
    "                    \"stream\": stream\n",
    "                }\n",
    "            ),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "        return result['Body']\n",
    "\n",
    "    else:\n",
    "        result = sm_rt_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            InferenceComponentName=inference_component_name,\n",
    "            CustomAttributes='accept_eula=true',\n",
    "            Body=json.dumps(\n",
    "                {\n",
    "                    \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\n{0}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\".format(prompt),\n",
    "                    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.1, \"top_p\": 0.9}\n",
    "                }\n",
    "            ),\n",
    "            ContentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        return result[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "def build_div(text):\n",
    "    return \"\"\"\n",
    "<div style=\"background-color: lightblue; padding: 10px; margin: 10px; border-radius: 5px;\">\n",
    "    {0}\n",
    "</div>\n",
    "\"\"\".format(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ff40f-ceab-40d6-9dec-90dbb3f656b1",
   "metadata": {},
   "source": [
    "Utility class to parse streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e8148-0198-44d1-a504-9c46c70c8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "\n",
    "\n",
    "def print_stream(stream):\n",
    "    for line in LineIterator(stream):\n",
    "        try:\n",
    "            if line != b'':\n",
    "                resp = json.loads(line)\n",
    "                print(resp[\"token\"].get(\"text\"), end='')\n",
    "        except:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f55f2-ae88-48db-ae24-a68a26c68f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"I've always struggled with math - can you explain how fractals work in a way that's easy to understand?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a11db3-6f77-4f6c-8019-761f882e0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = execute_inference(prompt, endpoint_name, ic_name, stream=True)\n",
    "print_stream(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d7fc7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49cb0c",
   "metadata": {},
   "source": [
    "### Delete resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "base_model_id = base_model_id\n",
    "\n",
    "model_name = f\"{base_model_id}-sft\"\n",
    "endpoint_config_name = f\"{base_model_id}-sft-config\"\n",
    "endpoint_name = f\"{base_model_id}-sft-endpoint\"\n",
    "ic_name = f\"{base_model_id}-sft-ic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete inference component\n",
    "inference_component.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a27b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "fine_tuned_model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint (optional - if you want to remove the endpoint too)\n",
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint config (optional)\n",
    "endpoint_config.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
