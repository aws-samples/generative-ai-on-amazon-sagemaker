{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe766eb7",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO) Training with SageMaker\n",
    "\n",
    "This notebook demonstrates how to use the **DPOTrainer** to fine-tune large language models using Direct Preference Optimization (DPO). DPO is a technique that trains models to align with human preferences by learning from preference data without requiring a separate reward model.\n",
    "\n",
    "## What is DPO?\n",
    "\n",
    "Direct Preference Optimization (DPO) is a method for training language models to follow human preferences. Unlike traditional RLHF (Reinforcement Learning from Human Feedback), DPO directly optimizes the model using preference pairs without needing a reward model.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Simpler than RLHF - no reward model required\n",
    "- More stable training process\n",
    "- Direct optimization on preference data\n",
    "- Works with LoRA for efficient fine-tuning\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Prepare Preference Dataset**: Upload preference data in JSONL format\n",
    "2. **Register Dataset**: Create a SageMaker AI Registry dataset\n",
    "3. **Configure DPO Trainer**: Set up model, training parameters, and resources\n",
    "4. **Execute Training**: Run the DPO fine-tuning job\n",
    "5. **Track Results**: Monitor training with MLflow integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e9836",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6dccaa",
   "metadata": {},
   "source": [
    "### Prerequistes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6580037",
   "metadata": {},
   "source": [
    "#### Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bee7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "sess = Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sess = Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=sess.boto_region_name)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a604450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.ai_registry.dataset import DataSet\n",
    "\n",
    "# Required config\n",
    "base_model_id = \"meta-textgeneration-llama-3-2-1b-instruct\"\n",
    "training_dataset = DataSet.get(name=\"humanlike-dpo-train\")\n",
    "validation_dataset = DataSet.get(name=\"humanlike-dpo-val\")\n",
    "\n",
    "# Optional Configs\n",
    "mlflow_resource_arn = \"arn:aws:sagemaker:<region>:<account>:mlflow-app/app-xxxxxxx\" # If you do not want to use the default app\n",
    "job_name = f\"dpo-{base_model_id.split('/')[-1].replace('.', '-')}\"\n",
    "mlflow_experiment_name = \"humanlike-llama3-2-1b-dpo\"\n",
    "\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{base_model_id}-dpo\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{base_model_id}-dpo\"\n",
    "\n",
    "os.environ[\"SAGEMAKER_MLFLOW_CUSTOM_ENDPOINT\"] = (\n",
    "    f\"https://mlflow.sagemaker.{sess.boto_region_name}.app.aws\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69cbc0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead4b63",
   "metadata": {},
   "source": [
    "### Create Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463d77a-91c7-42ea-82f8-8612bbfb8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import ModelPackageGroup\n",
    "\n",
    "model_package_group_name = f\"{base_model_id}-dpo\"\n",
    "\n",
    "model_package_group = ModelPackageGroup.create(\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_package_group_description='store models from SageMaker serverless customization' #Required Description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ffb26-5752-40b0-841c-758abb66d893",
   "metadata": {},
   "source": [
    "# Part 1: Configure and Execute DPO Training\n",
    "\n",
    "### Step 1: Creating the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc292279-4f3a-431c-82ef-3c18cc26cdf0",
   "metadata": {},
   "source": [
    "#### Create DPO Trainer (Direct Preference Optimization)\n",
    "\n",
    "Direct Preference Optimization (DPO) is a method for training language models to follow human preferences. Unlike traditional RLHF (Reinforcement Learning from Human Feedback), DPO directly optimizes the model using preference pairs without needing a reward model.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Simpler than RLHF - no reward model required\n",
    "- More stable training process\n",
    "- Direct optimization on preference data\n",
    "- Works with LoRA for efficient fine-tuning\n",
    "\n",
    "##### Key Parameters:\n",
    "- `model` Base model to fine-tune (from SageMaker Hub)\n",
    "- `training_type` Fine-tuning method (LoRA recommended for efficiency)\n",
    "- `training_dataset` ARN of the registered preference dataset. Training Dataset - either Dataset ARN or S3 Path of the dataset (Please note these are required for a training job to run, can be either provided via Trainer or .train())\n",
    "- `model_package_group` Where to store the fine-tuned model\n",
    "- `mlflow_resource_arn`: MLFlow app ARN to track the training job (optional)\n",
    "- `mlflow_experiment_name`: MLFlow app experiment name(str) (optional)\n",
    "- `mlflow_run_name`: MLFlow app run name(str) (optional) not sure what this is?\n",
    "- `validation_dataset`: Validation Dataset - either Dataset ARN or S3 Path of the dataset (optional)\n",
    "- `s3_output_path`: S3 path for the trained model artifacts (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d396a-23e8-4aee-8679-c01b0741b0fd",
   "metadata": {},
   "source": [
    "### Training Features:\n",
    "- **Serverless Training**: Automatically managed compute resources\n",
    "- **LoRA Integration**: Parameter-efficient fine-tuning\n",
    "- **MLflow Tracking**: Automatic experiment and metrics logging\n",
    "- **Model Versioning**: Automatic model package creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4e6e4",
   "metadata": {},
   "source": [
    "### Run Severless Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7abd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.dpo_trainer import DPOTrainer\n",
    "from sagemaker.train.common import TrainingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2382bfa-1b98-4561-8c22-d3ecfa389c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=base_model_id,\n",
    "    training_type=TrainingType.LORA,\n",
    "    model_package_group=model_package_group,\n",
    "    training_dataset=training_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    s3_output_path=output_path,\n",
    "    mlflow_resource_arn=mlflow_resource_arn,\n",
    "    mlflow_experiment_name=mlflow_experiment_name,\n",
    "    base_job_name=job_name,\n",
    "    sagemaker_session=sess,\n",
    "    accept_eula=True,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567c89d",
   "metadata": {},
   "source": [
    "Print Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac695f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from rich.pretty import pprint\n",
    "\n",
    "print(\"Default Finetuning options:\")\n",
    "pprint(trainer.hyperparameters.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec060925",
   "metadata": {},
   "source": [
    "Override Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a21213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.hyperparameters.learning_rate = 0.0001\n",
    "trainer.hyperparameters.global_batch_size = 64\n",
    "trainer.hyperparameters.max_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModified/user defined options:\")\n",
    "pprint(trainer.hyperparameters.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from rich.pretty import pprint\n",
    "\n",
    "training_job = trainer.train(wait=True)\n",
    "\n",
    "TRAINING_JOB_NAME = training_job.training_job_name\n",
    "\n",
    "pprint(training_job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
