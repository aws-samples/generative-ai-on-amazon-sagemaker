{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f45f8ce7",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with PEFT of Amazon Nova using Amazon SageMaker Training Job\n",
    "\n",
    "In this notebook, we fine-tune LLM on Amazon SageMaker AI, using Python scripts and SageMaker ModelTrainer for executing a training job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae95dd9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1352",
   "metadata": {},
   "source": [
    "## Lab 1: Prerequisites\n",
    "\n",
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ef8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d20e0c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be805a",
   "metadata": {},
   "source": [
    "## Lab 2: Prepare the dataset\n",
    "\n",
    "We are going to load [HuggingFaceH4/Multilingual-Thinking](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17016773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"HuggingFaceH4/Multilingual-Thinking\", split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=10, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of val elements: \", len(val))\n",
    "print(\"Number of test elements: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52f2d2",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "model_id = \"o200k_base\"\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count number of tokens in a string\"\"\"\n",
    "    encoding = tiktoken.get_encoding(model_id)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def count_message_tokens(messages_dict):\n",
    "    \"\"\"Count total tokens in the entire message structure\"\"\"\n",
    "    # Convert the entire message structure to JSON string and count tokens\n",
    "    message_json = json.dumps(messages_dict, ensure_ascii=False)\n",
    "    return count_tokens(message_json)\n",
    "\n",
    "\n",
    "def truncate_entire_message(messages, max_tokens=30000):\n",
    "    \"\"\"Truncate the entire message structure if it's still too long\"\"\"\n",
    "    current_tokens = count_message_tokens(messages)\n",
    "\n",
    "    if current_tokens <= max_tokens:\n",
    "        return messages\n",
    "\n",
    "    # Calculate proportional limits based on max_tokens\n",
    "    think_limit = max(500, max_tokens // 8)  # ~12.5% for thinking\n",
    "    content_limit = max(1000, max_tokens // 4)  # ~25% for regular content\n",
    "\n",
    "    # Strategy: Remove or truncate the longest content sections first\n",
    "    for message in messages.get(\"messages\", []):\n",
    "        if message.get(\"role\") == \"assistant\":\n",
    "            for content in message.get(\"content\", []):\n",
    "                if \"text\" in content:\n",
    "                    text = content[\"text\"]\n",
    "                    # If this is a think section, truncate it more aggressively\n",
    "                    if text.startswith(\"<think>\"):\n",
    "                        # Extract just the thinking part\n",
    "                        think_start = text.find(\"<think>\") + 7\n",
    "                        think_end = text.find(\"</think>\")\n",
    "                        if think_end != -1:\n",
    "                            thinking_content = text[think_start:think_end]\n",
    "                            # Truncate thinking to smaller proportional size\n",
    "                            truncated_thinking = truncate_message(\n",
    "                                thinking_content, max_tokens=think_limit\n",
    "                            )\n",
    "                            content[\"text\"] = f\"<think>\\n{truncated_thinking}\\n</think>\"\n",
    "                        else:\n",
    "                            # If malformed, just truncate the whole thing\n",
    "                            content[\"text\"] = truncate_message(\n",
    "                                text, max_tokens=think_limit\n",
    "                            )\n",
    "                    else:\n",
    "                        # Regular content, truncate with proportional limit\n",
    "                        content[\"text\"] = truncate_message(\n",
    "                            text, max_tokens=content_limit\n",
    "                        )\n",
    "\n",
    "        # Check if we're under the limit now\n",
    "        current_tokens = count_message_tokens(messages)\n",
    "        if current_tokens <= max_tokens:\n",
    "            break\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def truncate_message(message_str, max_tokens=20000):\n",
    "    \"\"\"Truncate the text based on token count, not character count\"\"\"\n",
    "    current_tokens = count_tokens(message_str)\n",
    "\n",
    "    if current_tokens <= max_tokens:\n",
    "        return message_str\n",
    "\n",
    "    print(f\"Truncating message from {current_tokens} to ~{max_tokens} tokens...\")\n",
    "\n",
    "    # Get the encoding\n",
    "    encoding = tiktoken.get_encoding(model_id)\n",
    "\n",
    "    # Encode the text to tokens\n",
    "    tokens = encoding.encode(message_str)\n",
    "\n",
    "    # Truncate to max_tokens - 3 (to leave room for \"...\")\n",
    "    truncated_tokens = tokens[: max_tokens - 3]\n",
    "\n",
    "    # Decode back to text and add ellipsis\n",
    "    truncated_text = encoding.decode(truncated_tokens) + \"...\"\n",
    "\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e658df",
   "metadata": {},
   "source": [
    "Let's format the dataset by using the prompt style for Amazon Nova:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\"text\": Content of the user prompt]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\"text\": Content of the answer]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34270434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "max_tokens = 3072\n",
    "max_tokens_think = 2250\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    \"\"\"Prepare dataset in the required format for Nova models\"\"\"\n",
    "    messages = {\"system\": [], \"messages\": []}\n",
    "\n",
    "    for el in sample[\"messages\"]:\n",
    "        if el[\"role\"] == \"system\":\n",
    "            system_prompt = \"\"\"\n",
    "            You are an AI assistant that thinks in {language} but responds in English.\n",
    "\n",
    "            IMPORTANT: Follow this exact format for every response:\n",
    "            1. First, write your reasoning and thoughts inside <think>...</think> tags\n",
    "            2. Then, provide your final answer in English\n",
    "\n",
    "            Always think through the problem in {language}, then translate your conclusion to English for the final response.\n",
    "            \"\"\"\n",
    "\n",
    "            system_prompt = system_prompt.format(language=sample[\"reasoning_language\"])\n",
    "            system_prompt = textwrap.dedent(system_prompt).strip()\n",
    "\n",
    "            messages[\"system\"].append({\"text\": system_prompt.lower()})\n",
    "        elif el[\"role\"] == \"user\":\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": el[\"content\"].lower()}]}\n",
    "            )\n",
    "        else:\n",
    "            if (\n",
    "                el[\"thinking\"] is not None\n",
    "                and el[\"thinking\"] != \"\"\n",
    "                and el[\"thinking\"] != \"null\"\n",
    "            ):\n",
    "                # Truncate thinking content by tokens, not characters\n",
    "                reasoning_text = truncate_message(\n",
    "                    el[\"thinking\"].lower(), max_tokens=max_tokens_think\n",
    "                )  # Leave room for other content\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\"text\": f\"<think>\\n{reasoning_text}\\n</think>\"},\n",
    "                            {\"text\": el[\"content\"].lower()},\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                content_text = truncate_message(\n",
    "                    el[\"content\"].lower(), max_tokens=max_tokens\n",
    "                )\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\"text\": content_text},\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Check total token count and further truncate if needed\n",
    "    total_tokens = count_message_tokens(messages)\n",
    "    if total_tokens > max_tokens:\n",
    "        print(\n",
    "            f\"Warning: Total message tokens ({total_tokens}) still too high, applying additional truncation...\"\n",
    "        )\n",
    "        messages = truncate_entire_message(messages, max_tokens=max_tokens)\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 3072\n",
    "max_tokens_think = 2250\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    \"\"\"Parse sample and format it for test dataset.\"\"\"\n",
    "    message = {\n",
    "        \"system\": \"\",\n",
    "        \"query\": \"\",\n",
    "        \"response\": \"\",\n",
    "    }\n",
    "\n",
    "    for el in sample[\"messages\"]:\n",
    "        if el[\"role\"] == \"system\":\n",
    "            system_prompt = \"\"\"\n",
    "            You are an AI assistant that thinks in {language} but responds in English.\n",
    "\n",
    "            IMPORTANT: Follow this exact format for every response:\n",
    "            1. First, write your reasoning and thoughts inside <think>...</think> tags\n",
    "            2. Then, provide your final answer in English\n",
    "\n",
    "            Always think through the problem in {language}, then translate your conclusion to English for the final response.\n",
    "            \"\"\"\n",
    "\n",
    "            system_prompt = system_prompt.format(language=sample[\"reasoning_language\"])\n",
    "            system_prompt = textwrap.dedent(system_prompt).strip()\n",
    "\n",
    "            message[\"system\"] = system_prompt.lower()\n",
    "        elif el[\"role\"] == \"user\":\n",
    "            message[\"query\"] = el[\"content\"].lower()\n",
    "        else:\n",
    "            if (\n",
    "                el[\"thinking\"] is not None\n",
    "                and el[\"thinking\"] != \"\"\n",
    "                and el[\"thinking\"] != \"null\"\n",
    "            ):\n",
    "                reasoning_text = truncate_message(\n",
    "                    el[\"thinking\"].lower(), max_tokens=max_tokens_think\n",
    "                )\n",
    "                message[\"response\"] = (\n",
    "                    f\"<think>\\n{reasoning_text}\\n</think>\\n{el['content']}\".lower()\n",
    "                )\n",
    "            else:\n",
    "                message[\"response\"] = truncate_message(\n",
    "                    el[\"content\"].lower(), max_tokens=max_tokens\n",
    "                )\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38bf93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset})\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=list(dataset[\"train\"].features)\n",
    ")\n",
    "\n",
    "print(json.dumps(train_dataset[randint(0, len(train_dataset) - 1)], indent=2))\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(\n",
    "    prepare_dataset, remove_columns=list(dataset[\"val\"].features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd3b99",
   "metadata": {},
   "source": [
    "The validation dataset will be formatted with the structure below:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": \"Optional - String containing the system prompt, that sets the behavior, role, or personality of the model\",\n",
    "    \"query\": \"String containing the input prompt\",\n",
    "    \"response\": \"String containing the expected model output\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c7fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset_test, remove_columns=list(dataset[\"test\"].features)\n",
    ")\n",
    "\n",
    "print(json.dumps(test_dataset[randint(0, len(test_dataset) - 1)], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b853dac",
   "metadata": {},
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda5ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-sft-peft\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-sft-peft\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/gen_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dataset.to_json(\"./data/train/dataset.jsonl\")\n",
    "val_dataset.to_json(\"./data/val/dataset.jsonl\")\n",
    "test_dataset.to_json(\"./data/test/gen_qa.jsonl\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/gen_qa.jsonl\", bucket_name, f\"{input_path}/test/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(val_dataset_s3_path)\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26cd13",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f91f9",
   "metadata": {},
   "source": [
    "## Lab 3: Model fine-tuning\n",
    "\n",
    "We now define the PyTorch estimator to run the a Supervised fine-tuning (SFT) workload on the formatted tool-calling dataset for Amazon Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d04f10b",
   "metadata": {},
   "source": [
    "Let's define the container to execute the SFT workload for Amazon Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-fine-tune-repo:SM-TJ-SFT-ai-league\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = \"./recipes/nova_micro_1_0_g5_g6_12x_gpu_lora_sft.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba95c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-nova-micro-sft-peft\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"training_config\": {\n",
    "        \"trainer\": {\n",
    "            \"max_epochs\": epochs,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"optim\": {\n",
    "                \"lr\": 2e-5,\n",
    "                \"weight_decay\": 0.01,\n",
    "                \"sched\": {\"warmup_steps\": 50},\n",
    "            },\n",
    "            \"peft\": {\n",
    "                \"peft_scheme\": \"lora\",\n",
    "                \"lora_tuning\": {\n",
    "                    \"alpha\": 32,\n",
    "                    \"adapter_dropout\": 0.1,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    tags=[{\"Key\": \"max_epochs\", \"Value\": str(epochs)}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")\n",
    "\n",
    "val_input = TrainingInput(\n",
    "    s3_data=val_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f49779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(inputs={\"train\": train_input, \"validation\": val_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019969d5-6016-47a7-9a65-3f6ac30f7b79",
   "metadata": {},
   "source": [
    "You can monitor the job directly from your notebook output. You can also refer the SageMaker AI console, which shows the status of the job and the corresponding CloudWatch logs for governance and observability, as shown in the following screenshots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
