{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee19860",
   "metadata": {},
   "source": [
    "## Lab 4: Model deployment and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f15640",
   "metadata": {},
   "source": [
    "After training and evaluating our model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Bedrock Custom Model feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models. This provides:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa5aaa",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc00adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\"bedrock\", region_name=sess.boto_region_name)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac03806-95e8-4792-b2fa-0c215784fecc",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d7fe34-43a3-4b4a-99f0-ab95c5256a0d",
   "metadata": {},
   "source": [
    "Retrieve last completed job from SageMaker AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282c39e-7bfa-42d3-b78c-3918f54c5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_job_name(job_name_prefix):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "    matching_jobs = []\n",
    "    next_token = None\n",
    "\n",
    "    while True:\n",
    "        # Prepare the search parameters\n",
    "        search_params = {\n",
    "            'Resource': 'TrainingJob',\n",
    "            'SearchExpression': {\n",
    "                'Filters': [\n",
    "                    {\n",
    "                        'Name': 'TrainingJobName',\n",
    "                        'Operator': 'Contains',\n",
    "                        'Value': job_name_prefix\n",
    "                    },\n",
    "                    {\n",
    "                        'Name': 'TrainingJobStatus',\n",
    "                        'Operator': 'Equals',\n",
    "                        'Value': \"Completed\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            'SortBy': 'CreationTime',\n",
    "            'SortOrder': 'Descending',\n",
    "            'MaxResults': 100\n",
    "        }\n",
    "\n",
    "        # Add NextToken if we have one\n",
    "        if next_token:\n",
    "            search_params['NextToken'] = next_token\n",
    "\n",
    "        # Make the search request\n",
    "        search_response = sagemaker_client.search(**search_params)\n",
    "\n",
    "        # Filter and add matching jobs\n",
    "        matching_jobs.extend([\n",
    "            job['TrainingJob']['TrainingJobName'] \n",
    "            for job in search_response['Results']\n",
    "            if job['TrainingJob']['TrainingJobName'].startswith(job_name_prefix)\n",
    "        ])\n",
    "\n",
    "        # Check if we have more results to fetch\n",
    "        next_token = search_response.get('NextToken')\n",
    "        if not next_token or matching_jobs:  # Stop if we found at least one match or no more results\n",
    "            break\n",
    "\n",
    "    if not matching_jobs:\n",
    "        print(f\"No completed training jobs found starting with prefix '{job_name_prefix}'\")\n",
    "        return None\n",
    "\n",
    "    return matching_jobs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e176-134c-4e28-bcd0-3527df45a791",
   "metadata": {},
   "source": [
    "Get Checkpoint configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc0acb-a3ac-4904-bdfd-6ed8dee395c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_checkpoint_s3_uri(training_job_name):\n",
    "    sagemaker = boto3.client('sagemaker')\n",
    "    try:\n",
    "        response = sagemaker.describe_training_job(TrainingJobName=training_job_name)\n",
    "        checkpoint_config = response['CheckpointConfig']['S3Uri']\n",
    "        return checkpoint_config\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving checkpoint configuration: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d234005-14ba-4b51-a7e9-efd3cb67188b",
   "metadata": {},
   "source": [
    "Extract the right checkpoint path from the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9f838-0d58-4358-930a-baece4970031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def filter_s3_paths(message, base_path):\n",
    "    pattern = r's3://[^\\s]+'\n",
    "    matches = re.findall(pattern, message)\n",
    "    return [match.rstrip('.') for match in matches if base_path in match]\n",
    "\n",
    "def get_logs_containing_text(log_group_name, search_text, region='us-east-1'):\n",
    "    logs_client = boto3.client('logs', region_name=region)\n",
    "    matching_events = []\n",
    "    \n",
    "    paginator = logs_client.get_paginator('filter_log_events')\n",
    "    for page in paginator.paginate(logGroupName=log_group_name):\n",
    "        for event in page['events']:\n",
    "            if search_text in event['message']:\n",
    "                matching_events.append(event)\n",
    "    \n",
    "    return matching_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6139dde3-8da1-4146-815b-d5e458f09a78",
   "metadata": {},
   "source": [
    "Utility functions to check the manifest.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d99e2-2899-4d6d-8bf8-5186fad884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "def extract_tar_gz(tar_path, extract_to='.'):\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(extract_to)\n",
    "\n",
    "def download_s3_file(s3_path, local_path):\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = s3_path.split('/')[2]\n",
    "    key = '/'.join(s3_path.split('/')[3:])\n",
    "    s3.download_file(bucket, key, local_path)\n",
    "\n",
    "def get_checkpoint_path(manifest_path):\n",
    "    with open(manifest_path) as f:\n",
    "        return json.load(f)['checkpoint_s3_bucket']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1623605-23ec-4e4d-9aaf-43198a5398c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T11:48:17.340258Z",
     "iopub.status.busy": "2025-10-27T11:48:17.339978Z",
     "iopub.status.idle": "2025-10-27T11:48:17.345003Z",
     "shell.execute_reply": "2025-10-27T11:48:17.344234Z",
     "shell.execute_reply.started": "2025-10-27T11:48:17.340235Z"
    }
   },
   "source": [
    "#### Update model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c23ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = get_last_job_name(\"train-nova-micro-sft-peft\")\n",
    "\n",
    "print(\"Job name:\", job_name)\n",
    "\n",
    "if job_name:\n",
    "    model_path = get_sagemaker_checkpoint_s3_uri(job_name)\n",
    "\n",
    "    print(\"Model path:\", model_path)\n",
    "\n",
    "    # Define name for imported model\n",
    "    imported_model_name = \"nova-micro-sagemaker-sft-peft-reasoning\"\n",
    "\n",
    "    # Define the guardrail ID (Optional)\n",
    "    guardrail_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19254ce1",
   "metadata": {},
   "source": [
    "### Creating the Bedrock Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    request_params = {\n",
    "        \"modelName\": imported_model_name,\n",
    "        \"modelSourceConfig\": {\n",
    "            \"s3DataSource\": {\n",
    "                \"s3Uri\": model_path,\n",
    "            }\n",
    "        },\n",
    "        \"roleArn\": role,\n",
    "        \"clientRequestToken\": \"NovaRecipeSageMaker\",\n",
    "    }\n",
    "    response = bedrock.create_custom_model(**request_params)\n",
    "    model_arn = response[\"modelArn\"]\n",
    "\n",
    "    print(\"/***************************************************/\")\n",
    "    print(f\"Model import job created with ARN: {model_arn}\")\n",
    "    print(\"/***************************************************/\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ValidationException':\n",
    "        print(\"S3 URI invalid, downloading manifest to find correct path...\")\n",
    "\n",
    "        try:\n",
    "            download_s3_file(f\"s3://{sess.default_bucket()}/train-nova-micro-sft-peft/{job_name}/output/output.tar.gz\", \"./tmp_output/output.tar.gz\")\n",
    "            extract_tar_gz('./tmp_output/output.tar.gz', './tmp_output')\n",
    "            model_path = get_checkpoint_path('./tmp_output/manifest.json')\n",
    "\n",
    "            request_params[\"modelSourceConfig\"][\"s3DataSource\"][\"s3Uri\"] = model_path\n",
    "            response = bedrock.create_custom_model(**request_params)\n",
    "            model_arn = response[\"modelArn\"]\n",
    "\n",
    "            print(\"/***************************************************/\")\n",
    "            print(f\"Model import job created with ARN: {model_arn}\")\n",
    "            print(\"/***************************************************/\")\n",
    "        except:\n",
    "            print(\"Manifest not found, searching CloudWatch logs for correct path...\")\n",
    "\n",
    "            logs = get_logs_containing_text(\n",
    "                '/aws/sagemaker/TrainingJobs',\n",
    "                model_path\n",
    "            )\n",
    "\n",
    "            for log in logs:\n",
    "                s3_paths = filter_s3_paths(log['message'], model_path)\n",
    "                if s3_paths:\n",
    "                    model_path = s3_paths[0]\n",
    "                    print(f\"Found S3 path in logs: {model_path}\")\n",
    "                    break\n",
    "\n",
    "            request_params[\"modelSourceConfig\"][\"s3DataSource\"][\"s3Uri\"] = model_path\n",
    "            response = bedrock.create_custom_model(**request_params)\n",
    "            model_arn = response[\"modelArn\"]\n",
    "\n",
    "            print(\"/***************************************************/\")\n",
    "            print(f\"Model import job created with ARN: {model_arn}\")\n",
    "            print(\"/***************************************************/\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6d208",
   "metadata": {},
   "source": [
    "### Monitoring the Model status\n",
    "\n",
    "After initiating the model import, we need to monitor its progress. The status goes through several states:\n",
    "\n",
    "* CREATING: Model is being imported\n",
    "* ACTIVE: Import successful\n",
    "* FAILED: Import encountered errors\n",
    "\n",
    "This cell polls the Bedrock API every 60 seconds to check the status of the model import, continuing until it reaches a terminal state (ACTIVE or FAILED). Once the import completes successfully, we'll have the model ARN which we can use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61321d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "model_arn = None\n",
    "\n",
    "while True:\n",
    "    response = bedrock.list_custom_models(sortBy='CreationTime',sortOrder='Descending')\n",
    "    model_summaries = response[\"modelSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"modelName\"] == imported_model_name:\n",
    "            status = model[\"modelStatus\"].upper()\n",
    "            model_arn = model[\"modelArn\"]\n",
    "            print(f'{model[\"modelStatus\"].upper()} {model[\"modelArn\"]} ...')\n",
    "            if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "\n",
    "model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96091ba9",
   "metadata": {},
   "source": [
    "##### ⚠️ After the model is ACTIVE, deploy a custom model for on-demand inference!\n",
    "\n",
    "Please refer to the official [AWS Documentation](https://docs.aws.amazon.com/nova/latest/userguide/deploy-custom-model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_params = {\n",
    "    \"clientRequestToken\": \"NovaRecipeSageMakerODI\",\n",
    "    \"modelDeploymentName\": f\"{imported_model_name}-odi\",\n",
    "    \"modelArn\": model_arn,\n",
    "}\n",
    "\n",
    "response = bedrock.create_custom_model_deployment(**request_params)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef19d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "custom_model_arn = None\n",
    "\n",
    "while True:\n",
    "    response = bedrock.list_custom_model_deployments(\n",
    "        sortBy=\"CreationTime\", sortOrder=\"Descending\"\n",
    "    )\n",
    "    model_summaries = response[\"modelDeploymentSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"customModelDeploymentName\"] == f\"{imported_model_name}-odi\":\n",
    "            status = model[\"status\"].upper()\n",
    "            custom_model_arn = model[\"customModelDeploymentArn\"]\n",
    "            print(f'{model[\"status\"].upper()} {model[\"customModelDeploymentArn\"]} ...')\n",
    "            if status in [\"CREATING\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "\n",
    "custom_model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73800829",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ca59c-1a6b-4420-824b-a30ad116e8c1",
   "metadata": {},
   "source": [
    "### Testing the Deployed Model\n",
    "\n",
    "Now that our model is deployed to Amazon Bedrock, we can invoke it for inference. We'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75ce42-ced1-43d1-9516-d0d4c727f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sess.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac1cb5-489e-4549-a706-fa5eaf169cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    guardrail_id=None,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        guardrail_id (str): Identifier of the guardrail to apply\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "    if guardrail_id:\n",
    "        kwargs[\"guardrailConfig\"] = {\n",
    "            \"guardrailIdentifier\": guardrail_id,\n",
    "            \"guardrailVersion\": \"1\",\n",
    "        }\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6f6b1-ae0a-4f8e-a9fd-80175d2a76ce",
   "metadata": {},
   "source": [
    "Use the custom model deployment ARN created with the Bedrock Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77272c7a-f360-459d-902a-b84bdbe5bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "model_arn = (\n",
    "    custom_model_arn\n",
    "    if custom_model_arn is not None\n",
    "    else \"<BEDROCK_CUSTOM_MODEL_DEPLOYMENT_ARN>\"\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant that thinks in {language} but responds in English.\n",
    "\n",
    "IMPORTANT: Follow this exact format for every response:\n",
    "1. First, write your reasoning and thoughts inside <think>...</think> tags\n",
    "2. Then, provide your final answer in English\n",
    "\n",
    "Always think through the problem in {language}, then translate your conclusion to English for the final response.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = system_prompt.format(language=\"Spanish\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"Hello, how are you?\"\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "response = generate(\n",
    "    model_id=model_arn,\n",
    "    system_prompt=textwrap.dedent(system_prompt).strip(),\n",
    "    messages=messages,\n",
    "    max_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    guardrail_id=guardrail_id,\n",
    ")\n",
    "\n",
    "print(json.dumps(response[\"output\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69520d8-fed4-4ca9-b8a8-f222b40683ed",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8895be5",
   "metadata": {},
   "source": [
    "## Lab 5: Model evaluation\n",
    "\n",
    "We are now going to evaluate the model with the LLM as a Judge evaluation task. First, let's define the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sess.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7006f",
   "metadata": {},
   "source": [
    "To invoke the fine-tuned model for inference, we'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25277f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    guardrail_id=None,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        guardrail_id (str): Identifier of the guardrail to apply\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "    if guardrail_id:\n",
    "        kwargs[\"guardrailConfig\"] = {\n",
    "            \"guardrailIdentifier\": guardrail_id,\n",
    "            \"guardrailVersion\": \"1\",\n",
    "        }\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a5871",
   "metadata": {},
   "source": [
    "#### Generate model inference on the created test dataset\n",
    "\n",
    "In the following cell, we are going to invoke the model to create the dataset for LLM as a judge.\n",
    "\n",
    "The required dataset structure for LLM as a judge task is:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"String containing the input prompt and instructions.\",\n",
    "    \"response_A\": \"String containing the ground truth output\",\n",
    "    \"response_B\": \"String containing the customized model output.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promt_base = f\"\"\"\n",
    "You are an expert LLM evaluator tasked with comparing responses from a base model and a fine-tuned model to determine if the fine-tuned model has successfully learned to follow specific formatting and reasoning instructions.\n",
    "\n",
    "## EVALUATION CRITERIA\n",
    "\n",
    "The fine-tuned model should demonstrate the following behaviors that the base model likely does not:\n",
    "\n",
    "1. **Reasoning Language Compliance**: The model should think/reason in the language specified in the system prompt's \"reasoning language\" field\n",
    "2. **Structured Thinking**: All reasoning must be enclosed within `<think>...</think>` tags\n",
    "3. **English Response**: The final answer/response must be in English, regardless of the reasoning language\n",
    "4. **Reasoning Quality**: The thinking process should be coherent and relevant to the user's question\n",
    "\n",
    "## EVALUATION PROCESS\n",
    "\n",
    "For each model response, evaluate:\n",
    "\n",
    "### Format Compliance\n",
    "- **Think Tags Present**: Does the response contain `<think>...</think>` tags?\n",
    "\n",
    "- **Language Separation**: Is there clear separation between reasoning language and English response?\n",
    "### Content Quality\n",
    "- **Reasoning Coherence**: Is the thinking process logical and relevant?\n",
    "\n",
    "- **Response Completeness** Does the English response adequately address the user's question?\n",
    "\n",
    "### Instruction Following\n",
    "- **System Prompt Adherence** : Does the model follow the specific instructions in the system prompt?\n",
    "\n",
    "## OUTPUT FORMAT\n",
    "\n",
    "For each evaluation, provide:\n",
    "\n",
    "1. **Format Compliance Score**: X/60\n",
    "   - Language Separation: X/40 (explanation)\n",
    "   - Think Tags: X/20 (explanation)\n",
    "2. **Instruction Following Score**: X/30 (explanation)\n",
    "3. **Content Quality Score**: X/10\n",
    "   - Reasoning Coherence: X/5 (explanation)\n",
    "   - Response Completeness: X/5 (explanation)\n",
    "4. **Total Score**: X/100\n",
    "5. **Overall Assessment**: [Brief summary of strengths and weaknesses]\n",
    "6. **Recommendation**: [Which model performed better and why]\n",
    "\n",
    "## SPECIAL CONSIDERATIONS\n",
    "\n",
    "- If the system prompt specifies a reasoning language other than English, pay close attention to whether the model actually thinks in that language\n",
    "- Look for code-switching or language mixing within the think tags\n",
    "- Note any creative or unexpected but valid interpretations of the instructions in your evaluation\n",
    "\n",
    "Remember: The goal is to determine if fine-tuning successfully taught the model to follow the specific format and language instructions while maintaining response quality.\n",
    "\n",
    "At the end of your evaluation, you MUST put your final preference in the tags <final_preference>...</final_preference> like:\n",
    "<final_preference>\n",
    "Response B - Score 80/100\n",
    "</final_preference>\n",
    "\n",
    "This is the baseline to evaluate:\n",
    "\n",
    "System prompt:\n",
    "\n",
    "{{system_prompt}}\n",
    "\n",
    "Question:\n",
    "\n",
    "{{question}}\n",
    "\n",
    "Target answer:\n",
    "\n",
    "{{answer}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd737b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"json\", data_files=\"./data/test/gen_qa.jsonl\", split=\"train\"\n",
    ")\n",
    "\n",
    "llm_val_dataset = []\n",
    "\n",
    "model_id = (\n",
    "    custom_model_arn\n",
    "    if custom_model_arn is not None\n",
    "    else \"<CUSTOM_MODEL_DEPLOYMENT_ARN>\"\n",
    ")\n",
    "\n",
    "request_times = []\n",
    "index = 1\n",
    "\n",
    "for el in test_dataset:\n",
    "    print(\"Processing row \", index)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": el[\"query\"]}]},\n",
    "    ]\n",
    "\n",
    "    # Rate limit for first request\n",
    "    current_time = time.time()\n",
    "    request_times = [t for t in request_times if current_time - t < 60]\n",
    "\n",
    "    if len(request_times) >= 5:\n",
    "        wait_time = 60 - (current_time - request_times[0])\n",
    "        print(f\"Waiting for {wait_time:.2f}s\")\n",
    "        time.sleep(wait_time)\n",
    "        request_times = request_times[1:]\n",
    "\n",
    "    request_times.append(time.time())\n",
    "\n",
    "    response_base = generate(\n",
    "        model_id=\"us.amazon.nova-micro-v1:0\",\n",
    "        system_prompt=el[\"system\"],\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        guardrail_id=guardrail_id,\n",
    "    )\n",
    "\n",
    "    # Rate limit for second request\n",
    "    current_time = time.time()\n",
    "    request_times = [t for t in request_times if current_time - t < 60]\n",
    "\n",
    "    if len(request_times) >= 5:\n",
    "        wait_time = 60 - (current_time - request_times[0])\n",
    "        print(f\"Waiting for {wait_time:.2f}s\")\n",
    "        time.sleep(wait_time)\n",
    "        request_times = request_times[1:]\n",
    "\n",
    "    request_times.append(time.time())\n",
    "\n",
    "    response_ft = generate(\n",
    "        model_id=model_id,\n",
    "        system_prompt=el[\"system\"],\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        guardrail_id=guardrail_id,\n",
    "    )\n",
    "\n",
    "    system_prompt = system_promt_base.format(\n",
    "        system_prompt=el[\"system\"],\n",
    "        question=el[\"query\"],\n",
    "        answer=el[\"response\"],\n",
    "    )\n",
    "\n",
    "    llm_val_dataset.append([\n",
    "        textwrap.dedent(system_prompt).strip(),\n",
    "        el[\"system\"],\n",
    "        el[\"query\"],\n",
    "        response_base[\"output\"][\"message\"][\"content\"][0][\"text\"],\n",
    "        response_ft[\"output\"][\"message\"][\"content\"][0][\"text\"],\n",
    "    ])\n",
    "\n",
    "    index += 1\n",
    "\n",
    "print(\"Inference completed!\")\n",
    "\n",
    "llm_judge_df = pd.DataFrame(\n",
    "    llm_val_dataset, columns=[\"llm_eval_prompt\", \"system_prompt\", \"question\", \"response_A\", \"response_B\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cd9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_df.to_json(\"./llm_judge_results_base_vs_pt.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f5c3c",
   "metadata": {},
   "source": [
    "### Use Amazon Nova Pro as Judge\n",
    "\n",
    "We invoke Amazon Nova Pro on the generated dataset to evaluate the fine-tuned model against the base one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "def extract_final_preference(text):\n",
    "    \"\"\"\n",
    "    Extract content within <final_preference>...</final_preference> tags\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing the tags\n",
    "\n",
    "    Returns:\n",
    "        str: The content within the tags, or None if not found\n",
    "    \"\"\"\n",
    "    # Pattern to match content between <final_preference> tags\n",
    "    pattern = r\"<final_preference>(.*?)</final_preference>\"\n",
    "\n",
    "    # Search for the pattern (re.DOTALL allows . to match newlines)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        # Return the captured group (content between tags)\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "request_times = []\n",
    "results = []\n",
    "\n",
    "for index, el in llm_judge_df.iterrows():\n",
    "    print(f\"Index: {index + 1}\")\n",
    "\n",
    "    # Rate limiting: max 5 requests per 60 seconds\n",
    "    current_time = time.time()\n",
    "    request_times = [t for t in request_times if current_time - t < 60]\n",
    "\n",
    "    if len(request_times) >= 5:\n",
    "        wait_time = 60 - (current_time - request_times[0])\n",
    "        print(f\"Waiting for {wait_time:.2f}s\")\n",
    "        time.sleep(wait_time)\n",
    "        request_times = request_times[1:]\n",
    "\n",
    "    request_times.append(time.time())\n",
    "\n",
    "    # Your existing code\n",
    "    prompt = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        Response A:\n",
    "        {response_A}\n",
    "\n",
    "        Response B:\n",
    "        {response_B}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "    prompt = prompt.format(\n",
    "        response_A=el[\"response_A\"],\n",
    "        response_B=el[\"response_B\"],\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": prompt}]},\n",
    "    ]\n",
    "\n",
    "    response = generate(\n",
    "        model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "        system_prompt=el[\"llm_eval_prompt\"],\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        guardrail_id=guardrail_id,\n",
    "    )\n",
    "\n",
    "    results.append([\n",
    "        extract_final_preference(response[\"output\"][\"message\"][\"content\"][0][\"text\"]),\n",
    "        response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    ])\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    results, columns=[\"preference\", \"details\"]\n",
    ")\n",
    "\n",
    "results_df.to_json(\"./llm_judge_results.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac409a91",
   "metadata": {},
   "source": [
    "#### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bafdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def parse_preference_data(json_file_path):\n",
    "    \"\"\"Parse the JSON file and extract preference data\"\"\"\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    preferences = []\n",
    "    scores_a = []\n",
    "    scores_b = []\n",
    "\n",
    "    for item in data:\n",
    "        preference_text = item.get(\"preference\", \"\")\n",
    "\n",
    "        if \"Response A\" in preference_text:\n",
    "            preferences.append(\"A Preferred\")\n",
    "            # Extract score for A\n",
    "            score_match = re.search(r\"Score (\\d+)/100\", preference_text)\n",
    "            if score_match:\n",
    "                scores_a.append(int(score_match.group(1)))\n",
    "        elif \"Response B\" in preference_text:\n",
    "            preferences.append(\"B Preferred\")\n",
    "            # Extract score for B\n",
    "            score_match = re.search(r\"Score (\\d+)/100\", preference_text)\n",
    "            if score_match:\n",
    "                scores_b.append(int(score_match.group(1)))\n",
    "        else:\n",
    "            preferences.append(\"Ties\")\n",
    "\n",
    "    return preferences, scores_a, scores_b\n",
    "\n",
    "\n",
    "def create_preference_pie_chart(preferences):\n",
    "    \"\"\"Create pie chart for preference distribution\"\"\"\n",
    "    preference_counts = Counter(preferences)\n",
    "\n",
    "    # Calculate percentages\n",
    "    total = len(preferences)\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    colors = []\n",
    "\n",
    "    for pref in [\"A Preferred\", \"B Preferred\", \"Ties\"]:\n",
    "        count = preference_counts.get(pref, 0)\n",
    "        percentage = (count / total) * 100\n",
    "        labels.append(f\"{pref}\\n{percentage:.1f}%\")\n",
    "        sizes.append(count)\n",
    "\n",
    "        if pref == \"A Preferred\":\n",
    "            colors.append(\"#FF6B6B\")  # Red/pink\n",
    "        elif pref == \"B Preferred\":\n",
    "            colors.append(\"#4ECDC4\")  # Teal/turquoise\n",
    "        else:\n",
    "            colors.append(\"#95E1D3\")  # Light green\n",
    "\n",
    "    # Remove entries with 0 count\n",
    "    filtered_data = [\n",
    "        (label, size, color)\n",
    "        for label, size, color in zip(labels, sizes, colors)\n",
    "        if size > 0\n",
    "    ]\n",
    "    if filtered_data:\n",
    "        labels, sizes, colors = zip(*filtered_data)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct=\"\", startangle=90)\n",
    "    plt.title(\n",
    "        \"Preference Distribution\\n(Valid Judgments Only)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def create_score_comparison_bar_chart(scores_a, scores_b):\n",
    "    \"\"\"Create bar chart comparing A vs B scores\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Count scores for A and B\n",
    "    count_a = len(scores_a)\n",
    "    count_b = len(scores_b)\n",
    "\n",
    "    categories = [\"A Scores\", \"B Scores\"]\n",
    "    counts = [count_a, count_b]\n",
    "    colors = [\"#FF6B6B\", \"#4ECDC4\"]  # Red for A, Teal for B\n",
    "\n",
    "    bars = plt.bar(categories, counts, color=colors)\n",
    "\n",
    "    # Add count labels on top of bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.1,\n",
    "            str(count),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "    # Add difference annotation\n",
    "    if count_b > count_a:\n",
    "        diff = count_b - count_a\n",
    "        plt.annotate(\n",
    "            f\"B leads by {diff}\",\n",
    "            xy=(1, count_b * 0.75),\n",
    "            xytext=(0.5, count_b * 0.75),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "            ha=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "    elif count_a > count_b:\n",
    "        diff = count_a - count_b\n",
    "        plt.annotate(\n",
    "            f\"A leads by {diff}\",\n",
    "            xy=(0, count_a * 0.75),\n",
    "            xytext=(0.5, count_a * 0.75),\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "            ha=\"center\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.title(\"A vs B Score Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Score Count\", fontsize=12)\n",
    "    plt.ylim(0, max(counts) * 1.2)\n",
    "\n",
    "    # Remove top and right spines\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa786915",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(\"./\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6aa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data\n",
    "preferences, scores_a, scores_b = parse_preference_data(evaluation_results_path)\n",
    "\n",
    "print(f\"Total judgments: {len(preferences)}\")\n",
    "print(f\"A Preferred: {preferences.count('A Preferred')}\")\n",
    "print(f\"B Preferred: {preferences.count('B Preferred')}\")\n",
    "print(f\"Ties: {preferences.count('Ties')}\")\n",
    "print(f\"A Scores: {scores_a}\")\n",
    "print(f\"B Scores: {scores_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plots\n",
    "fig1 = create_preference_pie_chart(preferences)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"./preference_distribution.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig2 = create_score_comparison_bar_chart(scores_a, scores_b)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"./score_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
